__________________________________________________________________________________________________
concatenate_73 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_73[0][0]
                                                                 conv2d_347[0][0]
__________________________________________________________________________________________________
conv2d_354 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_73[0][0]
__________________________________________________________________________________________________
conv2d_355 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_354[0][0]
__________________________________________________________________________________________________
up_sampling2d_74 (UpSampling2D) (None, 128, 128, 128 0           conv2d_355[0][0]
__________________________________________________________________________________________________
concatenate_74 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_74[0][0]
                                                                 conv2d_345[0][0]
__________________________________________________________________________________________________
conv2d_356 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_74[0][0]
__________________________________________________________________________________________________
conv2d_357 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_356[0][0]
__________________________________________________________________________________________________
up_sampling2d_75 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_357[0][0]
__________________________________________________________________________________________________
concatenate_75 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_75[0][0]
                                                                 conv2d_343[0][0]
__________________________________________________________________________________________________
conv2d_358 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_75[0][0]
__________________________________________________________________________________________________
conv2d_359 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_358[0][0]
__________________________________________________________________________________________________
conv2d_360 (Conv2D)             (None, 256, 256, 1)  33          conv2d_359[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6925 - acc: 0.7630 - precision_18: 0.0767 - auc_18: 0.4588 - recall_18: 0.03752021-09-25 01:31:32.840546: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 01:31:32.840663: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 01:31:33.373685: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 01:31:33.387086: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33
2021-09-25 01:31:33.390222: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33/xeon-09.trace.json.gz
2021-09-25 01:31:33.417878: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33
2021-09-25 01:31:33.425846: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33/xeon-09.memory_profile.json.gz
2021-09-25 01:31:33.438370: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33Dumped tool data for xplane.pb to logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:15 - loss: 0.6923 - acc: 0.6986 - precision_18: 0.0767 - auc_18: 0.3100 - recall_18: 0.0117WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1994s vs `on_train_batch_end` time: 0.4000s). Check your callbacks.
254/254 [==============================] - 57s 223ms/step - loss: 0.4260 - acc: 0.8350 - precision_18: 0.7082 - auc_18: 0.7986 - recall_18: 0.4130 - val_loss: 0.3998 - val_acc: 0.8185 - val_precision_18: 0.5755 - val_auc_18: 0.8362 - val_recall_18: 0.5544
Epoch 2/100
254/254 [==============================] - 56s 219ms/step - loss: 0.3578 - acc: 0.8479 - precision_18: 0.7301 - auc_18: 0.8572 - recall_18: 0.4846 - val_loss: 0.3080 - val_acc: 0.8750 - val_precision_18: 0.8082 - val_auc_18: 0.9095 - val_recall_18: 0.5382
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2596 - acc: 0.8931 - precision_18: 0.8247 - auc_18: 0.9262 - recall_18: 0.6608 - val_loss: 0.2155 - val_acc: 0.9237 - val_precision_18: 0.8328 - val_auc_18: 0.9524 - val_recall_18: 0.8054
Epoch 4/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2293 - acc: 0.9069 - precision_18: 0.8433 - auc_18: 0.9442 - recall_18: 0.7203 - val_loss: 0.1996 - val_acc: 0.9216 - val_precision_18: 0.8402 - val_auc_18: 0.9556 - val_recall_18: 0.7764
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1988 - acc: 0.9189 - precision_18: 0.8689 - auc_18: 0.9574 - recall_18: 0.7579 - val_loss: 0.1800 - val_acc: 0.9302 - val_precision_18: 0.8164 - val_auc_18: 0.9694 - val_recall_18: 0.8526
Epoch 6/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1857 - acc: 0.9225 - precision_18: 0.8715 - auc_18: 0.9633 - recall_18: 0.7736 - val_loss: 0.1750 - val_acc: 0.9349 - val_precision_18: 0.9012 - val_auc_18: 0.9736 - val_recall_18: 0.7859
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1824 - acc: 0.9237 - precision_18: 0.8719 - auc_18: 0.9645 - recall_18: 0.7801 - val_loss: 0.1768 - val_acc: 0.9305 - val_precision_18: 0.9005 - val_auc_18: 0.9652 - val_recall_18: 0.7508
Epoch 8/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1734 - acc: 0.9274 - precision_18: 0.8782 - auc_18: 0.9678 - recall_18: 0.7918 - val_loss: 0.1720 - val_acc: 0.9339 - val_precision_18: 0.9345 - val_auc_18: 0.9711 - val_recall_18: 0.7510
Epoch 9/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1710 - acc: 0.9285 - precision_18: 0.8808 - auc_18: 0.9688 - recall_18: 0.7961 - val_loss: 0.1767 - val_acc: 0.9368 - val_precision_18: 0.9411 - val_auc_18: 0.9731 - val_recall_18: 0.7444
Epoch 10/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1700 - acc: 0.9290 - precision_18: 0.8848 - auc_18: 0.9690 - recall_18: 0.7919 - val_loss: 0.1843 - val_acc: 0.9344 - val_precision_18: 0.9168 - val_auc_18: 0.9599 - val_recall_18: 0.7619
Epoch 11/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1664 - acc: 0.9296 - precision_18: 0.8841 - auc_18: 0.9711 - recall_18: 0.7987 - val_loss: 0.1572 - val_acc: 0.9397 - val_precision_18: 0.9319 - val_auc_18: 0.9759 - val_recall_18: 0.7710
Epoch 12/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1759 - acc: 0.9267 - precision_18: 0.8797 - auc_18: 0.9670 - recall_18: 0.7866 - val_loss: 0.1491 - val_acc: 0.9351 - val_precision_18: 0.8171 - val_auc_18: 0.9796 - val_recall_18: 0.8921
Epoch 13/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1645 - acc: 0.9310 - precision_18: 0.8875 - auc_18: 0.9713 - recall_18: 0.8010 - val_loss: 0.1588 - val_acc: 0.9373 - val_precision_18: 0.8181 - val_auc_18: 0.9813 - val_recall_18: 0.9104
Epoch 14/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1615 - acc: 0.9315 - precision_18: 0.8884 - auc_18: 0.9722 - recall_18: 0.8025 - val_loss: 0.1476 - val_acc: 0.9414 - val_precision_18: 0.8955 - val_auc_18: 0.9756 - val_recall_18: 0.8103
Epoch 15/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1535 - acc: 0.9342 - precision_18: 0.8938 - auc_18: 0.9756 - recall_18: 0.8099 - val_loss: 0.1754 - val_acc: 0.9386 - val_precision_18: 0.9097 - val_auc_18: 0.9640 - val_recall_18: 0.8041
Epoch 16/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1533 - acc: 0.9345 - precision_18: 0.8949 - auc_18: 0.9758 - recall_18: 0.8109 - val_loss: 0.1382 - val_acc: 0.9462 - val_precision_18: 0.9152 - val_auc_18: 0.9784 - val_recall_18: 0.8026
Epoch 17/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1549 - acc: 0.9345 - precision_18: 0.8987 - auc_18: 0.9750 - recall_18: 0.8067 - val_loss: 0.1700 - val_acc: 0.9344 - val_precision_18: 0.8376 - val_auc_18: 0.9770 - val_recall_18: 0.8702
Epoch 18/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1559 - acc: 0.9340 - precision_18: 0.8934 - auc_18: 0.9745 - recall_18: 0.8102 - val_loss: 0.1308 - val_acc: 0.9490 - val_precision_18: 0.9017 - val_auc_18: 0.9809 - val_recall_18: 0.8457
Epoch 19/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1528 - acc: 0.9347 - precision_18: 0.8933 - auc_18: 0.9755 - recall_18: 0.8130 - val_loss: 0.1449 - val_acc: 0.9417 - val_precision_18: 0.9215 - val_auc_18: 0.9797 - val_recall_18: 0.8038
Epoch 20/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1552 - acc: 0.9335 - precision_18: 0.8930 - auc_18: 0.9747 - recall_18: 0.8084 - val_loss: 0.1312 - val_acc: 0.9472 - val_precision_18: 0.8607 - val_auc_18: 0.9791 - val_recall_18: 0.8640
Epoch 21/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1563 - acc: 0.9338 - precision_18: 0.8909 - auc_18: 0.9751 - recall_18: 0.8121 - val_loss: 0.1525 - val_acc: 0.9421 - val_precision_18: 0.9063 - val_auc_18: 0.9806 - val_recall_18: 0.8377
Epoch 22/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1492 - acc: 0.9362 - precision_18: 0.8997 - auc_18: 0.9767 - recall_18: 0.8142 - val_loss: 0.1465 - val_acc: 0.9420 - val_precision_18: 0.8542 - val_auc_18: 0.9789 - val_recall_18: 0.8615
Epoch 23/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1493 - acc: 0.9368 - precision_18: 0.9011 - auc_18: 0.9763 - recall_18: 0.8156 - val_loss: 0.1364 - val_acc: 0.9467 - val_precision_18: 0.8881 - val_auc_18: 0.9805 - val_recall_18: 0.8556
Epoch 24/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1507 - acc: 0.9359 - precision_18: 0.8992 - auc_18: 0.9763 - recall_18: 0.8130 - val_loss: 0.1411 - val_acc: 0.9435 - val_precision_18: 0.8603 - val_auc_18: 0.9806 - val_recall_18: 0.8818
Epoch 25/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1451 - acc: 0.9380 - precision_18: 0.9002 - auc_18: 0.9783 - recall_18: 0.8238 - val_loss: 0.1445 - val_acc: 0.9429 - val_precision_18: 0.8287 - val_auc_18: 0.9847 - val_recall_18: 0.9140
Epoch 26/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1430 - acc: 0.9385 - precision_18: 0.9026 - auc_18: 0.9790 - recall_18: 0.8223 - val_loss: 0.1360 - val_acc: 0.9453 - val_precision_18: 0.8793 - val_auc_18: 0.9798 - val_recall_18: 0.8580
Epoch 27/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1468 - acc: 0.9378 - precision_18: 0.9030 - auc_18: 0.9773 - recall_18: 0.8196 - val_loss: 0.1262 - val_acc: 0.9510 - val_precision_18: 0.9196 - val_auc_18: 0.9843 - val_recall_18: 0.8435
Epoch 28/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1407 - acc: 0.9398 - precision_18: 0.9066 - auc_18: 0.9797 - recall_18: 0.8256 - val_loss: 0.1633 - val_acc: 0.9351 - val_precision_18: 0.8046 - val_auc_18: 0.9806 - val_recall_18: 0.9174
Epoch 29/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1424 - acc: 0.9382 - precision_18: 0.8990 - auc_18: 0.9789 - recall_18: 0.8268 - val_loss: 0.1804 - val_acc: 0.9223 - val_precision_18: 0.7755 - val_auc_18: 0.9793 - val_recall_18: 0.9232
Epoch 30/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1420 - acc: 0.9386 - precision_18: 0.8989 - auc_18: 0.9792 - recall_18: 0.8293 - val_loss: 0.1213 - val_acc: 0.9512 - val_precision_18: 0.9403 - val_auc_18: 0.9856 - val_recall_18: 0.8215
Epoch 31/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1418 - acc: 0.9391 - precision_18: 0.9059 - auc_18: 0.9790 - recall_18: 0.8233 - val_loss: 0.1384 - val_acc: 0.9481 - val_precision_18: 0.8717 - val_auc_18: 0.9797 - val_recall_18: 0.8825
Epoch 32/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1382 - acc: 0.9406 - precision_18: 0.9063 - auc_18: 0.9800 - recall_18: 0.8310 - val_loss: 0.1343 - val_acc: 0.9495 - val_precision_18: 0.8808 - val_auc_18: 0.9855 - val_recall_18: 0.8832
Epoch 33/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1372 - acc: 0.9407 - precision_18: 0.9032 - auc_18: 0.9806 - recall_18: 0.8338 - val_loss: 0.1276 - val_acc: 0.9487 - val_precision_18: 0.8753 - val_auc_18: 0.9841 - val_recall_18: 0.8824
Epoch 34/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1454 - acc: 0.9379 - precision_18: 0.9006 - auc_18: 0.9780 - recall_18: 0.8221 - val_loss: 0.1284 - val_acc: 0.9500 - val_precision_18: 0.8730 - val_auc_18: 0.9830 - val_recall_18: 0.8715
Epoch 35/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1342 - acc: 0.9418 - precision_18: 0.9063 - auc_18: 0.9812 - recall_18: 0.8362 - val_loss: 0.1320 - val_acc: 0.9480 - val_precision_18: 0.8879 - val_auc_18: 0.9838 - val_recall_18: 0.8803
Epoch 36/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1360 - acc: 0.9418 - precision_18: 0.9088 - auc_18: 0.9806 - recall_18: 0.8327 - val_loss: 0.1290 - val_acc: 0.9486 - val_precision_18: 0.8902 - val_auc_18: 0.9819 - val_recall_18: 0.8650
Epoch 37/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1348 - acc: 0.9417 - precision_18: 0.9072 - auc_18: 0.9812 - recall_18: 0.8347 - val_loss: 0.1425 - val_acc: 0.9456 - val_precision_18: 0.9679 - val_auc_18: 0.9842 - val_recall_18: 0.7737
Epoch 38/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1322 - acc: 0.9426 - precision_18: 0.9069 - auc_18: 0.9818 - recall_18: 0.8412 - val_loss: 0.1276 - val_acc: 0.9514 - val_precision_18: 0.9159 - val_auc_18: 0.9833 - val_recall_18: 0.8461
Epoch 39/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1327 - acc: 0.9426 - precision_18: 0.9115 - auc_18: 0.9817 - recall_18: 0.8340 - val_loss: 0.1405 - val_acc: 0.9447 - val_precision_18: 0.9088 - val_auc_18: 0.9814 - val_recall_18: 0.8311
Epoch 40/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1328 - acc: 0.9424 - precision_18: 0.9067 - auc_18: 0.9819 - recall_18: 0.8406 - val_loss: 0.1334 - val_acc: 0.9498 - val_precision_18: 0.9512 - val_auc_18: 0.9819 - val_recall_18: 0.7877
Epoch 41/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1317 - acc: 0.9431 - precision_18: 0.9091 - auc_18: 0.9819 - recall_18: 0.8403 - val_loss: 0.1396 - val_acc: 0.9433 - val_precision_18: 0.8786 - val_auc_18: 0.9796 - val_recall_18: 0.8599
Epoch 42/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1390 - acc: 0.9397 - precision_18: 0.9065 - auc_18: 0.9802 - recall_18: 0.8268 - val_loss: 0.1293 - val_acc: 0.9500 - val_precision_18: 0.8995 - val_auc_18: 0.9837 - val_recall_18: 0.8579
Epoch 43/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1333 - acc: 0.9413 - precision_18: 0.9075 - auc_18: 0.9820 - recall_18: 0.8342 - val_loss: 0.1373 - val_acc: 0.9467 - val_precision_18: 0.8870 - val_auc_18: 0.9813 - val_recall_18: 0.8647
Epoch 44/100
254/254 [==============================] - ETA: 0s - loss: 0.1302 - acc: 0.9439 - precision_18: 0.9096 - auc_18: 0.9827 - recall_18: 0.8435Restoring model weights from the end of the best epoch.
254/254 [==============================] - 56s 219ms/step - loss: 0.1302 - acc: 0.9439 - precision_18: 0.9096 - auc_18: 0.9827 - recall_18: 0.8435 - val_loss: 0.1225 - val_acc: 0.9514 - val_precision_18: 0.9258 - val_auc_18: 0.9827 - val_recall_18: 0.8381
Epoch 00044: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1347 - acc: 0.9476 - precision_18: 0.9290 - auc_18: 0.9822 - recall_18: 0.8288
---------------TEST METRICS----------------------
jaccard_index 0.7829513311599599
test_sensitivity 0.840265239703776
test_specifitivity 0.975969874425899
test_accuracy 0.9456261168134973
test_precision 0.9096697121524707
test_jaccard_score 0.7829513311599599
test_dicecoef 0.8735911445419847
isic_eval_score 0.9929078014184397
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-021309.h5
[0. 0. 0. 0. 0.] [0.94562612 0.78295133 0.90966971 0.84026524 0.97596987]

-------------------------
Rep: 1
-------------------------

2021-09-25 02:13:10.651657: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 02:13:10.651782: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Normalized per channel
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 4.962215403927742e-17,
Validation samples: 383, channel mean: 0.0053282898964007895
Model built.
Model: "functional_39"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_20 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_361 (Conv2D)             (None, 256, 256, 32) 320         input_20[0][0]
__________________________________________________________________________________________________
conv2d_362 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_361[0][0]
__________________________________________________________________________________________________
max_pooling2d_76 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_362[0][0]
__________________________________________________________________________________________________
conv2d_363 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_76[0][0]
__________________________________________________________________________________________________
conv2d_364 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_363[0][0]
__________________________________________________________________________________________________
max_pooling2d_77 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_364[0][0]
__________________________________________________________________________________________________
conv2d_365 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_77[0][0]
__________________________________________________________________________________________________
conv2d_366 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_365[0][0]
__________________________________________________________________________________________________
max_pooling2d_78 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_366[0][0]
__________________________________________________________________________________________________
conv2d_367 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_78[0][0]
__________________________________________________________________________________________________
conv2d_368 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_367[0][0]
__________________________________________________________________________________________________
max_pooling2d_79 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_368[0][0]
__________________________________________________________________________________________________
conv2d_369 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_79[0][0]
__________________________________________________________________________________________________
conv2d_370 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_369[0][0]
__________________________________________________________________________________________________
up_sampling2d_76 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_370[0][0]
__________________________________________________________________________________________________
concatenate_76 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_76[0][0]
                                                                 conv2d_368[0][0]
__________________________________________________________________________________________________
conv2d_371 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_76[0][0]
__________________________________________________________________________________________________
conv2d_372 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_371[0][0]
__________________________________________________________________________________________________
up_sampling2d_77 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_372[0][0]
__________________________________________________________________________________________________
concatenate_77 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_77[0][0]
                                                                 conv2d_366[0][0]
__________________________________________________________________________________________________
conv2d_373 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_77[0][0]
__________________________________________________________________________________________________
conv2d_374 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_373[0][0]
__________________________________________________________________________________________________
up_sampling2d_78 (UpSampling2D) (None, 128, 128, 128 0           conv2d_374[0][0]
__________________________________________________________________________________________________
concatenate_78 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_78[0][0]
                                                                 conv2d_364[0][0]
__________________________________________________________________________________________________
conv2d_375 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_78[0][0]
__________________________________________________________________________________________________
conv2d_376 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_375[0][0]
__________________________________________________________________________________________________
up_sampling2d_79 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_376[0][0]
__________________________________________________________________________________________________
concatenate_79 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_79[0][0]
                                                                 conv2d_362[0][0]
__________________________________________________________________________________________________
conv2d_377 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_79[0][0]
__________________________________________________________________________________________________
conv2d_378 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_377[0][0]
__________________________________________________________________________________________________
conv2d_379 (Conv2D)             (None, 256, 256, 1)  33          conv2d_378[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6929 - acc: 0.7967 - precision_19: 0.1574 - auc_19: 0.5000 - recall_19: 0.04872021-09-25 02:13:19.698195: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 02:13:19.698306: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 02:13:20.205399: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 02:13:20.224579: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20
2021-09-25 02:13:20.229105: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20/xeon-09.trace.json.gz
2021-09-25 02:13:20.256197: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20
2021-09-25 02:13:20.265111: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20/xeon-09.memory_profile.json.gz
2021-09-25 02:13:20.284886: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20Dumped tool data for xplane.pb to logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:14 - loss: 0.6913 - acc: 0.7154 - precision_19: 0.1599 - auc_19: 0.4029 - recall_19: 0.0156WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1946s vs `on_train_batch_end` time: 0.3937s). Check your callbacks.
254/254 [==============================] - 57s 224ms/step - loss: 0.4237 - acc: 0.8356 - precision_19: 0.7004 - auc_19: 0.7863 - recall_19: 0.4290 - val_loss: 0.4165 - val_acc: 0.8483 - val_precision_19: 0.7955 - val_auc_19: 0.8087 - val_recall_19: 0.3847
Epoch 2/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3876 - acc: 0.8411 - precision_19: 0.7092 - auc_19: 0.8230 - recall_19: 0.4617 - val_loss: 0.3782 - val_acc: 0.8467 - val_precision_19: 0.7071 - val_auc_19: 0.8299 - val_recall_19: 0.4727
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3490 - acc: 0.8620 - precision_19: 0.7745 - auc_19: 0.8505 - recall_19: 0.5213 - val_loss: 0.3266 - val_acc: 0.8822 - val_precision_19: 0.7170 - val_auc_19: 0.8898 - val_recall_19: 0.7424
Epoch 4/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2738 - acc: 0.8943 - precision_19: 0.8412 - auc_19: 0.9116 - recall_19: 0.6455 - val_loss: 0.2154 - val_acc: 0.9205 - val_precision_19: 0.8800 - val_auc_19: 0.9457 - val_recall_19: 0.7221
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2428 - acc: 0.9027 - precision_19: 0.8557 - auc_19: 0.9330 - recall_19: 0.6800 - val_loss: 0.2108 - val_acc: 0.9232 - val_precision_19: 0.8640 - val_auc_19: 0.9505 - val_recall_19: 0.7439
Epoch 6/100
254/254 [==============================] - 56s 221ms/step - loss: 0.2205 - acc: 0.9105 - precision_19: 0.8637 - auc_19: 0.9464 - recall_19: 0.7136 - val_loss: 0.2043 - val_acc: 0.9258 - val_precision_19: 0.9348 - val_auc_19: 0.9594 - val_recall_19: 0.7072
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2081 - acc: 0.9159 - precision_19: 0.8712 - auc_19: 0.9521 - recall_19: 0.7366 - val_loss: 0.1830 - val_acc: 0.9285 - val_precision_19: 0.9115 - val_auc_19: 0.9621 - val_recall_19: 0.7295
Epoch 8/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1994 - acc: 0.9176 - precision_19: 0.8724 - auc_19: 0.9560 - recall_19: 0.7437 - val_loss: 0.2023 - val_acc: 0.9232 - val_precision_19: 0.8751 - val_auc_19: 0.9554 - val_recall_19: 0.7578
Epoch 9/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1979 - acc: 0.9181 - precision_19: 0.8707 - auc_19: 0.9581 - recall_19: 0.7502 - val_loss: 0.2208 - val_acc: 0.9106 - val_precision_19: 0.9874 - val_auc_19: 0.9653 - val_recall_19: 0.5798
Epoch 10/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1902 - acc: 0.9207 - precision_19: 0.8751 - auc_19: 0.9617 - recall_19: 0.7576 - val_loss: 0.1925 - val_acc: 0.9261 - val_precision_19: 0.9355 - val_auc_19: 0.9609 - val_recall_19: 0.7022
Epoch 11/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1925 - acc: 0.9205 - precision_19: 0.8780 - auc_19: 0.9603 - recall_19: 0.7562 - val_loss: 0.1858 - val_acc: 0.9272 - val_precision_19: 0.9505 - val_auc_19: 0.9676 - val_recall_19: 0.6916
Epoch 12/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1860 - acc: 0.9227 - precision_19: 0.8776 - auc_19: 0.9638 - recall_19: 0.7664 - val_loss: 0.1837 - val_acc: 0.9396 - val_precision_19: 0.8865 - val_auc_19: 0.9723 - val_recall_19: 0.8183
Epoch 13/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1780 - acc: 0.9258 - precision_19: 0.8805 - auc_19: 0.9671 - recall_19: 0.7814 - val_loss: 0.1593 - val_acc: 0.9373 - val_precision_19: 0.8405 - val_auc_19: 0.9768 - val_recall_19: 0.8733
Epoch 14/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1932 - acc: 0.9192 - precision_19: 0.8663 - auc_19: 0.9595 - recall_19: 0.7617 - val_loss: 0.1750 - val_acc: 0.9347 - val_precision_19: 0.9499 - val_auc_19: 0.9695 - val_recall_19: 0.7215
Epoch 15/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1737 - acc: 0.9274 - precision_19: 0.8839 - auc_19: 0.9682 - recall_19: 0.7844 - val_loss: 0.1593 - val_acc: 0.9393 - val_precision_19: 0.8560 - val_auc_19: 0.9778 - val_recall_19: 0.8745
Epoch 16/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1681 - acc: 0.9288 - precision_19: 0.8843 - auc_19: 0.9705 - recall_19: 0.7932 - val_loss: 0.1417 - val_acc: 0.9459 - val_precision_19: 0.8995 - val_auc_19: 0.9750 - val_recall_19: 0.8183
Epoch 17/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1774 - acc: 0.9265 - precision_19: 0.8886 - auc_19: 0.9670 - recall_19: 0.7756 - val_loss: 0.1657 - val_acc: 0.9390 - val_precision_19: 0.8699 - val_auc_19: 0.9762 - val_recall_19: 0.8494
Epoch 18/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1714 - acc: 0.9279 - precision_19: 0.8858 - auc_19: 0.9691 - recall_19: 0.7870 - val_loss: 0.1504 - val_acc: 0.9436 - val_precision_19: 0.8638 - val_auc_19: 0.9771 - val_recall_19: 0.8633
Epoch 19/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1665 - acc: 0.9303 - precision_19: 0.8871 - auc_19: 0.9709 - recall_19: 0.7967 - val_loss: 0.1499 - val_acc: 0.9431 - val_precision_19: 0.9243 - val_auc_19: 0.9766 - val_recall_19: 0.8078
Epoch 20/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1653 - acc: 0.9305 - precision_19: 0.8895 - auc_19: 0.9713 - recall_19: 0.7974 - val_loss: 0.1320 - val_acc: 0.9498 - val_precision_19: 0.8967 - val_auc_19: 0.9774 - val_recall_19: 0.8338
Epoch 21/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1762 - acc: 0.9268 - precision_19: 0.8792 - auc_19: 0.9679 - recall_19: 0.7894 - val_loss: 0.1563 - val_acc: 0.9399 - val_precision_19: 0.8874 - val_auc_19: 0.9762 - val_recall_19: 0.8495
Epoch 22/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1618 - acc: 0.9313 - precision_19: 0.8912 - auc_19: 0.9725 - recall_19: 0.7985 - val_loss: 0.1524 - val_acc: 0.9419 - val_precision_19: 0.8780 - val_auc_19: 0.9745 - val_recall_19: 0.8291
Epoch 23/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1580 - acc: 0.9332 - precision_19: 0.8934 - auc_19: 0.9732 - recall_19: 0.8059 - val_loss: 0.1411 - val_acc: 0.9450 - val_precision_19: 0.8816 - val_auc_19: 0.9794 - val_recall_19: 0.8547
Epoch 24/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1774 - acc: 0.9262 - precision_19: 0.8845 - auc_19: 0.9663 - recall_19: 0.7780 - val_loss: 0.1468 - val_acc: 0.9430 - val_precision_19: 0.8865 - val_auc_19: 0.9755 - val_recall_19: 0.8441
Epoch 25/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1736 - acc: 0.9283 - precision_19: 0.8847 - auc_19: 0.9679 - recall_19: 0.7904 - val_loss: 0.1606 - val_acc: 0.9376 - val_precision_19: 0.8150 - val_auc_19: 0.9813 - val_recall_19: 0.9053
Epoch 26/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1683 - acc: 0.9300 - precision_19: 0.8825 - auc_19: 0.9696 - recall_19: 0.8012 - val_loss: 0.1460 - val_acc: 0.9438 - val_precision_19: 0.8974 - val_auc_19: 0.9761 - val_recall_19: 0.8278
Epoch 27/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1620 - acc: 0.9324 - precision_19: 0.8940 - auc_19: 0.9719 - recall_19: 0.8014 - val_loss: 0.1355 - val_acc: 0.9471 - val_precision_19: 0.9064 - val_auc_19: 0.9819 - val_recall_19: 0.8382
Epoch 28/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1615 - acc: 0.9316 - precision_19: 0.8918 - auc_19: 0.9725 - recall_19: 0.7998 - val_loss: 0.1604 - val_acc: 0.9383 - val_precision_19: 0.8291 - val_auc_19: 0.9803 - val_recall_19: 0.8940
Epoch 29/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1616 - acc: 0.9316 - precision_19: 0.8899 - auc_19: 0.9727 - recall_19: 0.8027 - val_loss: 0.1741 - val_acc: 0.9282 - val_precision_19: 0.7957 - val_auc_19: 0.9788 - val_recall_19: 0.9173
Epoch 30/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1579 - acc: 0.9333 - precision_19: 0.8916 - auc_19: 0.9734 - recall_19: 0.8106 - val_loss: 0.1357 - val_acc: 0.9474 - val_precision_19: 0.9362 - val_auc_19: 0.9814 - val_recall_19: 0.8067
Epoch 31/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1555 - acc: 0.9343 - precision_19: 0.8980 - auc_19: 0.9739 - recall_19: 0.8072 - val_loss: 0.1425 - val_acc: 0.9431 - val_precision_19: 0.8460 - val_auc_19: 0.9810 - val_recall_19: 0.8907
Epoch 32/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1570 - acc: 0.9338 - precision_19: 0.8936 - auc_19: 0.9741 - recall_19: 0.8104 - val_loss: 0.1359 - val_acc: 0.9481 - val_precision_19: 0.8927 - val_auc_19: 0.9814 - val_recall_19: 0.8609
Epoch 33/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1501 - acc: 0.9359 - precision_19: 0.8933 - auc_19: 0.9765 - recall_19: 0.8206 - val_loss: 0.1260 - val_acc: 0.9506 - val_precision_19: 0.8942 - val_auc_19: 0.9837 - val_recall_19: 0.8685
Epoch 34/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1511 - acc: 0.9358 - precision_19: 0.8976 - auc_19: 0.9759 - recall_19: 0.8144 - val_loss: 0.1302 - val_acc: 0.9499 - val_precision_19: 0.8795 - val_auc_19: 0.9813 - val_recall_19: 0.8627
Epoch 35/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1479 - acc: 0.9370 - precision_19: 0.9012 - auc_19: 0.9770 - recall_19: 0.8165 - val_loss: 0.1379 - val_acc: 0.9442 - val_precision_19: 0.8708 - val_auc_19: 0.9829 - val_recall_19: 0.8831
Epoch 36/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1451 - acc: 0.9382 - precision_19: 0.9006 - auc_19: 0.9779 - recall_19: 0.8239 - val_loss: 0.1345 - val_acc: 0.9471 - val_precision_19: 0.9024 - val_auc_19: 0.9797 - val_recall_19: 0.8425
Epoch 37/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1505 - acc: 0.9363 - precision_19: 0.8969 - auc_19: 0.9760 - recall_19: 0.8189 - val_loss: 0.1779 - val_acc: 0.9349 - val_precision_19: 0.9728 - val_auc_19: 0.9790 - val_recall_19: 0.7183
Epoch 38/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1421 - acc: 0.9393 - precision_19: 0.9024 - auc_19: 0.9790 - recall_19: 0.8290 - val_loss: 0.1364 - val_acc: 0.9477 - val_precision_19: 0.8679 - val_auc_19: 0.9828 - val_recall_19: 0.8855
Epoch 39/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1426 - acc: 0.9384 - precision_19: 0.9034 - auc_19: 0.9789 - recall_19: 0.8220 - val_loss: 0.1378 - val_acc: 0.9451 - val_precision_19: 0.9067 - val_auc_19: 0.9802 - val_recall_19: 0.8353
Epoch 40/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1406 - acc: 0.9393 - precision_19: 0.9028 - auc_19: 0.9797 - recall_19: 0.8292 - val_loss: 0.1474 - val_acc: 0.9446 - val_precision_19: 0.9720 - val_auc_19: 0.9811 - val_recall_19: 0.7428
Epoch 41/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1411 - acc: 0.9392 - precision_19: 0.8997 - auc_19: 0.9794 - recall_19: 0.8311 - val_loss: 0.1561 - val_acc: 0.9335 - val_precision_19: 0.8283 - val_auc_19: 0.9785 - val_recall_19: 0.8785
Epoch 42/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1405 - acc: 0.9393 - precision_19: 0.9032 - auc_19: 0.9796 - recall_19: 0.8284 - val_loss: 0.1344 - val_acc: 0.9477 - val_precision_19: 0.8842 - val_auc_19: 0.9824 - val_recall_19: 0.8643
Epoch 43/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1390 - acc: 0.9398 - precision_19: 0.9053 - auc_19: 0.9799 - recall_19: 0.8296 - val_loss: 0.1537 - val_acc: 0.9383 - val_precision_19: 0.8449 - val_auc_19: 0.9800 - val_recall_19: 0.8773
Epoch 44/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1385 - acc: 0.9405 - precision_19: 0.9040 - auc_19: 0.9802 - recall_19: 0.8331 - val_loss: 0.1246 - val_acc: 0.9514 - val_precision_19: 0.9318 - val_auc_19: 0.9819 - val_recall_19: 0.8321
Epoch 45/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1417 - acc: 0.9395 - precision_19: 0.9063 - auc_19: 0.9792 - recall_19: 0.8245 - val_loss: 0.1344 - val_acc: 0.9460 - val_precision_19: 0.9227 - val_auc_19: 0.9811 - val_recall_19: 0.8167
Epoch 46/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1463 - acc: 0.9372 - precision_19: 0.9012 - auc_19: 0.9781 - recall_19: 0.8181 - val_loss: 0.1297 - val_acc: 0.9478 - val_precision_19: 0.8783 - val_auc_19: 0.9827 - val_recall_19: 0.8725
Epoch 47/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1351 - acc: 0.9414 - precision_19: 0.9070 - auc_19: 0.9811 - recall_19: 0.8347 - val_loss: 0.1198 - val_acc: 0.9521 - val_precision_19: 0.9111 - val_auc_19: 0.9844 - val_recall_19: 0.8606
Epoch 48/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1352 - acc: 0.9414 - precision_19: 0.9043 - auc_19: 0.9807 - recall_19: 0.8384 - val_loss: 0.1275 - val_acc: 0.9507 - val_precision_19: 0.9406 - val_auc_19: 0.9830 - val_recall_19: 0.8164
Epoch 49/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1346 - acc: 0.9416 - precision_19: 0.9081 - auc_19: 0.9813 - recall_19: 0.8335 - val_loss: 0.1573 - val_acc: 0.9395 - val_precision_19: 0.8278 - val_auc_19: 0.9820 - val_recall_19: 0.9029
Epoch 50/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1329 - acc: 0.9422 - precision_19: 0.9075 - auc_19: 0.9816 - recall_19: 0.8370 - val_loss: 0.1344 - val_acc: 0.9424 - val_precision_19: 0.8416 - val_auc_19: 0.9827 - val_recall_19: 0.8987
Epoch 51/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1423 - acc: 0.9394 - precision_19: 0.9042 - auc_19: 0.9785 - recall_19: 0.8275 - val_loss: 0.1276 - val_acc: 0.9479 - val_precision_19: 0.8717 - val_auc_19: 0.9839 - val_recall_19: 0.8821
Epoch 52/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1385 - acc: 0.9402 - precision_19: 0.9020 - auc_19: 0.9801 - recall_19: 0.8335 - val_loss: 0.1658 - val_acc: 0.9357 - val_precision_19: 0.8079 - val_auc_19: 0.9822 - val_recall_19: 0.9172
Epoch 53/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1305 - acc: 0.9431 - precision_19: 0.9097 - auc_19: 0.9823 - recall_19: 0.8395 - val_loss: 0.1243 - val_acc: 0.9494 - val_precision_19: 0.8809 - val_auc_19: 0.9843 - val_recall_19: 0.8788
Epoch 54/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1347 - acc: 0.9417 - precision_19: 0.9051 - auc_19: 0.9817 - recall_19: 0.8390 - val_loss: 0.1341 - val_acc: 0.9475 - val_precision_19: 0.9281 - val_auc_19: 0.9807 - val_recall_19: 0.8175
Epoch 55/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1348 - acc: 0.9416 - precision_19: 0.9075 - auc_19: 0.9811 - recall_19: 0.8344 - val_loss: 0.1115 - val_acc: 0.9558 - val_precision_19: 0.9148 - val_auc_19: 0.9861 - val_recall_19: 0.8698
Epoch 56/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1318 - acc: 0.9422 - precision_19: 0.9087 - auc_19: 0.9820 - recall_19: 0.8357 - val_loss: 0.1307 - val_acc: 0.9474 - val_precision_19: 0.8720 - val_auc_19: 0.9824 - val_recall_19: 0.8886
Epoch 57/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1318 - acc: 0.9425 - precision_19: 0.9061 - auc_19: 0.9818 - recall_19: 0.8408 - val_loss: 0.1217 - val_acc: 0.9512 - val_precision_19: 0.8842 - val_auc_19: 0.9860 - val_recall_19: 0.8826
Epoch 58/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1328 - acc: 0.9428 - precision_19: 0.9085 - auc_19: 0.9818 - recall_19: 0.8409 - val_loss: 0.1452 - val_acc: 0.9456 - val_precision_19: 0.9663 - val_auc_19: 0.9814 - val_recall_19: 0.7603
Epoch 59/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1350 - acc: 0.9416 - precision_19: 0.9086 - auc_19: 0.9810 - recall_19: 0.8326 - val_loss: 0.1542 - val_acc: 0.9362 - val_precision_19: 0.8271 - val_auc_19: 0.9803 - val_recall_19: 0.9029
Epoch 60/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1332 - acc: 0.9419 - precision_19: 0.9083 - auc_19: 0.9818 - recall_19: 0.8351 - val_loss: 0.1133 - val_acc: 0.9541 - val_precision_19: 0.9117 - val_auc_19: 0.9867 - val_recall_19: 0.8683
Epoch 61/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1294 - acc: 0.9434 - precision_19: 0.9098 - auc_19: 0.9825 - recall_19: 0.8416 - val_loss: 0.1254 - val_acc: 0.9518 - val_precision_19: 0.9159 - val_auc_19: 0.9832 - val_recall_19: 0.8511
Epoch 62/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1357 - acc: 0.9422 - precision_19: 0.9071 - auc_19: 0.9812 - recall_19: 0.8386 - val_loss: 0.1207 - val_acc: 0.9500 - val_precision_19: 0.8904 - val_auc_19: 0.9842 - val_recall_19: 0.8678
Epoch 63/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1301 - acc: 0.9433 - precision_19: 0.9085 - auc_19: 0.9823 - recall_19: 0.8409 - val_loss: 0.1153 - val_acc: 0.9536 - val_precision_19: 0.9330 - val_auc_19: 0.9862 - val_recall_19: 0.8389
Epoch 64/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1359 - acc: 0.9416 - precision_19: 0.9078 - auc_19: 0.9803 - recall_19: 0.8333 - val_loss: 0.1251 - val_acc: 0.9502 - val_precision_19: 0.9448 - val_auc_19: 0.9848 - val_recall_19: 0.8220
Epoch 65/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1327 - acc: 0.9429 - precision_19: 0.9102 - auc_19: 0.9813 - recall_19: 0.8383 - val_loss: 0.1254 - val_acc: 0.9511 - val_precision_19: 0.9043 - val_auc_19: 0.9805 - val_recall_19: 0.8443
Epoch 66/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1300 - acc: 0.9441 - precision_19: 0.9107 - auc_19: 0.9819 - recall_19: 0.8443 - val_loss: 0.1212 - val_acc: 0.9509 - val_precision_19: 0.9456 - val_auc_19: 0.9865 - val_recall_19: 0.8270
Epoch 67/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1267 - acc: 0.9443 - precision_19: 0.9107 - auc_19: 0.9836 - recall_19: 0.8448 - val_loss: 0.1161 - val_acc: 0.9557 - val_precision_19: 0.9249 - val_auc_19: 0.9836 - val_recall_19: 0.8538
Epoch 68/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1300 - acc: 0.9435 - precision_19: 0.9124 - auc_19: 0.9821 - recall_19: 0.8380 - val_loss: 0.1267 - val_acc: 0.9460 - val_precision_19: 0.8472 - val_auc_19: 0.9854 - val_recall_19: 0.9047
Epoch 69/100
254/254 [==============================] - ETA: 0s - loss: 0.1263 - acc: 0.9444 - precision_19: 0.9095 - auc_19: 0.9835 - recall_19: 0.8473Restoring model weights from the end of the best epoch.
254/254 [==============================] - 56s 219ms/step - loss: 0.1263 - acc: 0.9444 - precision_19: 0.9095 - auc_19: 0.9835 - recall_19: 0.8473 - val_loss: 0.1376 - val_acc: 0.9420 - val_precision_19: 0.8409 - val_auc_19: 0.9818 - val_recall_19: 0.8949
Epoch 00069: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1303 - acc: 0.9494 - precision_19: 0.9036 - auc_19: 0.9830 - recall_19: 0.8663
---------------TEST METRICS----------------------
jaccard_index 0.8028049092620341
test_sensitivity 0.8752965882161342
test_specifitivity 0.9711306338487306
test_accuracy 0.9497019990961603
test_precision 0.8972449759706855
test_jaccard_score 0.8028049092620341
test_dicecoef 0.8861348948073916
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-031812.h5
[0.94562612 0.78295133 0.90966971 0.84026524 0.97596987] [0.949702   0.80280491 0.89724498 0.87529659 0.97113063]

-------------------------
Rep: 2
-------------------------

2021-09-25 03:18:13.487523: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 03:18:13.487656: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Normalized per channel
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 4.962215403927742e-17,
Validation samples: 383, channel mean: 0.0053282898964007895
Model built.
Model: "functional_41"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_21 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_380 (Conv2D)             (None, 256, 256, 32) 320         input_21[0][0]
__________________________________________________________________________________________________
conv2d_381 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_380[0][0]
__________________________________________________________________________________________________
max_pooling2d_80 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_381[0][0]
__________________________________________________________________________________________________
conv2d_382 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_80[0][0]
__________________________________________________________________________________________________
conv2d_383 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_382[0][0]
__________________________________________________________________________________________________
max_pooling2d_81 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_383[0][0]
__________________________________________________________________________________________________
conv2d_384 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_81[0][0]
__________________________________________________________________________________________________
conv2d_385 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_384[0][0]
__________________________________________________________________________________________________
max_pooling2d_82 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_385[0][0]
__________________________________________________________________________________________________
conv2d_386 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_82[0][0]
__________________________________________________________________________________________________
conv2d_387 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_386[0][0]
__________________________________________________________________________________________________
max_pooling2d_83 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_387[0][0]
__________________________________________________________________________________________________
conv2d_388 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_83[0][0]
__________________________________________________________________________________________________
conv2d_389 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_388[0][0]
__________________________________________________________________________________________________
up_sampling2d_80 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_389[0][0]
__________________________________________________________________________________________________
concatenate_80 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_80[0][0]
                                                                 conv2d_387[0][0]
__________________________________________________________________________________________________
conv2d_390 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_80[0][0]
__________________________________________________________________________________________________
conv2d_391 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_390[0][0]
__________________________________________________________________________________________________
up_sampling2d_81 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_391[0][0]
__________________________________________________________________________________________________
concatenate_81 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_81[0][0]
                                                                 conv2d_385[0][0]
__________________________________________________________________________________________________
conv2d_392 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_81[0][0]
__________________________________________________________________________________________________
conv2d_393 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_392[0][0]
__________________________________________________________________________________________________
up_sampling2d_82 (UpSampling2D) (None, 128, 128, 128 0           conv2d_393[0][0]
__________________________________________________________________________________________________
concatenate_82 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_82[0][0]
                                                                 conv2d_383[0][0]
__________________________________________________________________________________________________
conv2d_394 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_82[0][0]
__________________________________________________________________________________________________
conv2d_395 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_394[0][0]
__________________________________________________________________________________________________
up_sampling2d_83 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_395[0][0]
__________________________________________________________________________________________________
concatenate_83 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_83[0][0]
                                                                 conv2d_381[0][0]
__________________________________________________________________________________________________
conv2d_396 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_83[0][0]
__________________________________________________________________________________________________
conv2d_397 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_396[0][0]
__________________________________________________________________________________________________
conv2d_398 (Conv2D)             (None, 256, 256, 1)  33          conv2d_397[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6927 - acc: 0.7240 - precision_20: 0.3299 - auc_20: 0.5007 - recall_20: 0.60152021-09-25 03:18:22.581701: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 03:18:22.581784: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 03:18:23.105046: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 03:18:23.121102: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23
2021-09-25 03:18:23.127235: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23/xeon-09.trace.json.gz
2021-09-25 03:18:23.154952: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23
2021-09-25 03:18:23.164300: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23/xeon-09.memory_profile.json.gz
2021-09-25 03:18:23.181490: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23Dumped tool data for xplane.pb to logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:15 - loss: 0.6906 - acc: 0.6791 - precision_20: 0.3299 - auc_20: 0.3783 - recall_20: 0.1885WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1976s vs `on_train_batch_end` time: 0.4037s). Check your callbacks.
254/254 [==============================] - 57s 223ms/step - loss: 0.4150 - acc: 0.8397 - precision_20: 0.7101 - auc_20: 0.8020 - recall_20: 0.4485 - val_loss: 0.4143 - val_acc: 0.8606 - val_precision_20: 0.8569 - val_auc_20: 0.8562 - val_recall_20: 0.4129
Epoch 2/100
254/254 [==============================] - 56s 219ms/step - loss: 0.3777 - acc: 0.8502 - precision_20: 0.7378 - auc_20: 0.8262 - recall_20: 0.4886 - val_loss: 0.2617 - val_acc: 0.9062 - val_precision_20: 0.9120 - val_auc_20: 0.9172 - val_recall_20: 0.6170
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2908 - acc: 0.8859 - precision_20: 0.8194 - auc_20: 0.8986 - recall_20: 0.6206 - val_loss: 0.2345 - val_acc: 0.9138 - val_precision_20: 0.8138 - val_auc_20: 0.9415 - val_recall_20: 0.7741
Epoch 4/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2504 - acc: 0.8997 - precision_20: 0.8430 - auc_20: 0.9297 - recall_20: 0.6781 - val_loss: 0.2136 - val_acc: 0.9211 - val_precision_20: 0.8213 - val_auc_20: 0.9565 - val_recall_20: 0.8008
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2442 - acc: 0.9022 - precision_20: 0.8452 - auc_20: 0.9346 - recall_20: 0.6911 - val_loss: 0.1943 - val_acc: 0.9261 - val_precision_20: 0.8523 - val_auc_20: 0.9552 - val_recall_20: 0.7756
Epoch 6/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2255 - acc: 0.9089 - precision_20: 0.8571 - auc_20: 0.9431 - recall_20: 0.7134 - val_loss: 0.1931 - val_acc: 0.9267 - val_precision_20: 0.9016 - val_auc_20: 0.9615 - val_recall_20: 0.7430
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2190 - acc: 0.9118 - precision_20: 0.8592 - auc_20: 0.9472 - recall_20: 0.7282 - val_loss: 0.1784 - val_acc: 0.9328 - val_precision_20: 0.9163 - val_auc_20: 0.9647 - val_recall_20: 0.7474
Epoch 8/100
254/254 [==============================] - 56s 219ms/step - loss: 0.2038 - acc: 0.9168 - precision_20: 0.8654 - auc_20: 0.9543 - recall_20: 0.7483 - val_loss: 0.1879 - val_acc: 0.9272 - val_precision_20: 0.8783 - val_auc_20: 0.9645 - val_recall_20: 0.7753
Epoch 9/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1947 - acc: 0.9194 - precision_20: 0.8680 - auc_20: 0.9591 - recall_20: 0.7615 - val_loss: 0.2113 - val_acc: 0.9188 - val_precision_20: 0.9765 - val_auc_20: 0.9579 - val_recall_20: 0.6267
Epoch 10/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1851 - acc: 0.9227 - precision_20: 0.8748 - auc_20: 0.9631 - recall_20: 0.7696 - val_loss: 0.1758 - val_acc: 0.9334 - val_precision_20: 0.9216 - val_auc_20: 0.9657 - val_recall_20: 0.7520
Epoch 11/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1854 - acc: 0.9226 - precision_20: 0.8792 - auc_20: 0.9629 - recall_20: 0.7663 - val_loss: 0.1765 - val_acc: 0.9353 - val_precision_20: 0.8702 - val_auc_20: 0.9685 - val_recall_20: 0.8157
Epoch 12/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1800 - acc: 0.9253 - precision_20: 0.8786 - auc_20: 0.9655 - recall_20: 0.7804 - val_loss: 0.1584 - val_acc: 0.9407 - val_precision_20: 0.8974 - val_auc_20: 0.9714 - val_recall_20: 0.8115
Epoch 13/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1728 - acc: 0.9282 - precision_20: 0.8854 - auc_20: 0.9683 - recall_20: 0.7882 - val_loss: 0.1522 - val_acc: 0.9400 - val_precision_20: 0.8444 - val_auc_20: 0.9786 - val_recall_20: 0.8828
Epoch 14/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1717 - acc: 0.9282 - precision_20: 0.8842 - auc_20: 0.9678 - recall_20: 0.7905 - val_loss: 0.1519 - val_acc: 0.9399 - val_precision_20: 0.9135 - val_auc_20: 0.9741 - val_recall_20: 0.7830
Epoch 15/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1659 - acc: 0.9305 - precision_20: 0.8906 - auc_20: 0.9709 - recall_20: 0.7933 - val_loss: 0.1668 - val_acc: 0.9302 - val_precision_20: 0.8077 - val_auc_20: 0.9780 - val_recall_20: 0.9010
Epoch 16/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1622 - acc: 0.9312 - precision_20: 0.8865 - auc_20: 0.9723 - recall_20: 0.8037 - val_loss: 0.1286 - val_acc: 0.9497 - val_precision_20: 0.9174 - val_auc_20: 0.9810 - val_recall_20: 0.8199
Epoch 17/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1640 - acc: 0.9304 - precision_20: 0.8909 - auc_20: 0.9716 - recall_20: 0.7939 - val_loss: 0.1625 - val_acc: 0.9351 - val_precision_20: 0.8433 - val_auc_20: 0.9758 - val_recall_20: 0.8658
Epoch 18/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1641 - acc: 0.9308 - precision_20: 0.8874 - auc_20: 0.9714 - recall_20: 0.8004 - val_loss: 0.1346 - val_acc: 0.9476 - val_precision_20: 0.9048 - val_auc_20: 0.9808 - val_recall_20: 0.8346
Epoch 19/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1595 - acc: 0.9324 - precision_20: 0.8870 - auc_20: 0.9735 - recall_20: 0.8079 - val_loss: 0.1456 - val_acc: 0.9440 - val_precision_20: 0.9348 - val_auc_20: 0.9798 - val_recall_20: 0.8017
Epoch 20/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1607 - acc: 0.9322 - precision_20: 0.8914 - auc_20: 0.9730 - recall_20: 0.8037 - val_loss: 0.1315 - val_acc: 0.9474 - val_precision_20: 0.8642 - val_auc_20: 0.9789 - val_recall_20: 0.8605
Epoch 21/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1637 - acc: 0.9307 - precision_20: 0.8844 - auc_20: 0.9718 - recall_20: 0.8043 - val_loss: 0.1388 - val_acc: 0.9466 - val_precision_20: 0.8993 - val_auc_20: 0.9833 - val_recall_20: 0.8672
Epoch 22/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1557 - acc: 0.9340 - precision_20: 0.8947 - auc_20: 0.9748 - recall_20: 0.8087 - val_loss: 0.1472 - val_acc: 0.9419 - val_precision_20: 0.8620 - val_auc_20: 0.9772 - val_recall_20: 0.8502
Epoch 23/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1597 - acc: 0.9335 - precision_20: 0.8922 - auc_20: 0.9720 - recall_20: 0.8087 - val_loss: 0.1493 - val_acc: 0.9393 - val_precision_20: 0.8369 - val_auc_20: 0.9799 - val_recall_20: 0.8853
Epoch 24/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1531 - acc: 0.9348 - precision_20: 0.8988 - auc_20: 0.9755 - recall_20: 0.8074 - val_loss: 0.1477 - val_acc: 0.9395 - val_precision_20: 0.8342 - val_auc_20: 0.9798 - val_recall_20: 0.8985
Epoch 25/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1548 - acc: 0.9347 - precision_20: 0.8918 - auc_20: 0.9751 - recall_20: 0.8164 - val_loss: 0.1633 - val_acc: 0.9385 - val_precision_20: 0.8021 - val_auc_20: 0.9863 - val_recall_20: 0.9346
Epoch 26/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1470 - acc: 0.9370 - precision_20: 0.8943 - auc_20: 0.9772 - recall_20: 0.8246 - val_loss: 0.1387 - val_acc: 0.9450 - val_precision_20: 0.8926 - val_auc_20: 0.9797 - val_recall_20: 0.8398
Epoch 27/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1605 - acc: 0.9333 - precision_20: 0.8945 - auc_20: 0.9723 - recall_20: 0.8054 - val_loss: 0.1361 - val_acc: 0.9460 - val_precision_20: 0.9174 - val_auc_20: 0.9814 - val_recall_20: 0.8201
Epoch 28/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1537 - acc: 0.9349 - precision_20: 0.8977 - auc_20: 0.9751 - recall_20: 0.8099 - val_loss: 0.1844 - val_acc: 0.9187 - val_precision_20: 0.7442 - val_auc_20: 0.9826 - val_recall_20: 0.9413
Epoch 29/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1481 - acc: 0.9365 - precision_20: 0.8952 - auc_20: 0.9770 - recall_20: 0.8226 - val_loss: 0.1685 - val_acc: 0.9287 - val_precision_20: 0.7939 - val_auc_20: 0.9814 - val_recall_20: 0.9241
Epoch 30/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1447 - acc: 0.9375 - precision_20: 0.8968 - auc_20: 0.9782 - recall_20: 0.8269 - val_loss: 0.1261 - val_acc: 0.9521 - val_precision_20: 0.9321 - val_auc_20: 0.9836 - val_recall_20: 0.8347
Epoch 31/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1444 - acc: 0.9382 - precision_20: 0.9011 - auc_20: 0.9778 - recall_20: 0.8249 - val_loss: 0.1434 - val_acc: 0.9431 - val_precision_20: 0.8426 - val_auc_20: 0.9803 - val_recall_20: 0.8961
Epoch 32/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1460 - acc: 0.9374 - precision_20: 0.8969 - auc_20: 0.9777 - recall_20: 0.8257 - val_loss: 0.1346 - val_acc: 0.9488 - val_precision_20: 0.8808 - val_auc_20: 0.9838 - val_recall_20: 0.8797
Epoch 33/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1437 - acc: 0.9383 - precision_20: 0.8983 - auc_20: 0.9784 - recall_20: 0.8274 - val_loss: 0.1293 - val_acc: 0.9490 - val_precision_20: 0.8794 - val_auc_20: 0.9823 - val_recall_20: 0.8788
Epoch 34/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1446 - acc: 0.9381 - precision_20: 0.9012 - auc_20: 0.9778 - recall_20: 0.8223 - val_loss: 0.1316 - val_acc: 0.9499 - val_precision_20: 0.8729 - val_auc_20: 0.9814 - val_recall_20: 0.8715
Epoch 35/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1400 - acc: 0.9393 - precision_20: 0.9025 - auc_20: 0.9796 - recall_20: 0.8273 - val_loss: 0.1390 - val_acc: 0.9429 - val_precision_20: 0.8485 - val_auc_20: 0.9847 - val_recall_20: 0.9086
Epoch 36/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1405 - acc: 0.9400 - precision_20: 0.9013 - auc_20: 0.9791 - recall_20: 0.8323 - val_loss: 0.1353 - val_acc: 0.9456 - val_precision_20: 0.9009 - val_auc_20: 0.9795 - val_recall_20: 0.8366
Epoch 37/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1788 - acc: 0.9278 - precision_20: 0.8828 - auc_20: 0.9677 - recall_20: 0.7892 - val_loss: 0.1755 - val_acc: 0.9354 - val_precision_20: 0.9791 - val_auc_20: 0.9789 - val_recall_20: 0.7158
Epoch 38/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1416 - acc: 0.9394 - precision_20: 0.9033 - auc_20: 0.9789 - recall_20: 0.8287 - val_loss: 0.1297 - val_acc: 0.9519 - val_precision_20: 0.8824 - val_auc_20: 0.9832 - val_recall_20: 0.8891
Epoch 39/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1410 - acc: 0.9395 - precision_20: 0.9054 - auc_20: 0.9790 - recall_20: 0.8248 - val_loss: 0.1462 - val_acc: 0.9415 - val_precision_20: 0.8664 - val_auc_20: 0.9800 - val_recall_20: 0.8667
Epoch 40/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1381 - acc: 0.9397 - precision_20: 0.9043 - auc_20: 0.9804 - recall_20: 0.8291 - val_loss: 0.1367 - val_acc: 0.9473 - val_precision_20: 0.9694 - val_auc_20: 0.9824 - val_recall_20: 0.7590
Epoch 41/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1359 - acc: 0.9406 - precision_20: 0.9032 - auc_20: 0.9806 - recall_20: 0.8340 - val_loss: 0.1450 - val_acc: 0.9382 - val_precision_20: 0.8437 - val_auc_20: 0.9804 - val_recall_20: 0.8814
Epoch 42/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1378 - acc: 0.9399 - precision_20: 0.9054 - auc_20: 0.9803 - recall_20: 0.8292 - val_loss: 0.1309 - val_acc: 0.9491 - val_precision_20: 0.8851 - val_auc_20: 0.9833 - val_recall_20: 0.8710
Epoch 43/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1371 - acc: 0.9402 - precision_20: 0.9043 - auc_20: 0.9805 - recall_20: 0.8326 - val_loss: 0.1410 - val_acc: 0.9437 - val_precision_20: 0.8633 - val_auc_20: 0.9823 - val_recall_20: 0.8805
Epoch 44/100
254/254 [==============================] - ETA: 0s - loss: 0.1357 - acc: 0.9413 - precision_20: 0.9058 - auc_20: 0.9811 - recall_20: 0.8344Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 218ms/step - loss: 0.1357 - acc: 0.9413 - precision_20: 0.9058 - auc_20: 0.9811 - recall_20: 0.8344 - val_loss: 0.1352 - val_acc: 0.9472 - val_precision_20: 0.9471 - val_auc_20: 0.9809 - val_recall_20: 0.7956
Epoch 00044: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1398 - acc: 0.9469 - precision_20: 0.9207 - auc_20: 0.9800 - recall_20: 0.8346
---------------TEST METRICS----------------------
jaccard_index 0.7770268811251573
test_sensitivity 0.849272809857098
test_specifitivity 0.9724030801591694
test_accuracy 0.9448709690824468
test_precision 0.8986100979458086
test_jaccard_score 0.7770268811251573
test_dicecoef 0.8732451349475114
isic_eval_score 0.9929078014184397
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-035955.h5
[1.89532812 1.58575624 1.80691469 1.71556183 1.94710051] [0.94487097 0.77702688 0.8986101  0.84927281 0.97240308]

-------------------------
Averaged metrics for Baseline + Augumentations + Per Channel Normalization - lesion: [0.94673303 0.78759437 0.9018416  0.85494488 0.97316786]
-------------------------


-------------------------
RUN: Baseline + Augumentations + Gaussian Blur - lesion, PARAMS: {'augumentation': True, 'gaussian_blur': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-25 03:59:55.984366: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 03:59:55.984485: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied gaussian blur on all input images
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 0.5810265875815174,
Validation samples: 383, channel mean: 0.5863744105403145
Model built.
Model: "functional_43"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_22 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_399 (Conv2D)             (None, 256, 256, 32) 320         input_22[0][0]
__________________________________________________________________________________________________
conv2d_400 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_399[0][0]
__________________________________________________________________________________________________
max_pooling2d_84 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_400[0][0]
__________________________________________________________________________________________________
conv2d_401 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_84[0][0]
__________________________________________________________________________________________________
conv2d_402 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_401[0][0]
__________________________________________________________________________________________________
max_pooling2d_85 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_402[0][0]
__________________________________________________________________________________________________
conv2d_403 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_85[0][0]
__________________________________________________________________________________________________
conv2d_404 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_403[0][0]
__________________________________________________________________________________________________
max_pooling2d_86 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_404[0][0]
__________________________________________________________________________________________________
conv2d_405 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_86[0][0]
__________________________________________________________________________________________________
conv2d_406 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_405[0][0]
__________________________________________________________________________________________________
max_pooling2d_87 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_406[0][0]
__________________________________________________________________________________________________
conv2d_407 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_87[0][0]
__________________________________________________________________________________________________
conv2d_408 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_407[0][0]
__________________________________________________________________________________________________
up_sampling2d_84 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_408[0][0]
__________________________________________________________________________________________________
concatenate_84 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_84[0][0]
                                                                 conv2d_406[0][0]
__________________________________________________________________________________________________
conv2d_409 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_84[0][0]
__________________________________________________________________________________________________
conv2d_410 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_409[0][0]
__________________________________________________________________________________________________
up_sampling2d_85 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_410[0][0]
__________________________________________________________________________________________________
concatenate_85 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_85[0][0]
                                                                 conv2d_404[0][0]
__________________________________________________________________________________________________
conv2d_411 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_85[0][0]
__________________________________________________________________________________________________
conv2d_412 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_411[0][0]
__________________________________________________________________________________________________
up_sampling2d_86 (UpSampling2D) (None, 128, 128, 128 0           conv2d_412[0][0]
__________________________________________________________________________________________________
concatenate_86 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_86[0][0]
                                                                 conv2d_402[0][0]
__________________________________________________________________________________________________
conv2d_413 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_86[0][0]
__________________________________________________________________________________________________
conv2d_414 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_413[0][0]
__________________________________________________________________________________________________
up_sampling2d_87 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_414[0][0]
__________________________________________________________________________________________________
concatenate_87 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_87[0][0]
                                                                 conv2d_400[0][0]
__________________________________________________________________________________________________
conv2d_415 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_87[0][0]
__________________________________________________________________________________________________
conv2d_416 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_415[0][0]
__________________________________________________________________________________________________
conv2d_417 (Conv2D)             (None, 256, 256, 1)  33          conv2d_416[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6926 - acc: 0.7683 - precision_21: 0.2681 - auc_21: 0.5074 - recall_21: 0.21442021-09-25 04:00:34.794123: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 04:00:34.794763: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 04:00:35.319430: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 04:00:35.330743: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35
2021-09-25 04:00:35.334084: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35/xeon-09.trace.json.gz
2021-09-25 04:00:35.361324: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35
2021-09-25 04:00:35.369348: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35/xeon-09.memory_profile.json.gz
2021-09-25 04:00:35.383945: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35Dumped tool data for xplane.pb to logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:14 - loss: 0.6859 - acc: 0.7013 - precision_21: 0.2681 - auc_21: 0.4188 - recall_21: 0.0672WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2071s vs `on_train_batch_end` time: 0.3834s). Check your callbacks.
254/254 [==============================] - 57s 224ms/step - loss: 0.4908 - acc: 0.7966 - precision_21: 0.7713 - auc_21: 0.7735 - recall_21: 0.0889 - val_loss: 0.3949 - val_acc: 0.8518 - val_precision_21: 0.7913 - val_auc_21: 0.7990 - val_recall_21: 0.4102
Epoch 2/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3587 - acc: 0.8499 - precision_21: 0.8039 - auc_21: 0.8476 - recall_21: 0.4126 - val_loss: 0.3437 - val_acc: 0.8728 - val_precision_21: 0.8441 - val_auc_21: 0.8580 - val_recall_21: 0.4906
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3394 - acc: 0.8548 - precision_21: 0.8001 - auc_21: 0.8676 - recall_21: 0.4468 - val_loss: 0.3473 - val_acc: 0.8622 - val_precision_21: 0.7526 - val_auc_21: 0.8610 - val_recall_21: 0.5307
Epoch 4/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3210 - acc: 0.8652 - precision_21: 0.8199 - auc_21: 0.8785 - recall_21: 0.4950 - val_loss: 0.3288 - val_acc: 0.8728 - val_precision_21: 0.8135 - val_auc_21: 0.8704 - val_recall_21: 0.5162
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3077 - acc: 0.8748 - precision_21: 0.8438 - auc_21: 0.8852 - recall_21: 0.5297 - val_loss: 0.3189 - val_acc: 0.8883 - val_precision_21: 0.7793 - val_auc_21: 0.8890 - val_recall_21: 0.6384
Epoch 6/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2873 - acc: 0.8861 - precision_21: 0.8562 - auc_21: 0.8985 - recall_21: 0.5809 - val_loss: 0.2923 - val_acc: 0.8904 - val_precision_21: 0.7825 - val_auc_21: 0.9021 - val_recall_21: 0.6851
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2667 - acc: 0.8952 - precision_21: 0.8496 - auc_21: 0.9137 - recall_21: 0.6435 - val_loss: 0.2637 - val_acc: 0.8999 - val_precision_21: 0.7720 - val_auc_21: 0.9164 - val_recall_21: 0.7406
Epoch 8/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2408 - acc: 0.9058 - precision_21: 0.8619 - auc_21: 0.9315 - recall_21: 0.6900 - val_loss: 0.2185 - val_acc: 0.9220 - val_precision_21: 0.8678 - val_auc_21: 0.9452 - val_recall_21: 0.7600
Epoch 9/100
254/254 [==============================] - 56s 219ms/step - loss: 0.2168 - acc: 0.9135 - precision_21: 0.8649 - auc_21: 0.9474 - recall_21: 0.7320 - val_loss: 0.2343 - val_acc: 0.9119 - val_precision_21: 0.9600 - val_auc_21: 0.9488 - val_recall_21: 0.6038
Epoch 10/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2036 - acc: 0.9178 - precision_21: 0.8725 - auc_21: 0.9536 - recall_21: 0.7445 - val_loss: 0.1806 - val_acc: 0.9319 - val_precision_21: 0.8522 - val_auc_21: 0.9635 - val_recall_21: 0.8243
Epoch 11/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1985 - acc: 0.9191 - precision_21: 0.8787 - auc_21: 0.9572 - recall_21: 0.7477 - val_loss: 0.1853 - val_acc: 0.9291 - val_precision_21: 0.8577 - val_auc_21: 0.9628 - val_recall_21: 0.7966
Epoch 12/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1894 - acc: 0.9226 - precision_21: 0.8855 - auc_21: 0.9610 - recall_21: 0.7571 - val_loss: 0.2183 - val_acc: 0.9091 - val_precision_21: 0.7333 - val_auc_21: 0.9672 - val_recall_21: 0.8945
Epoch 13/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1760 - acc: 0.9271 - precision_21: 0.8858 - auc_21: 0.9674 - recall_21: 0.7816 - val_loss: 0.1659 - val_acc: 0.9376 - val_precision_21: 0.8443 - val_auc_21: 0.9770 - val_recall_21: 0.8696
Epoch 14/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1742 - acc: 0.9267 - precision_21: 0.8834 - auc_21: 0.9682 - recall_21: 0.7825 - val_loss: 0.1575 - val_acc: 0.9394 - val_precision_21: 0.8666 - val_auc_21: 0.9728 - val_recall_21: 0.8347
Epoch 15/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1687 - acc: 0.9289 - precision_21: 0.8891 - auc_21: 0.9704 - recall_21: 0.7859 - val_loss: 0.1768 - val_acc: 0.9286 - val_precision_21: 0.8144 - val_auc_21: 0.9740 - val_recall_21: 0.8795
Epoch 16/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1634 - acc: 0.9315 - precision_21: 0.8913 - auc_21: 0.9722 - recall_21: 0.7984 - val_loss: 0.1404 - val_acc: 0.9459 - val_precision_21: 0.9128 - val_auc_21: 0.9770 - val_recall_21: 0.8035
Epoch 17/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1680 - acc: 0.9299 - precision_21: 0.8915 - auc_21: 0.9701 - recall_21: 0.7903 - val_loss: 0.1575 - val_acc: 0.9393 - val_precision_21: 0.8834 - val_auc_21: 0.9758 - val_recall_21: 0.8339
Epoch 18/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1693 - acc: 0.9293 - precision_21: 0.8877 - auc_21: 0.9701 - recall_21: 0.7914 - val_loss: 0.1593 - val_acc: 0.9407 - val_precision_21: 0.8567 - val_auc_21: 0.9751 - val_recall_21: 0.8566
Epoch 19/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1613 - acc: 0.9316 - precision_21: 0.8869 - auc_21: 0.9731 - recall_21: 0.8038 - val_loss: 0.1419 - val_acc: 0.9451 - val_precision_21: 0.9191 - val_auc_21: 0.9790 - val_recall_21: 0.8232
Epoch 20/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1613 - acc: 0.9314 - precision_21: 0.8939 - auc_21: 0.9732 - recall_21: 0.7964 - val_loss: 0.1449 - val_acc: 0.9405 - val_precision_21: 0.8172 - val_auc_21: 0.9785 - val_recall_21: 0.8880
Epoch 21/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1590 - acc: 0.9328 - precision_21: 0.8903 - auc_21: 0.9736 - recall_21: 0.8077 - val_loss: 0.1529 - val_acc: 0.9412 - val_precision_21: 0.8727 - val_auc_21: 0.9806 - val_recall_21: 0.8746
Epoch 22/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1571 - acc: 0.9338 - precision_21: 0.8942 - auc_21: 0.9742 - recall_21: 0.8084 - val_loss: 0.1557 - val_acc: 0.9409 - val_precision_21: 0.8578 - val_auc_21: 0.9763 - val_recall_21: 0.8503
Epoch 23/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1535 - acc: 0.9356 - precision_21: 0.8988 - auc_21: 0.9748 - recall_21: 0.8115 - val_loss: 0.1747 - val_acc: 0.9266 - val_precision_21: 0.7909 - val_auc_21: 0.9739 - val_recall_21: 0.8876
Epoch 24/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1532 - acc: 0.9348 - precision_21: 0.8977 - auc_21: 0.9760 - recall_21: 0.8093 - val_loss: 0.1679 - val_acc: 0.9320 - val_precision_21: 0.8084 - val_auc_21: 0.9785 - val_recall_21: 0.8981
Epoch 25/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1571 - acc: 0.9338 - precision_21: 0.8935 - auc_21: 0.9738 - recall_21: 0.8095 - val_loss: 0.1719 - val_acc: 0.9379 - val_precision_21: 0.8198 - val_auc_21: 0.9798 - val_recall_21: 0.8983
Epoch 26/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1498 - acc: 0.9358 - precision_21: 0.8983 - auc_21: 0.9771 - recall_21: 0.8130 - val_loss: 0.1406 - val_acc: 0.9443 - val_precision_21: 0.8895 - val_auc_21: 0.9776 - val_recall_21: 0.8399
Epoch 27/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1558 - acc: 0.9346 - precision_21: 0.8974 - auc_21: 0.9743 - recall_21: 0.8085 - val_loss: 0.1343 - val_acc: 0.9476 - val_precision_21: 0.8855 - val_auc_21: 0.9820 - val_recall_21: 0.8657
Epoch 28/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1501 - acc: 0.9360 - precision_21: 0.8984 - auc_21: 0.9768 - recall_21: 0.8149 - val_loss: 0.1555 - val_acc: 0.9445 - val_precision_21: 0.8740 - val_auc_21: 0.9772 - val_recall_21: 0.8635
Epoch 29/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1514 - acc: 0.9359 - precision_21: 0.9013 - auc_21: 0.9761 - recall_21: 0.8117 - val_loss: 0.1777 - val_acc: 0.9267 - val_precision_21: 0.8089 - val_auc_21: 0.9737 - val_recall_21: 0.8842
Epoch 30/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1482 - acc: 0.9365 - precision_21: 0.9021 - auc_21: 0.9773 - recall_21: 0.8148 - val_loss: 0.1298 - val_acc: 0.9492 - val_precision_21: 0.9108 - val_auc_21: 0.9827 - val_recall_21: 0.8425
Epoch 31/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1472 - acc: 0.9374 - precision_21: 0.9024 - auc_21: 0.9776 - recall_21: 0.8181 - val_loss: 0.1624 - val_acc: 0.9347 - val_precision_21: 0.8054 - val_auc_21: 0.9798 - val_recall_21: 0.9081
Epoch 32/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1521 - acc: 0.9357 - precision_21: 0.8974 - auc_21: 0.9756 - recall_21: 0.8161 - val_loss: 0.1449 - val_acc: 0.9424 - val_precision_21: 0.8538 - val_auc_21: 0.9811 - val_recall_21: 0.8815
Epoch 33/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1561 - acc: 0.9344 - precision_21: 0.8971 - auc_21: 0.9744 - recall_21: 0.8072 - val_loss: 0.1338 - val_acc: 0.9506 - val_precision_21: 0.9140 - val_auc_21: 0.9819 - val_recall_21: 0.8456
Epoch 34/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1482 - acc: 0.9373 - precision_21: 0.9005 - auc_21: 0.9770 - recall_21: 0.8188 - val_loss: 0.1327 - val_acc: 0.9505 - val_precision_21: 0.8956 - val_auc_21: 0.9807 - val_recall_21: 0.8458
Epoch 35/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1411 - acc: 0.9390 - precision_21: 0.9030 - auc_21: 0.9798 - recall_21: 0.8253 - val_loss: 0.1466 - val_acc: 0.9388 - val_precision_21: 0.8440 - val_auc_21: 0.9820 - val_recall_21: 0.8933
Epoch 36/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1461 - acc: 0.9375 - precision_21: 0.9010 - auc_21: 0.9779 - recall_21: 0.8188 - val_loss: 0.1310 - val_acc: 0.9472 - val_precision_21: 0.8868 - val_auc_21: 0.9815 - val_recall_21: 0.8618
Epoch 37/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1443 - acc: 0.9381 - precision_21: 0.9024 - auc_21: 0.9785 - recall_21: 0.8216 - val_loss: 0.1736 - val_acc: 0.9386 - val_precision_21: 0.9214 - val_auc_21: 0.9713 - val_recall_21: 0.7824
Epoch 38/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1418 - acc: 0.9393 - precision_21: 0.9038 - auc_21: 0.9792 - recall_21: 0.8272 - val_loss: 0.1363 - val_acc: 0.9473 - val_precision_21: 0.8888 - val_auc_21: 0.9786 - val_recall_21: 0.8558
Epoch 39/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1415 - acc: 0.9390 - precision_21: 0.9047 - auc_21: 0.9794 - recall_21: 0.8234 - val_loss: 0.1457 - val_acc: 0.9411 - val_precision_21: 0.8906 - val_auc_21: 0.9782 - val_recall_21: 0.8335
Epoch 40/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1397 - acc: 0.9398 - precision_21: 0.9057 - auc_21: 0.9803 - recall_21: 0.8283 - val_loss: 0.1269 - val_acc: 0.9516 - val_precision_21: 0.9214 - val_auc_21: 0.9810 - val_recall_21: 0.8271
Epoch 41/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1428 - acc: 0.9390 - precision_21: 0.9003 - auc_21: 0.9788 - recall_21: 0.8291 - val_loss: 0.1593 - val_acc: 0.9327 - val_precision_21: 0.8229 - val_auc_21: 0.9779 - val_recall_21: 0.8829
Epoch 42/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1424 - acc: 0.9384 - precision_21: 0.9046 - auc_21: 0.9795 - recall_21: 0.8224 - val_loss: 0.1392 - val_acc: 0.9466 - val_precision_21: 0.8746 - val_auc_21: 0.9827 - val_recall_21: 0.8710
Epoch 43/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1435 - acc: 0.9388 - precision_21: 0.9031 - auc_21: 0.9791 - recall_21: 0.8264 - val_loss: 0.1380 - val_acc: 0.9472 - val_precision_21: 0.9025 - val_auc_21: 0.9796 - val_recall_21: 0.8490
Epoch 44/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1383 - acc: 0.9406 - precision_21: 0.9073 - auc_21: 0.9805 - recall_21: 0.8291 - val_loss: 0.1234 - val_acc: 0.9520 - val_precision_21: 0.9280 - val_auc_21: 0.9825 - val_recall_21: 0.8388
Epoch 45/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1409 - acc: 0.9397 - precision_21: 0.9051 - auc_21: 0.9794 - recall_21: 0.8277 - val_loss: 0.1313 - val_acc: 0.9479 - val_precision_21: 0.9004 - val_auc_21: 0.9814 - val_recall_21: 0.8511
Epoch 46/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1374 - acc: 0.9408 - precision_21: 0.9065 - auc_21: 0.9806 - recall_21: 0.8310 - val_loss: 0.1307 - val_acc: 0.9471 - val_precision_21: 0.8651 - val_auc_21: 0.9825 - val_recall_21: 0.8867
Epoch 47/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1385 - acc: 0.9403 - precision_21: 0.9076 - auc_21: 0.9802 - recall_21: 0.8273 - val_loss: 0.1189 - val_acc: 0.9532 - val_precision_21: 0.9090 - val_auc_21: 0.9844 - val_recall_21: 0.8686
Epoch 48/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1394 - acc: 0.9406 - precision_21: 0.9051 - auc_21: 0.9798 - recall_21: 0.8329 - val_loss: 0.1311 - val_acc: 0.9479 - val_precision_21: 0.9535 - val_auc_21: 0.9826 - val_recall_21: 0.7902
Epoch 49/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1345 - acc: 0.9423 - precision_21: 0.9128 - auc_21: 0.9813 - recall_21: 0.8314 - val_loss: 0.1560 - val_acc: 0.9363 - val_precision_21: 0.8150 - val_auc_21: 0.9815 - val_recall_21: 0.9058
Epoch 50/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1369 - acc: 0.9402 - precision_21: 0.9052 - auc_21: 0.9810 - recall_21: 0.8289 - val_loss: 0.1409 - val_acc: 0.9421 - val_precision_21: 0.8489 - val_auc_21: 0.9805 - val_recall_21: 0.8860
Epoch 51/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1471 - acc: 0.9378 - precision_21: 0.9044 - auc_21: 0.9778 - recall_21: 0.8187 - val_loss: 0.1454 - val_acc: 0.9417 - val_precision_21: 0.8545 - val_auc_21: 0.9796 - val_recall_21: 0.8709
Epoch 52/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1386 - acc: 0.9400 - precision_21: 0.8992 - auc_21: 0.9803 - recall_21: 0.8362 - val_loss: 0.1563 - val_acc: 0.9479 - val_precision_21: 0.8786 - val_auc_21: 0.9807 - val_recall_21: 0.8772
Epoch 53/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1357 - acc: 0.9412 - precision_21: 0.9090 - auc_21: 0.9811 - recall_21: 0.8302 - val_loss: 0.1386 - val_acc: 0.9443 - val_precision_21: 0.8558 - val_auc_21: 0.9804 - val_recall_21: 0.8852
Epoch 54/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1365 - acc: 0.9415 - precision_21: 0.9068 - auc_21: 0.9809 - recall_21: 0.8359 - val_loss: 0.1436 - val_acc: 0.9416 - val_precision_21: 0.8475 - val_auc_21: 0.9812 - val_recall_21: 0.8860
Epoch 55/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1351 - acc: 0.9412 - precision_21: 0.9074 - auc_21: 0.9815 - recall_21: 0.8316 - val_loss: 0.1133 - val_acc: 0.9554 - val_precision_21: 0.8990 - val_auc_21: 0.9859 - val_recall_21: 0.8861
Epoch 56/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1300 - acc: 0.9428 - precision_21: 0.9130 - auc_21: 0.9828 - recall_21: 0.8336 - val_loss: 0.1349 - val_acc: 0.9442 - val_precision_21: 0.8589 - val_auc_21: 0.9823 - val_recall_21: 0.8893
Epoch 57/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1335 - acc: 0.9422 - precision_21: 0.9072 - auc_21: 0.9817 - recall_21: 0.8378 - val_loss: 0.1347 - val_acc: 0.9459 - val_precision_21: 0.8705 - val_auc_21: 0.9814 - val_recall_21: 0.8717
Epoch 58/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1309 - acc: 0.9431 - precision_21: 0.9092 - auc_21: 0.9827 - recall_21: 0.8412 - val_loss: 0.1360 - val_acc: 0.9498 - val_precision_21: 0.9342 - val_auc_21: 0.9789 - val_recall_21: 0.8119
Epoch 59/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1368 - acc: 0.9410 - precision_21: 0.9094 - auc_21: 0.9808 - recall_21: 0.8282 - val_loss: 0.2174 - val_acc: 0.9164 - val_precision_21: 0.7683 - val_auc_21: 0.9715 - val_recall_21: 0.8956
Epoch 60/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1341 - acc: 0.9419 - precision_21: 0.9072 - auc_21: 0.9817 - recall_21: 0.8362 - val_loss: 0.1191 - val_acc: 0.9518 - val_precision_21: 0.8952 - val_auc_21: 0.9849 - val_recall_21: 0.8757
Epoch 61/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1320 - acc: 0.9428 - precision_21: 0.9093 - auc_21: 0.9823 - recall_21: 0.8387 - val_loss: 0.1294 - val_acc: 0.9505 - val_precision_21: 0.8877 - val_auc_21: 0.9821 - val_recall_21: 0.8783
Epoch 62/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1381 - acc: 0.9414 - precision_21: 0.9087 - auc_21: 0.9804 - recall_21: 0.8319 - val_loss: 0.1245 - val_acc: 0.9474 - val_precision_21: 0.8752 - val_auc_21: 0.9841 - val_recall_21: 0.8728
Epoch 63/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1345 - acc: 0.9417 - precision_21: 0.9072 - auc_21: 0.9813 - recall_21: 0.8341 - val_loss: 0.1189 - val_acc: 0.9522 - val_precision_21: 0.8999 - val_auc_21: 0.9846 - val_recall_21: 0.8684
Epoch 64/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1326 - acc: 0.9429 - precision_21: 0.9075 - auc_21: 0.9822 - recall_21: 0.8406 - val_loss: 0.1355 - val_acc: 0.9483 - val_precision_21: 0.9607 - val_auc_21: 0.9823 - val_recall_21: 0.7977
Epoch 65/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1370 - acc: 0.9418 - precision_21: 0.9124 - auc_21: 0.9801 - recall_21: 0.8295 - val_loss: 0.1319 - val_acc: 0.9498 - val_precision_21: 0.8979 - val_auc_21: 0.9779 - val_recall_21: 0.8441
Epoch 66/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1323 - acc: 0.9430 - precision_21: 0.9099 - auc_21: 0.9816 - recall_21: 0.8395 - val_loss: 0.1214 - val_acc: 0.9505 - val_precision_21: 0.9219 - val_auc_21: 0.9855 - val_recall_21: 0.8496
Epoch 67/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1324 - acc: 0.9420 - precision_21: 0.9057 - auc_21: 0.9823 - recall_21: 0.8383 - val_loss: 0.1502 - val_acc: 0.9430 - val_precision_21: 0.9027 - val_auc_21: 0.9732 - val_recall_21: 0.8098
Epoch 68/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1367 - acc: 0.9418 - precision_21: 0.9079 - auc_21: 0.9805 - recall_21: 0.8341 - val_loss: 0.1229 - val_acc: 0.9501 - val_precision_21: 0.8848 - val_auc_21: 0.9838 - val_recall_21: 0.8751
Epoch 69/100
254/254 [==============================] - ETA: 0s - loss: 0.1286 - acc: 0.9436 - precision_21: 0.9128 - auc_21: 0.9834 - recall_21: 0.8394Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 218ms/step - loss: 0.1286 - acc: 0.9436 - precision_21: 0.9128 - auc_21: 0.9834 - recall_21: 0.8394 - val_loss: 0.1531 - val_acc: 0.9378 - val_precision_21: 0.8231 - val_auc_21: 0.9805 - val_recall_21: 0.8986
Epoch 00069: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1370 - acc: 0.9473 - precision_21: 0.8873 - auc_21: 0.9815 - recall_21: 0.8757
---------------TEST METRICS----------------------
jaccard_index 0.78562437348485
test_sensitivity 0.8782135280282972
test_specifitivity 0.9651565200259061
test_accuracy 0.945715938054078
test_precision 0.8789179533268864
test_jaccard_score 0.78562437348485
test_dicecoef 0.8785655994772871
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-050520.h5
[0. 0. 0. 0. 0.] [0.94571594 0.78562437 0.87891795 0.87821353 0.96515652]

-------------------------
Rep: 1
-------------------------

2021-09-25 05:05:20.750998: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 05:05:20.751131: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied gaussian blur on all input images
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 0.5810265875815174,
Validation samples: 383, channel mean: 0.5863744105403145
Model built.
Model: "functional_45"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_23 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_418 (Conv2D)             (None, 256, 256, 32) 320         input_23[0][0]
__________________________________________________________________________________________________
conv2d_419 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_418[0][0]
__________________________________________________________________________________________________
max_pooling2d_88 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_419[0][0]
__________________________________________________________________________________________________
conv2d_420 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_88[0][0]
__________________________________________________________________________________________________
conv2d_421 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_420[0][0]
__________________________________________________________________________________________________
max_pooling2d_89 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_421[0][0]
__________________________________________________________________________________________________
conv2d_422 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_89[0][0]
__________________________________________________________________________________________________
conv2d_423 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_422[0][0]
__________________________________________________________________________________________________
max_pooling2d_90 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_423[0][0]
__________________________________________________________________________________________________
conv2d_424 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_90[0][0]
__________________________________________________________________________________________________
conv2d_425 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_424[0][0]
__________________________________________________________________________________________________
max_pooling2d_91 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_425[0][0]
__________________________________________________________________________________________________
conv2d_426 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_91[0][0]
__________________________________________________________________________________________________
conv2d_427 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_426[0][0]
__________________________________________________________________________________________________
up_sampling2d_88 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_427[0][0]
__________________________________________________________________________________________________
concatenate_88 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_88[0][0]
                                                                 conv2d_425[0][0]
__________________________________________________________________________________________________
conv2d_428 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_88[0][0]
__________________________________________________________________________________________________
conv2d_429 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_428[0][0]
__________________________________________________________________________________________________
up_sampling2d_89 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_429[0][0]
__________________________________________________________________________________________________
concatenate_89 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_89[0][0]
                                                                 conv2d_423[0][0]
__________________________________________________________________________________________________
conv2d_430 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_89[0][0]
__________________________________________________________________________________________________
conv2d_431 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_430[0][0]
__________________________________________________________________________________________________
up_sampling2d_90 (UpSampling2D) (None, 128, 128, 128 0           conv2d_431[0][0]
__________________________________________________________________________________________________
concatenate_90 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_90[0][0]
                                                                 conv2d_421[0][0]
__________________________________________________________________________________________________
conv2d_432 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_90[0][0]
__________________________________________________________________________________________________
conv2d_433 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_432[0][0]
__________________________________________________________________________________________________
up_sampling2d_91 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_433[0][0]
__________________________________________________________________________________________________
concatenate_91 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_91[0][0]
                                                                 conv2d_419[0][0]
__________________________________________________________________________________________________
conv2d_434 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_91[0][0]
__________________________________________________________________________________________________
conv2d_435 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_434[0][0]
__________________________________________________________________________________________________
conv2d_436 (Conv2D)             (None, 256, 256, 1)  33          conv2d_435[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6988 - acc: 0.1748 - precision_22: 0.1705 - auc_22: 0.5494 - recall_22: 0.99912021-09-25 05:05:59.379492: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 05:05:59.380144: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 05:05:59.899101: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 05:05:59.911883: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59
2021-09-25 05:05:59.915426: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59/xeon-09.trace.json.gz
2021-09-25 05:05:59.950378: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59
2021-09-25 05:05:59.959171: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59/xeon-09.memory_profile.json.gz
2021-09-25 05:05:59.976089: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59Dumped tool data for xplane.pb to logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:15 - loss: 0.6916 - acc: 0.4043 - precision_22: 0.1704 - auc_22: 0.4187 - recall_22: 0.3130WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1993s vs `on_train_batch_end` time: 0.3988s). Check your callbacks.
254/254 [==============================] - 57s 224ms/step - loss: 0.4697 - acc: 0.8063 - precision_22: 0.7005 - auc_22: 0.7959 - recall_22: 0.1916 - val_loss: 0.3429 - val_acc: 0.8637 - val_precision_22: 0.8986 - val_auc_22: 0.8648 - val_recall_22: 0.4037
Epoch 2/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3461 - acc: 0.8505 - precision_22: 0.8030 - auc_22: 0.8643 - recall_22: 0.4189 - val_loss: 0.3663 - val_acc: 0.8490 - val_precision_22: 0.9187 - val_auc_22: 0.8539 - val_recall_22: 0.3157
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3267 - acc: 0.8603 - precision_22: 0.8399 - auc_22: 0.8745 - recall_22: 0.4454 - val_loss: 0.3338 - val_acc: 0.8700 - val_precision_22: 0.7302 - val_auc_22: 0.8808 - val_recall_22: 0.6224
Epoch 4/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2973 - acc: 0.8804 - precision_22: 0.8439 - auc_22: 0.8922 - recall_22: 0.5623 - val_loss: 0.2687 - val_acc: 0.9027 - val_precision_22: 0.8422 - val_auc_22: 0.9097 - val_recall_22: 0.6639
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2951 - acc: 0.8818 - precision_22: 0.8404 - auc_22: 0.8959 - recall_22: 0.5751 - val_loss: 0.2974 - val_acc: 0.8883 - val_precision_22: 0.7699 - val_auc_22: 0.8945 - val_recall_22: 0.6525
Epoch 6/100
254/254 [==============================] - 56s 221ms/step - loss: 0.2830 - acc: 0.8890 - precision_22: 0.8421 - auc_22: 0.9028 - recall_22: 0.6143 - val_loss: 0.2780 - val_acc: 0.8947 - val_precision_22: 0.7843 - val_auc_22: 0.9173 - val_recall_22: 0.7096
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2536 - acc: 0.9014 - precision_22: 0.8558 - auc_22: 0.9240 - recall_22: 0.6722 - val_loss: 0.2163 - val_acc: 0.9193 - val_precision_22: 0.8407 - val_auc_22: 0.9410 - val_recall_22: 0.7581
Epoch 8/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2315 - acc: 0.9092 - precision_22: 0.8585 - auc_22: 0.9364 - recall_22: 0.7136 - val_loss: 0.2478 - val_acc: 0.9162 - val_precision_22: 0.8613 - val_auc_22: 0.9170 - val_recall_22: 0.7367
Epoch 9/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2143 - acc: 0.9146 - precision_22: 0.8652 - auc_22: 0.9478 - recall_22: 0.7375 - val_loss: 0.2390 - val_acc: 0.9190 - val_precision_22: 0.9303 - val_auc_22: 0.9473 - val_recall_22: 0.6620
Epoch 10/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2025 - acc: 0.9177 - precision_22: 0.8714 - auc_22: 0.9548 - recall_22: 0.7453 - val_loss: 0.1891 - val_acc: 0.9290 - val_precision_22: 0.8654 - val_auc_22: 0.9566 - val_recall_22: 0.7905
Epoch 11/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1895 - acc: 0.9218 - precision_22: 0.8744 - auc_22: 0.9612 - recall_22: 0.7676 - val_loss: 0.1719 - val_acc: 0.9337 - val_precision_22: 0.8968 - val_auc_22: 0.9672 - val_recall_22: 0.7756
Epoch 12/100
254/254 [==============================] - 56s 221ms/step - loss: 0.1866 - acc: 0.9226 - precision_22: 0.8817 - auc_22: 0.9625 - recall_22: 0.7619 - val_loss: 0.1682 - val_acc: 0.9373 - val_precision_22: 0.8414 - val_auc_22: 0.9720 - val_recall_22: 0.8660
Epoch 13/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1763 - acc: 0.9276 - precision_22: 0.8844 - auc_22: 0.9667 - recall_22: 0.7864 - val_loss: 0.1691 - val_acc: 0.9334 - val_precision_22: 0.8143 - val_auc_22: 0.9780 - val_recall_22: 0.8932
Epoch 14/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1735 - acc: 0.9276 - precision_22: 0.8834 - auc_22: 0.9674 - recall_22: 0.7876 - val_loss: 0.1714 - val_acc: 0.9324 - val_precision_22: 0.8906 - val_auc_22: 0.9660 - val_recall_22: 0.7663
Epoch 15/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1699 - acc: 0.9295 - precision_22: 0.8881 - auc_22: 0.9699 - recall_22: 0.7904 - val_loss: 0.1787 - val_acc: 0.9299 - val_precision_22: 0.8153 - val_auc_22: 0.9746 - val_recall_22: 0.8856
Epoch 16/100
254/254 [==============================] - 56s 221ms/step - loss: 0.1673 - acc: 0.9293 - precision_22: 0.8846 - auc_22: 0.9708 - recall_22: 0.7956 - val_loss: 0.1394 - val_acc: 0.9451 - val_precision_22: 0.9329 - val_auc_22: 0.9776 - val_recall_22: 0.7788
Epoch 17/100
254/254 [==============================] - 56s 221ms/step - loss: 0.1634 - acc: 0.9306 - precision_22: 0.8937 - auc_22: 0.9723 - recall_22: 0.7916 - val_loss: 0.1589 - val_acc: 0.9367 - val_precision_22: 0.8623 - val_auc_22: 0.9766 - val_recall_22: 0.8473
Epoch 18/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1584 - acc: 0.9332 - precision_22: 0.8936 - auc_22: 0.9740 - recall_22: 0.8056 - val_loss: 0.1386 - val_acc: 0.9492 - val_precision_22: 0.9235 - val_auc_22: 0.9769 - val_recall_22: 0.8223
Epoch 19/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1605 - acc: 0.9321 - precision_22: 0.8916 - auc_22: 0.9733 - recall_22: 0.8012 - val_loss: 0.1626 - val_acc: 0.9378 - val_precision_22: 0.8784 - val_auc_22: 0.9742 - val_recall_22: 0.8331
Epoch 20/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1574 - acc: 0.9325 - precision_22: 0.8922 - auc_22: 0.9749 - recall_22: 0.8042 - val_loss: 0.1312 - val_acc: 0.9480 - val_precision_22: 0.8563 - val_auc_22: 0.9802 - val_recall_22: 0.8753
Epoch 21/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1546 - acc: 0.9351 - precision_22: 0.8932 - auc_22: 0.9754 - recall_22: 0.8164 - val_loss: 0.1495 - val_acc: 0.9424 - val_precision_22: 0.8745 - val_auc_22: 0.9813 - val_recall_22: 0.8782
Epoch 22/100
254/254 [==============================] - 56s 221ms/step - loss: 0.1473 - acc: 0.9371 - precision_22: 0.9036 - auc_22: 0.9772 - recall_22: 0.8148 - val_loss: 0.1578 - val_acc: 0.9404 - val_precision_22: 0.8441 - val_auc_22: 0.9772 - val_recall_22: 0.8669
Epoch 23/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1509 - acc: 0.9360 - precision_22: 0.8983 - auc_22: 0.9759 - recall_22: 0.8149 - val_loss: 0.1740 - val_acc: 0.9272 - val_precision_22: 0.7899 - val_auc_22: 0.9765 - val_recall_22: 0.8931
Epoch 24/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1485 - acc: 0.9364 - precision_22: 0.8999 - auc_22: 0.9774 - recall_22: 0.8154 - val_loss: 0.1453 - val_acc: 0.9406 - val_precision_22: 0.8443 - val_auc_22: 0.9803 - val_recall_22: 0.8887
Epoch 25/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1512 - acc: 0.9368 - precision_22: 0.8978 - auc_22: 0.9767 - recall_22: 0.8200 - val_loss: 0.1650 - val_acc: 0.9432 - val_precision_22: 0.8338 - val_auc_22: 0.9831 - val_recall_22: 0.9071
Epoch 26/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1419 - acc: 0.9386 - precision_22: 0.9034 - auc_22: 0.9793 - recall_22: 0.8220 - val_loss: 0.1383 - val_acc: 0.9447 - val_precision_22: 0.9071 - val_auc_22: 0.9788 - val_recall_22: 0.8214
Epoch 27/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1460 - acc: 0.9384 - precision_22: 0.9078 - auc_22: 0.9778 - recall_22: 0.8170 - val_loss: 0.1454 - val_acc: 0.9502 - val_precision_22: 0.9323 - val_auc_22: 0.9822 - val_recall_22: 0.8258
Epoch 28/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1456 - acc: 0.9375 - precision_22: 0.9033 - auc_22: 0.9788 - recall_22: 0.8171 - val_loss: 0.2016 - val_acc: 0.9185 - val_precision_22: 0.7512 - val_auc_22: 0.9773 - val_recall_22: 0.9223
Epoch 29/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1449 - acc: 0.9377 - precision_22: 0.9002 - auc_22: 0.9785 - recall_22: 0.8231 - val_loss: 0.1850 - val_acc: 0.9169 - val_precision_22: 0.7589 - val_auc_22: 0.9776 - val_recall_22: 0.9260
Epoch 30/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1437 - acc: 0.9382 - precision_22: 0.8989 - auc_22: 0.9787 - recall_22: 0.8277 - val_loss: 0.1322 - val_acc: 0.9459 - val_precision_22: 0.8897 - val_auc_22: 0.9826 - val_recall_22: 0.8498
Epoch 31/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1393 - acc: 0.9404 - precision_22: 0.9085 - auc_22: 0.9800 - recall_22: 0.8269 - val_loss: 0.1554 - val_acc: 0.9388 - val_precision_22: 0.8241 - val_auc_22: 0.9803 - val_recall_22: 0.9005
Epoch 32/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1443 - acc: 0.9381 - precision_22: 0.9005 - auc_22: 0.9785 - recall_22: 0.8251 - val_loss: 0.1454 - val_acc: 0.9415 - val_precision_22: 0.8342 - val_auc_22: 0.9827 - val_recall_22: 0.9069
Epoch 33/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1432 - acc: 0.9391 - precision_22: 0.9011 - auc_22: 0.9788 - recall_22: 0.8280 - val_loss: 0.1402 - val_acc: 0.9416 - val_precision_22: 0.8414 - val_auc_22: 0.9813 - val_recall_22: 0.8909
Epoch 34/100
254/254 [==============================] - ETA: 0s - loss: 0.1436 - acc: 0.9392 - precision_22: 0.9030 - auc_22: 0.9789 - recall_22: 0.8262Restoring model weights from the end of the best epoch.
254/254 [==============================] - 56s 220ms/step - loss: 0.1436 - acc: 0.9392 - precision_22: 0.9030 - auc_22: 0.9789 - recall_22: 0.8262 - val_loss: 0.1421 - val_acc: 0.9458 - val_precision_22: 0.8543 - val_auc_22: 0.9817 - val_recall_22: 0.8725
Epoch 00034: early stopping
36/36 [==============================] - 3s 76ms/step - loss: 0.1465 - acc: 0.9435 - precision_22: 0.8765 - auc_22: 0.9779 - recall_22: 0.8698
---------------TEST METRICS----------------------
jaccard_index 0.7573034781752033
test_sensitivity 0.8742567599124289
test_specifitivity 0.9604772935099035
test_accuracy 0.9411982543079566
test_precision 0.8643263664052484
test_jaccard_score 0.7573034781752033
test_dicecoef 0.869263203078232
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-053820.h5
[0.94571594 0.78562437 0.87891795 0.87821353 0.96515652] [0.94119825 0.75730348 0.86432637 0.87425676 0.96047729]

-------------------------
Rep: 2
-------------------------

2021-09-25 05:38:21.741793: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 05:38:21.742034: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied gaussian blur on all input images
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 0.5810265875815174,
Validation samples: 383, channel mean: 0.5863744105403145
Model built.
Model: "functional_47"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_24 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_437 (Conv2D)             (None, 256, 256, 32) 320         input_24[0][0]
__________________________________________________________________________________________________
conv2d_438 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_437[0][0]
__________________________________________________________________________________________________
max_pooling2d_92 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_438[0][0]
__________________________________________________________________________________________________
conv2d_439 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_92[0][0]
__________________________________________________________________________________________________
conv2d_440 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_439[0][0]
__________________________________________________________________________________________________
max_pooling2d_93 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_440[0][0]
__________________________________________________________________________________________________
conv2d_441 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_93[0][0]
__________________________________________________________________________________________________
conv2d_442 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_441[0][0]
__________________________________________________________________________________________________
max_pooling2d_94 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_442[0][0]
__________________________________________________________________________________________________
conv2d_443 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_94[0][0]
__________________________________________________________________________________________________
conv2d_444 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_443[0][0]
__________________________________________________________________________________________________
max_pooling2d_95 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_444[0][0]
__________________________________________________________________________________________________
conv2d_445 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_95[0][0]
__________________________________________________________________________________________________
conv2d_446 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_445[0][0]
__________________________________________________________________________________________________
up_sampling2d_92 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_446[0][0]
__________________________________________________________________________________________________
concatenate_92 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_92[0][0]
                                                                 conv2d_444[0][0]
__________________________________________________________________________________________________
conv2d_447 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_92[0][0]
__________________________________________________________________________________________________
conv2d_448 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_447[0][0]
__________________________________________________________________________________________________
up_sampling2d_93 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_448[0][0]
__________________________________________________________________________________________________
concatenate_93 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_93[0][0]
                                                                 conv2d_442[0][0]
__________________________________________________________________________________________________
conv2d_449 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_93[0][0]
__________________________________________________________________________________________________
conv2d_450 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_449[0][0]
__________________________________________________________________________________________________
up_sampling2d_94 (UpSampling2D) (None, 128, 128, 128 0           conv2d_450[0][0]
__________________________________________________________________________________________________
concatenate_94 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_94[0][0]
                                                                 conv2d_440[0][0]
__________________________________________________________________________________________________
conv2d_451 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_94[0][0]
__________________________________________________________________________________________________
conv2d_452 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_451[0][0]
__________________________________________________________________________________________________
up_sampling2d_95 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_452[0][0]
__________________________________________________________________________________________________
concatenate_95 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_95[0][0]
                                                                 conv2d_438[0][0]
__________________________________________________________________________________________________
conv2d_453 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_95[0][0]
__________________________________________________________________________________________________
conv2d_454 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_453[0][0]
__________________________________________________________________________________________________
conv2d_455 (Conv2D)             (None, 256, 256, 1)  33          conv2d_454[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6898 - acc: 0.8111 - precision_23: 0.1230 - auc_23: 0.6209 - recall_23: 0.02142021-09-25 05:39:00.169328: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 05:39:00.169941: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 05:39:00.678975: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 05:39:00.689556: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00
2021-09-25 05:39:00.695352: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00/xeon-09.trace.json.gz
2021-09-25 05:39:00.723380: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00
2021-09-25 05:39:00.731551: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00/xeon-09.memory_profile.json.gz
2021-09-25 05:39:00.744915: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00Dumped tool data for xplane.pb to logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:12 - loss: 0.6784 - acc: 0.7226 - precision_23: 0.1230 - auc_23: 0.4400 - recall_23: 0.0067WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1959s vs `on_train_batch_end` time: 0.3807s). Check your callbacks.
254/254 [==============================] - 57s 224ms/step - loss: 0.4124 - acc: 0.8130 - precision_23: 0.7581 - auc_23: 0.8026 - recall_23: 0.2053 - val_loss: 0.3529 - val_acc: 0.8605 - val_precision_23: 0.7310 - val_auc_23: 0.8533 - val_recall_23: 0.5430
Epoch 2/100
254/254 [==============================] - 56s 221ms/step - loss: 0.3286 - acc: 0.8617 - precision_23: 0.8130 - auc_23: 0.8729 - recall_23: 0.4788 - val_loss: 0.3405 - val_acc: 0.8731 - val_precision_23: 0.7724 - val_auc_23: 0.8719 - val_recall_23: 0.5691
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3167 - acc: 0.8679 - precision_23: 0.8338 - auc_23: 0.8813 - recall_23: 0.4965 - val_loss: 0.3338 - val_acc: 0.8735 - val_precision_23: 0.7703 - val_auc_23: 0.8781 - val_recall_23: 0.5825
Epoch 4/100
254/254 [==============================] - 56s 221ms/step - loss: 0.2999 - acc: 0.8788 - precision_23: 0.8432 - auc_23: 0.8920 - recall_23: 0.5535 - val_loss: 0.2932 - val_acc: 0.8887 - val_precision_23: 0.8320 - val_auc_23: 0.9017 - val_recall_23: 0.5927
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2997 - acc: 0.8802 - precision_23: 0.8481 - auc_23: 0.8930 - recall_23: 0.5578 - val_loss: 0.3253 - val_acc: 0.8741 - val_precision_23: 0.8909 - val_auc_23: 0.8772 - val_recall_23: 0.4424
Epoch 6/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2862 - acc: 0.8851 - precision_23: 0.8536 - auc_23: 0.9007 - recall_23: 0.5781 - val_loss: 0.3053 - val_acc: 0.8695 - val_precision_23: 0.7262 - val_auc_23: 0.9010 - val_recall_23: 0.6391
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2708 - acc: 0.8931 - precision_23: 0.8536 - auc_23: 0.9119 - recall_23: 0.6261 - val_loss: 0.2750 - val_acc: 0.8982 - val_precision_23: 0.8194 - val_auc_23: 0.9049 - val_recall_23: 0.6593
Epoch 8/100
243/254 [===========================>..] - ETA: 2s - loss: 0.2630 - acc: 0.8980 - precision_23: 0.8541 - auc_23: 0.9162 - recall_23: 0.6550client_loop: send disconnect: Connection reset by peer
(base)
ignac@DESKTOP-UR509JM MINGW64 ~
$ ^C
(base)
ignac@DESKTOP-UR509JM MINGW64 ~
$ ssh ignacys@recs.man.poznan.pl
Last login: Fri Sep 24 12:34:55 2021 from 83.8.24.17.ipv4.supernova.orange.pl
===============================================================================
24.01.2016

                                 IMPORTANT
Due to disk limitations quota has been introduced to all users. Soft quota 80G
for one day, hard quota 100G.

==============================================================================
04.06.2019

Core allocation policy has changed. Users should specify required number of
cores with --cpus-per-task option.

===============================================================================
18.02.2020

In order to log in into openshift or grafana, user should:

    1. Set an ldap password for their account. In order to do that, one should
       run "request_ldap_passwd_reset" and follow the instructions.
    2. Then send an email to marqs@man.poznan.pl and mzdunek@man.poznan.pl
        specifying a group you want to be added to (e.g. openshift or grafana).


Aby zalogowa si do usugi openshift lub grafana naley ustawi haso do konta
w ldapie. W tym celu naley:

    1. Wywoa skrypt "request_ldap_passwd_reset", a nastpnie postpowa
       zgodnie z poleceniami.
    2. Po ustawieniu hasa naley napisa emaila do marqs@man.poznan.pl oraz
       mzdunek@man.poznan.pl, w ktrym naley poda grup, do ktrej chce si
       by dodanym (np. openshift lub grafana).

===============================================================================
(base) [ignacys@ui ~]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
             29793       all       sh  ignacys  R   21:41:23      1 xeon-09
(base) [ignacys@ui ~]$ scancel 29793
(base) [ignacys@ui ~]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
             29793       all       sh  ignacys CG   21:41:28      1 xeon-09
(base) [ignacys@ui ~]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
             29793       all       sh  ignacys CG   21:41:28      1 xeon-09
(base) [ignacys@ui ~]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
(base) [ignacys@ui ~]$ salloc -N1 -n1 -p all -w xeon-e-v100
salloc: Granted job allocation 29800
srun: Step created for job 29800
(base) [ignacys@xeon-e-v100 ~]$ cd segmentation
(base) [ignacys@xeon-e-v100 segmentation]$ ls
benchmark.csv      data_loader.py            jaccard_eval.py  mlruns        preprocessing  runs_config.json  runs.json       src          train_gpu.py  utils.py
data_generator.py  experiment_controller.py  logs             npy_datasets  __pycache__    runs.csv          simplified.csv  test_gpu.py  UNet.py
(base) [ignacys@xeon-e-v100 segmentation]$ conda activate tfenv
(tfenv) [ignacys@xeon-e-v100 segmentation]$ python experiment_controller.py
2021-09-25 10:23:52.106128: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1

-------------------------
RUN: Baseline - bacteria, PARAMS: {}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-25 10:23:59.926220: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:23:59.928383: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-09-25 10:23:59.972978: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 1 GPUs
2021-09-25 10:23:59.975158: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcupti.so.10.1'; dlerror: libcupti.so.10.1: cannot open shared object file: No such file or directory
2021-09-25 10:23:59.976958: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcupti.so'; dlerror: libcupti.so: cannot open shared object file: No such file or directory
2021-09-25 10:23:59.976986: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 10:23:59.983712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-25 10:23:59.984458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2021-09-25 10:23:59.984483: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-09-25 10:24:00.316846: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-09-25 10:24:00.648977: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-09-25 10:24:01.249113: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-09-25 10:24:01.635099: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-09-25 10:24:01.763125: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-09-25 10:24:02.264600: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-09-25 10:24:02.264777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-25 10:24:02.265830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-25 10:24:02.266732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2021-09-25 10:24:02.267005: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-25 10:24:02.273364: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3000000000 Hz
2021-09-25 10:24:02.273510: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5598b64f29e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-09-25 10:24:02.273535: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-09-25 10:24:02.348529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-25 10:24:02.349409: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5598b64f4460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-09-25 10:24:02.349435: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2021-09-25 10:24:02.349690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-25 10:24:02.350438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2021-09-25 10:24:02.350467: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-09-25 10:24:02.350491: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-09-25 10:24:02.350509: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-09-25 10:24:02.350526: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-09-25 10:24:02.350543: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-09-25 10:24:02.350559: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-09-25 10:24:02.350577: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-09-25 10:24:02.350638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-25 10:24:02.351449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-25 10:24:02.352210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2021-09-25 10:24:02.352238: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-09-25 10:24:04.493481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-09-25 10:24:04.493519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0
2021-09-25 10:24:04.493529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N
2021-09-25 10:24:04.493703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-25 10:24:04.494512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-25 10:24:04.495283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:01:00.0, compute capability: 7.0)
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied normalization
Data augumentation off
Prep done
Training samples: 258, channel mean: 0.20299959845194301,
Validation samples: 63, channel mean: 0.20179995867161976
Model built.
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 256, 256, 32) 320         input_1[0][0]
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 256, 256, 32) 9248        conv2d[0][0]
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 128, 128, 32) 0           conv2d_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 128, 128, 64) 18496       max_pooling2d[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 128, 128, 64) 36928       conv2d_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           conv2d_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d_1[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 64, 64, 128)  147584      conv2d_4[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0           conv2d_5[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 256)  295168      max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 256)  590080      conv2d_6[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0           conv2d_7[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 16, 16, 512)  1180160     max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 16, 16, 512)  2359808     conv2d_8[0][0]
__________________________________________________________________________________________________
up_sampling2d (UpSampling2D)    (None, 32, 32, 512)  0           conv2d_9[0][0]
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 32, 32, 768)  0           up_sampling2d[0][0]
                                                                 conv2d_7[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_10[0][0]
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0           conv2d_11[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 64, 64, 384)  0           up_sampling2d_1[0][0]
                                                                 conv2d_5[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_1[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_12[0][0]
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 128 0           conv2d_13[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 128, 128, 192 0           up_sampling2d_2[0][0]
                                                                 conv2d_3[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_2[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_14[0][0]
__________________________________________________________________________________________________
up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 64) 0           conv2d_15[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 256, 256, 96) 0           up_sampling2d_3[0][0]
                                                                 conv2d_1[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 256, 256, 32) 27680       concatenate_3[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_16[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 256, 256, 1)  33          conv2d_17[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
2021-09-25 10:24:08.077153: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-09-25 10:24:09.025184: W tensorflow/stream_executor/gpu/asm_compiler.cc:81] Running ptxas --version returned 256
2021-09-25 10:24:09.103306: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output:
Relying on driver to perform ptx compilation.
Modify $PATH to customize ptxas location.
This message will be only logged once.
2021-09-25 10:24:09.974738: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
 1/33 [..............................] - ETA: 0s - loss: 0.6932 - acc: 0.7142 - precision: 0.1589 - auc: 0.3203 - recall: 0.19542021-09-25 10:24:48.318207: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:24:48.318879: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
WARNING:tensorflow:From /home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.
Instructions for updating:
use `tf.profiler.experimental.stop` instead.
2021-09-25 10:24:48.397263: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 10:24:48.407208: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-102359/train/plugins/profile/2021_09_25_10_24_48
2021-09-25 10:24:48.411216: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-102359/train/plugins/profile/2021_09_25_10_24_48/xeon-e-v100.trace.json.gz
2021-09-25 10:24:48.427866: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-102359/train/plugins/profile/2021_09_25_10_24_48
2021-09-25 10:24:48.434385: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-102359/train/plugins/profile/2021_09_25_10_24_48/xeon-e-v100.memory_profile.json.gz
2021-09-25 10:24:48.458753: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-102359/train/plugins/profile/2021_09_25_10_24_48Dumped tool data for xplane.pb to logs/fit/20210925-102359/train/plugins/profile/2021_09_25_10_24_48/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-102359/train/plugins/profile/2021_09_25_10_24_48/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-102359/train/plugins/profile/2021_09_25_10_24_48/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-102359/train/plugins/profile/2021_09_25_10_24_48/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-102359/train/plugins/profile/2021_09_25_10_24_48/xeon-e-v100.kernel_stats.pb

33/33 [==============================] - 58s 2s/step - loss: 0.5307 - acc: 0.8692 - precision: 0.7647 - auc: 0.7119 - recall: 0.2890 - val_loss: 0.3195 - val_acc: 0.8728 - val_precision: 0.5721 - val_auc: 0.8974 - val_recall: 0.7605
Epoch 2/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2562 - acc: 0.9027 - precision: 0.7218 - auc: 0.9090 - recall: 0.6585 - val_loss: 0.2125 - val_acc: 0.9161 - val_precision: 0.7420 - val_auc: 0.9375 - val_recall: 0.6838
Epoch 3/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2482 - acc: 0.9099 - precision: 0.7692 - auc: 0.9119 - recall: 0.6417 - val_loss: 0.2289 - val_acc: 0.9137 - val_precision: 0.8025 - val_auc: 0.9252 - val_recall: 0.5852
Epoch 4/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2361 - acc: 0.9138 - precision: 0.7929 - auc: 0.9141 - recall: 0.6400 - val_loss: 0.2473 - val_acc: 0.9011 - val_precision: 0.7270 - val_auc: 0.9217 - val_recall: 0.6639
Epoch 5/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2092 - acc: 0.9204 - precision: 0.8132 - auc: 0.9370 - recall: 0.6659 - val_loss: 0.1907 - val_acc: 0.9242 - val_precision: 0.8517 - val_auc: 0.9538 - val_recall: 0.6173
Epoch 6/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2092 - acc: 0.9173 - precision: 0.8012 - auc: 0.9387 - recall: 0.6572 - val_loss: 0.2018 - val_acc: 0.9188 - val_precision: 0.7428 - val_auc: 0.9450 - val_recall: 0.7433
Epoch 7/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1912 - acc: 0.9211 - precision: 0.7927 - auc: 0.9521 - recall: 0.7002 - val_loss: 0.1544 - val_acc: 0.9321 - val_precision: 0.7885 - val_auc: 0.9689 - val_recall: 0.7485
Epoch 8/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1798 - acc: 0.9311 - precision: 0.8149 - auc: 0.9567 - recall: 0.7485 - val_loss: 0.2020 - val_acc: 0.9263 - val_precision: 0.8459 - val_auc: 0.9540 - val_recall: 0.6692
Epoch 9/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1734 - acc: 0.9322 - precision: 0.8130 - auc: 0.9611 - recall: 0.7605 - val_loss: 0.1636 - val_acc: 0.9361 - val_precision: 0.7665 - val_auc: 0.9744 - val_recall: 0.8541
Epoch 10/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1520 - acc: 0.9408 - precision: 0.8219 - auc: 0.9713 - recall: 0.8141 - val_loss: 0.1271 - val_acc: 0.9489 - val_precision: 0.8632 - val_auc: 0.9809 - val_recall: 0.7980
Epoch 11/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1503 - acc: 0.9410 - precision: 0.8275 - auc: 0.9713 - recall: 0.8077 - val_loss: 0.1323 - val_acc: 0.9467 - val_precision: 0.7942 - val_auc: 0.9824 - val_recall: 0.8991
Epoch 12/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1284 - acc: 0.9507 - precision: 0.8522 - auc: 0.9786 - recall: 0.8448 - val_loss: 0.1073 - val_acc: 0.9617 - val_precision: 0.8660 - val_auc: 0.9865 - val_recall: 0.8695
Epoch 13/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1196 - acc: 0.9534 - precision: 0.8658 - auc: 0.9822 - recall: 0.8459 - val_loss: 0.1094 - val_acc: 0.9578 - val_precision: 0.8277 - val_auc: 0.9871 - val_recall: 0.9125
Epoch 14/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1050 - acc: 0.9599 - precision: 0.8782 - auc: 0.9860 - recall: 0.8764 - val_loss: 0.1376 - val_acc: 0.9509 - val_precision: 0.9139 - val_auc: 0.9768 - val_recall: 0.7651
Epoch 15/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1237 - acc: 0.9541 - precision: 0.8705 - auc: 0.9789 - recall: 0.8447 - val_loss: 0.1050 - val_acc: 0.9595 - val_precision: 0.8946 - val_auc: 0.9871 - val_recall: 0.8528
Epoch 16/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0989 - acc: 0.9620 - precision: 0.8844 - auc: 0.9876 - recall: 0.8826 - val_loss: 0.0859 - val_acc: 0.9666 - val_precision: 0.8878 - val_auc: 0.9900 - val_recall: 0.8987
Epoch 17/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0879 - acc: 0.9660 - precision: 0.9057 - auc: 0.9903 - recall: 0.8837 - val_loss: 0.0889 - val_acc: 0.9653 - val_precision: 0.8711 - val_auc: 0.9908 - val_recall: 0.9150
Epoch 18/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0927 - acc: 0.9639 - precision: 0.8928 - auc: 0.9893 - recall: 0.8858 - val_loss: 0.0964 - val_acc: 0.9623 - val_precision: 0.8403 - val_auc: 0.9916 - val_recall: 0.9492
Epoch 19/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0969 - acc: 0.9624 - precision: 0.8856 - auc: 0.9883 - recall: 0.8843 - val_loss: 0.0859 - val_acc: 0.9675 - val_precision: 0.8940 - val_auc: 0.9883 - val_recall: 0.8771
Epoch 20/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0899 - acc: 0.9665 - precision: 0.9049 - auc: 0.9895 - recall: 0.8887 - val_loss: 0.0819 - val_acc: 0.9710 - val_precision: 0.9272 - val_auc: 0.9902 - val_recall: 0.8821
Epoch 21/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0771 - acc: 0.9701 - precision: 0.9114 - auc: 0.9925 - recall: 0.9053 - val_loss: 0.0664 - val_acc: 0.9743 - val_precision: 0.8951 - val_auc: 0.9941 - val_recall: 0.9327
Epoch 22/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0683 - acc: 0.9732 - precision: 0.9233 - auc: 0.9942 - recall: 0.9121 - val_loss: 0.0742 - val_acc: 0.9714 - val_precision: 0.9136 - val_auc: 0.9922 - val_recall: 0.8932
Epoch 23/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0653 - acc: 0.9742 - precision: 0.9238 - auc: 0.9946 - recall: 0.9179 - val_loss: 0.0814 - val_acc: 0.9689 - val_precision: 0.9098 - val_auc: 0.9903 - val_recall: 0.8996
Epoch 24/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0626 - acc: 0.9752 - precision: 0.9278 - auc: 0.9950 - recall: 0.9201 - val_loss: 0.0688 - val_acc: 0.9741 - val_precision: 0.8964 - val_auc: 0.9937 - val_recall: 0.9299
Epoch 25/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0590 - acc: 0.9764 - precision: 0.9320 - auc: 0.9956 - recall: 0.9232 - val_loss: 0.0701 - val_acc: 0.9728 - val_precision: 0.9034 - val_auc: 0.9934 - val_recall: 0.9264
Epoch 26/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0759 - acc: 0.9718 - precision: 0.9168 - auc: 0.9918 - recall: 0.9101 - val_loss: 0.0798 - val_acc: 0.9705 - val_precision: 0.9233 - val_auc: 0.9896 - val_recall: 0.8840
Epoch 27/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0620 - acc: 0.9755 - precision: 0.9288 - auc: 0.9951 - recall: 0.9204 - val_loss: 0.0643 - val_acc: 0.9750 - val_precision: 0.9235 - val_auc: 0.9943 - val_recall: 0.9116
Epoch 28/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0591 - acc: 0.9764 - precision: 0.9335 - auc: 0.9955 - recall: 0.9213 - val_loss: 0.0718 - val_acc: 0.9746 - val_precision: 0.9140 - val_auc: 0.9930 - val_recall: 0.9126
Epoch 29/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0543 - acc: 0.9781 - precision: 0.9376 - auc: 0.9962 - recall: 0.9280 - val_loss: 0.0793 - val_acc: 0.9706 - val_precision: 0.9270 - val_auc: 0.9909 - val_recall: 0.8968
Epoch 30/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0526 - acc: 0.9788 - precision: 0.9415 - auc: 0.9964 - recall: 0.9280 - val_loss: 0.0675 - val_acc: 0.9751 - val_precision: 0.9200 - val_auc: 0.9928 - val_recall: 0.9203
Epoch 31/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0512 - acc: 0.9791 - precision: 0.9389 - auc: 0.9967 - recall: 0.9330 - val_loss: 0.0761 - val_acc: 0.9725 - val_precision: 0.9302 - val_auc: 0.9915 - val_recall: 0.8990
Epoch 32/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0501 - acc: 0.9795 - precision: 0.9411 - auc: 0.9968 - recall: 0.9330 - val_loss: 0.0708 - val_acc: 0.9740 - val_precision: 0.9081 - val_auc: 0.9933 - val_recall: 0.9227
Epoch 33/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0491 - acc: 0.9799 - precision: 0.9417 - auc: 0.9968 - recall: 0.9348 - val_loss: 0.0754 - val_acc: 0.9731 - val_precision: 0.9322 - val_auc: 0.9910 - val_recall: 0.8941
Epoch 34/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0521 - acc: 0.9791 - precision: 0.9411 - auc: 0.9964 - recall: 0.9303 - val_loss: 0.0777 - val_acc: 0.9724 - val_precision: 0.9130 - val_auc: 0.9908 - val_recall: 0.9114
Epoch 35/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0563 - acc: 0.9777 - precision: 0.9353 - auc: 0.9956 - recall: 0.9275 - val_loss: 0.0756 - val_acc: 0.9718 - val_precision: 0.9134 - val_auc: 0.9931 - val_recall: 0.9281
Epoch 36/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0664 - acc: 0.9742 - precision: 0.9277 - auc: 0.9939 - recall: 0.9133 - val_loss: 0.0774 - val_acc: 0.9716 - val_precision: 0.9078 - val_auc: 0.9919 - val_recall: 0.9132
Epoch 37/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0536 - acc: 0.9783 - precision: 0.9384 - auc: 0.9963 - recall: 0.9284 - val_loss: 0.0820 - val_acc: 0.9722 - val_precision: 0.8968 - val_auc: 0.9903 - val_recall: 0.9126
Epoch 38/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0518 - acc: 0.9788 - precision: 0.9408 - auc: 0.9965 - recall: 0.9290 - val_loss: 0.0715 - val_acc: 0.9720 - val_precision: 0.9113 - val_auc: 0.9926 - val_recall: 0.9172
Epoch 39/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0447 - acc: 0.9816 - precision: 0.9490 - auc: 0.9974 - recall: 0.9378 - val_loss: 0.0803 - val_acc: 0.9734 - val_precision: 0.9154 - val_auc: 0.9910 - val_recall: 0.9194
Epoch 40/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0411 - acc: 0.9828 - precision: 0.9503 - auc: 0.9978 - recall: 0.9442 - val_loss: 0.0673 - val_acc: 0.9759 - val_precision: 0.9247 - val_auc: 0.9927 - val_recall: 0.9202
Epoch 41/100
32/33 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9834 - precision: 0.9537 - auc: 0.9980 - recall: 0.9447Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 85ms/step - loss: 0.0395 - acc: 0.9834 - precision: 0.9536 - auc: 0.9980 - recall: 0.9443 - val_loss: 0.0789 - val_acc: 0.9734 - val_precision: 0.9338 - val_auc: 0.9896 - val_recall: 0.8947
Epoch 00041: early stopping
WARNING:tensorflow:From /home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
2021-09-25 10:27:41.596483: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:tensorflow:From /home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
6/6 [==============================] - 16s 3s/step - loss: 0.0736 - acc: 0.9710 - precision: 0.9077 - auc: 0.9931 - recall: 0.9142
---------------TEST METRICS----------------------
jaccard_index 0.8295164744442148
test_sensitivity 0.9232988252201668
test_specifitivity 0.9800015544091788
test_accuracy 0.9707970513237847
test_precision 0.8994654191077566
test_jaccard_score 0.8295164744442148
test_dicecoef 0.9112263062791903
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-102842.h5
[0. 0. 0. 0. 0.] [0.97079705 0.82951647 0.89946542 0.92329883 0.98000155]

-------------------------
Rep: 1
-------------------------

2021-09-25 10:28:43.524302: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:28:43.525516: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied normalization
Data augumentation off
Prep done
Training samples: 258, channel mean: 0.20299959845194301,
Validation samples: 63, channel mean: 0.20179995867161976
Model built.
Model: "functional_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_2 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 256, 256, 32) 320         input_2[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_19[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 128, 128, 32) 0           conv2d_20[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_21[0][0]
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 64, 64, 64)   0           conv2d_22[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_5[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_23[0][0]
__________________________________________________________________________________________________
max_pooling2d_6 (MaxPooling2D)  (None, 32, 32, 128)  0           conv2d_24[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_6[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_25[0][0]
__________________________________________________________________________________________________
max_pooling2d_7 (MaxPooling2D)  (None, 16, 16, 256)  0           conv2d_26[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_7[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 16, 16, 512)  2359808     conv2d_27[0][0]
__________________________________________________________________________________________________
up_sampling2d_4 (UpSampling2D)  (None, 32, 32, 512)  0           conv2d_28[0][0]
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 32, 32, 768)  0           up_sampling2d_4[0][0]
                                                                 conv2d_26[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate_4[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_29[0][0]
__________________________________________________________________________________________________
up_sampling2d_5 (UpSampling2D)  (None, 64, 64, 256)  0           conv2d_30[0][0]
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 64, 64, 384)  0           up_sampling2d_5[0][0]
                                                                 conv2d_24[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_5[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_31[0][0]
__________________________________________________________________________________________________
up_sampling2d_6 (UpSampling2D)  (None, 128, 128, 128 0           conv2d_32[0][0]
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 128, 128, 192 0           up_sampling2d_6[0][0]
                                                                 conv2d_22[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_6[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_33[0][0]
__________________________________________________________________________________________________
up_sampling2d_7 (UpSampling2D)  (None, 256, 256, 64) 0           conv2d_34[0][0]
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 256, 256, 96) 0           up_sampling2d_7[0][0]
                                                                 conv2d_20[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 256, 256, 32) 27680       concatenate_7[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_35[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 256, 256, 1)  33          conv2d_36[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6933 - acc: 0.1596 - precision_1: 0.1561 - auc_1: 0.9026 - recall_1: 0.99992021-09-25 10:28:45.841920: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:28:45.841973: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 10:28:46.105699: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 10:28:46.137126: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-102843/train/plugins/profile/2021_09_25_10_28_46
2021-09-25 10:28:46.141467: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-102843/train/plugins/profile/2021_09_25_10_28_46/xeon-e-v100.trace.json.gz
2021-09-25 10:28:46.159517: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-102843/train/plugins/profile/2021_09_25_10_28_46
2021-09-25 10:28:46.166786: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-102843/train/plugins/profile/2021_09_25_10_28_46/xeon-e-v100.memory_profile.json.gz
2021-09-25 10:28:46.188600: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-102843/train/plugins/profile/2021_09_25_10_28_46Dumped tool data for xplane.pb to logs/fit/20210925-102843/train/plugins/profile/2021_09_25_10_28_46/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-102843/train/plugins/profile/2021_09_25_10_28_46/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-102843/train/plugins/profile/2021_09_25_10_28_46/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-102843/train/plugins/profile/2021_09_25_10_28_46/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-102843/train/plugins/profile/2021_09_25_10_28_46/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6919 - acc: 0.4734 - precision_1: 0.2314 - auc_1: 0.7451 - recall_1: 0.8021WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0680s vs `on_train_batch_end` time: 0.2801s). Check your callbacks.
33/33 [==============================] - 3s 106ms/step - loss: 0.5315 - acc: 0.8466 - precision_1: 0.5444 - auc_1: 0.6786 - recall_1: 0.3774 - val_loss: 0.7936 - val_acc: 0.8512 - val_precision_1: 0.8511 - val_auc_1: 0.6936 - val_recall_1: 0.0659
Epoch 2/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2765 - acc: 0.9001 - precision_1: 0.7572 - auc_1: 0.8809 - recall_1: 0.5721 - val_loss: 0.2423 - val_acc: 0.9135 - val_precision_1: 0.7793 - val_auc_1: 0.9111 - val_recall_1: 0.5984
Epoch 3/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2576 - acc: 0.9048 - precision_1: 0.7580 - auc_1: 0.9059 - recall_1: 0.6135 - val_loss: 0.2678 - val_acc: 0.9062 - val_precision_1: 0.7065 - val_auc_1: 0.9147 - val_recall_1: 0.6714
Epoch 4/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2433 - acc: 0.9080 - precision_1: 0.7776 - auc_1: 0.9112 - recall_1: 0.6122 - val_loss: 0.2751 - val_acc: 0.8904 - val_precision_1: 0.6794 - val_auc_1: 0.9037 - val_recall_1: 0.6648
Epoch 5/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2362 - acc: 0.9095 - precision_1: 0.7784 - auc_1: 0.9177 - recall_1: 0.6239 - val_loss: 0.2096 - val_acc: 0.9168 - val_precision_1: 0.7873 - val_auc_1: 0.9381 - val_recall_1: 0.6324
Epoch 6/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2263 - acc: 0.9125 - precision_1: 0.8129 - auc_1: 0.9249 - recall_1: 0.6041 - val_loss: 0.2074 - val_acc: 0.9173 - val_precision_1: 0.7615 - val_auc_1: 0.9411 - val_recall_1: 0.6933
Epoch 7/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2276 - acc: 0.9121 - precision_1: 0.7918 - auc_1: 0.9273 - recall_1: 0.6274 - val_loss: 0.1898 - val_acc: 0.9292 - val_precision_1: 0.8571 - val_auc_1: 0.9480 - val_recall_1: 0.6345
Epoch 8/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1940 - acc: 0.9263 - precision_1: 0.8100 - auc_1: 0.9458 - recall_1: 0.7173 - val_loss: 0.1784 - val_acc: 0.9270 - val_precision_1: 0.8531 - val_auc_1: 0.9601 - val_recall_1: 0.6660
Epoch 9/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1741 - acc: 0.9336 - precision_1: 0.8261 - auc_1: 0.9574 - recall_1: 0.7522 - val_loss: 0.1722 - val_acc: 0.9353 - val_precision_1: 0.7373 - val_auc_1: 0.9774 - val_recall_1: 0.9151
Epoch 10/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1472 - acc: 0.9426 - precision_1: 0.8296 - auc_1: 0.9712 - recall_1: 0.8169 - val_loss: 0.1093 - val_acc: 0.9580 - val_precision_1: 0.8471 - val_auc_1: 0.9847 - val_recall_1: 0.8906
Epoch 11/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1333 - acc: 0.9487 - precision_1: 0.8566 - auc_1: 0.9771 - recall_1: 0.8241 - val_loss: 0.1136 - val_acc: 0.9543 - val_precision_1: 0.8155 - val_auc_1: 0.9872 - val_recall_1: 0.9224
Epoch 12/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1128 - acc: 0.9562 - precision_1: 0.8734 - auc_1: 0.9839 - recall_1: 0.8562 - val_loss: 0.1002 - val_acc: 0.9628 - val_precision_1: 0.8598 - val_auc_1: 0.9882 - val_recall_1: 0.8869
Epoch 13/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1054 - acc: 0.9596 - precision_1: 0.8846 - auc_1: 0.9856 - recall_1: 0.8659 - val_loss: 0.1074 - val_acc: 0.9579 - val_precision_1: 0.8145 - val_auc_1: 0.9892 - val_recall_1: 0.9371
Epoch 14/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0994 - acc: 0.9613 - precision_1: 0.8832 - auc_1: 0.9875 - recall_1: 0.8796 - val_loss: 0.1086 - val_acc: 0.9581 - val_precision_1: 0.9140 - val_auc_1: 0.9870 - val_recall_1: 0.8145
Epoch 15/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1045 - acc: 0.9591 - precision_1: 0.8876 - auc_1: 0.9865 - recall_1: 0.8583 - val_loss: 0.1000 - val_acc: 0.9608 - val_precision_1: 0.8770 - val_auc_1: 0.9874 - val_recall_1: 0.8843
Epoch 16/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1026 - acc: 0.9609 - precision_1: 0.8832 - auc_1: 0.9860 - recall_1: 0.8768 - val_loss: 0.0900 - val_acc: 0.9656 - val_precision_1: 0.8853 - val_auc_1: 0.9893 - val_recall_1: 0.8941
Epoch 17/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0909 - acc: 0.9650 - precision_1: 0.9030 - auc_1: 0.9892 - recall_1: 0.8802 - val_loss: 0.0876 - val_acc: 0.9667 - val_precision_1: 0.8836 - val_auc_1: 0.9911 - val_recall_1: 0.9083
Epoch 18/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0902 - acc: 0.9653 - precision_1: 0.8967 - auc_1: 0.9895 - recall_1: 0.8901 - val_loss: 0.0814 - val_acc: 0.9693 - val_precision_1: 0.9017 - val_auc_1: 0.9907 - val_recall_1: 0.9111
Epoch 19/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0786 - acc: 0.9693 - precision_1: 0.9080 - auc_1: 0.9921 - recall_1: 0.9041 - val_loss: 0.0772 - val_acc: 0.9712 - val_precision_1: 0.9168 - val_auc_1: 0.9897 - val_recall_1: 0.8789
Epoch 20/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0737 - acc: 0.9712 - precision_1: 0.9160 - auc_1: 0.9932 - recall_1: 0.9074 - val_loss: 0.0702 - val_acc: 0.9730 - val_precision_1: 0.9071 - val_auc_1: 0.9926 - val_recall_1: 0.9197
Epoch 21/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0741 - acc: 0.9709 - precision_1: 0.9118 - auc_1: 0.9928 - recall_1: 0.9102 - val_loss: 0.0657 - val_acc: 0.9750 - val_precision_1: 0.9257 - val_auc_1: 0.9939 - val_recall_1: 0.9009
Epoch 22/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0724 - acc: 0.9719 - precision_1: 0.9194 - auc_1: 0.9933 - recall_1: 0.9078 - val_loss: 0.0712 - val_acc: 0.9723 - val_precision_1: 0.8996 - val_auc_1: 0.9929 - val_recall_1: 0.9169
Epoch 23/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0677 - acc: 0.9734 - precision_1: 0.9210 - auc_1: 0.9942 - recall_1: 0.9159 - val_loss: 0.0791 - val_acc: 0.9708 - val_precision_1: 0.9045 - val_auc_1: 0.9907 - val_recall_1: 0.9187
Epoch 24/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0675 - acc: 0.9737 - precision_1: 0.9231 - auc_1: 0.9942 - recall_1: 0.9156 - val_loss: 0.0788 - val_acc: 0.9692 - val_precision_1: 0.8573 - val_auc_1: 0.9936 - val_recall_1: 0.9462
Epoch 25/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0696 - acc: 0.9732 - precision_1: 0.9206 - auc_1: 0.9936 - recall_1: 0.9152 - val_loss: 0.0862 - val_acc: 0.9674 - val_precision_1: 0.8602 - val_auc_1: 0.9925 - val_recall_1: 0.9467
Epoch 26/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0736 - acc: 0.9713 - precision_1: 0.9128 - auc_1: 0.9931 - recall_1: 0.9112 - val_loss: 0.0866 - val_acc: 0.9660 - val_precision_1: 0.8618 - val_auc_1: 0.9913 - val_recall_1: 0.9310
Epoch 27/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0737 - acc: 0.9713 - precision_1: 0.9193 - auc_1: 0.9931 - recall_1: 0.9036 - val_loss: 0.0660 - val_acc: 0.9748 - val_precision_1: 0.9142 - val_auc_1: 0.9937 - val_recall_1: 0.9214
Epoch 28/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0630 - acc: 0.9752 - precision_1: 0.9267 - auc_1: 0.9947 - recall_1: 0.9213 - val_loss: 0.0753 - val_acc: 0.9728 - val_precision_1: 0.9149 - val_auc_1: 0.9915 - val_recall_1: 0.8982
Epoch 29/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0612 - acc: 0.9758 - precision_1: 0.9294 - auc_1: 0.9952 - recall_1: 0.9221 - val_loss: 0.0799 - val_acc: 0.9700 - val_precision_1: 0.9260 - val_auc_1: 0.9908 - val_recall_1: 0.8943
Epoch 30/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0608 - acc: 0.9761 - precision_1: 0.9321 - auc_1: 0.9952 - recall_1: 0.9211 - val_loss: 0.0676 - val_acc: 0.9748 - val_precision_1: 0.9088 - val_auc_1: 0.9935 - val_recall_1: 0.9315
Epoch 31/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0570 - acc: 0.9773 - precision_1: 0.9320 - auc_1: 0.9958 - recall_1: 0.9290 - val_loss: 0.0720 - val_acc: 0.9725 - val_precision_1: 0.9354 - val_auc_1: 0.9923 - val_recall_1: 0.8935
Epoch 32/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0735 - acc: 0.9718 - precision_1: 0.9195 - auc_1: 0.9921 - recall_1: 0.9070 - val_loss: 0.0776 - val_acc: 0.9706 - val_precision_1: 0.8977 - val_auc_1: 0.9911 - val_recall_1: 0.9107
Epoch 33/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0736 - acc: 0.9722 - precision_1: 0.9167 - auc_1: 0.9925 - recall_1: 0.9130 - val_loss: 0.0754 - val_acc: 0.9720 - val_precision_1: 0.9456 - val_auc_1: 0.9916 - val_recall_1: 0.8726
Epoch 34/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0636 - acc: 0.9750 - precision_1: 0.9288 - auc_1: 0.9949 - recall_1: 0.9172 - val_loss: 0.0710 - val_acc: 0.9736 - val_precision_1: 0.9285 - val_auc_1: 0.9923 - val_recall_1: 0.9016
Epoch 35/100
32/33 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9778 - precision_1: 0.9361 - auc_1: 0.9959 - recall_1: 0.9286Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 85ms/step - loss: 0.0559 - acc: 0.9779 - precision_1: 0.9359 - auc_1: 0.9959 - recall_1: 0.9284 - val_loss: 0.0794 - val_acc: 0.9733 - val_precision_1: 0.9341 - val_auc_1: 0.9900 - val_recall_1: 0.9132
Epoch 00035: early stopping
6/6 [==============================] - 0s 25ms/step - loss: 0.0901 - acc: 0.9634 - precision_1: 0.9125 - auc_1: 0.9909 - recall_1: 0.8567
---------------TEST METRICS----------------------
jaccard_index 0.810363356791802
test_sensitivity 0.8820499323206498
test_specifitivity 0.9809273184174819
test_accuracy 0.9648766411675347
test_precision 0.8996184320133623
test_jaccard_score 0.810363356791802
test_dicecoef 0.8907475633024543
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-103039.h5
[0.97079705 0.82951647 0.89946542 0.92329883 0.98000155] [0.96487664 0.81036336 0.89961843 0.88204993 0.98092732]

-------------------------
Rep: 2
-------------------------

2021-09-25 10:30:40.488116: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:30:40.488179: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied normalization
Data augumentation off
Prep done
Training samples: 258, channel mean: 0.20299959845194301,
Validation samples: 63, channel mean: 0.20179995867161976
Model built.
Model: "functional_5"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_3 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 256, 256, 32) 320         input_3[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_38[0][0]
__________________________________________________________________________________________________
max_pooling2d_8 (MaxPooling2D)  (None, 128, 128, 32) 0           conv2d_39[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d_8[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_40[0][0]
__________________________________________________________________________________________________
max_pooling2d_9 (MaxPooling2D)  (None, 64, 64, 64)   0           conv2d_41[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_9[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_42[0][0]
__________________________________________________________________________________________________
max_pooling2d_10 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_43[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_10[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_44[0][0]
__________________________________________________________________________________________________
max_pooling2d_11 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_11[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 16, 16, 512)  2359808     conv2d_46[0][0]
__________________________________________________________________________________________________
up_sampling2d_8 (UpSampling2D)  (None, 32, 32, 512)  0           conv2d_47[0][0]
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 32, 32, 768)  0           up_sampling2d_8[0][0]
                                                                 conv2d_45[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate_8[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_48[0][0]
__________________________________________________________________________________________________
up_sampling2d_9 (UpSampling2D)  (None, 64, 64, 256)  0           conv2d_49[0][0]
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 64, 64, 384)  0           up_sampling2d_9[0][0]
                                                                 conv2d_43[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_9[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_50[0][0]
__________________________________________________________________________________________________
up_sampling2d_10 (UpSampling2D) (None, 128, 128, 128 0           conv2d_51[0][0]
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_10[0][0]
                                                                 conv2d_41[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_10[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_52[0][0]
__________________________________________________________________________________________________
up_sampling2d_11 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_53[0][0]
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_11[0][0]
                                                                 conv2d_39[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 256, 256, 32) 27680       concatenate_11[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_54[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 256, 256, 1)  33          conv2d_55[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6933 - acc: 0.1557 - precision_2: 0.1554 - auc_2: 0.9044 - recall_2: 0.99982021-09-25 10:30:42.589121: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:30:42.589174: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 10:30:42.845323: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 10:30:42.854433: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-103040/train/plugins/profile/2021_09_25_10_30_42
2021-09-25 10:30:42.858179: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-103040/train/plugins/profile/2021_09_25_10_30_42/xeon-e-v100.trace.json.gz
2021-09-25 10:30:42.875041: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-103040/train/plugins/profile/2021_09_25_10_30_42
2021-09-25 10:30:42.882731: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-103040/train/plugins/profile/2021_09_25_10_30_42/xeon-e-v100.memory_profile.json.gz
2021-09-25 10:30:42.899543: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-103040/train/plugins/profile/2021_09_25_10_30_42Dumped tool data for xplane.pb to logs/fit/20210925-103040/train/plugins/profile/2021_09_25_10_30_42/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-103040/train/plugins/profile/2021_09_25_10_30_42/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-103040/train/plugins/profile/2021_09_25_10_30_42/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-103040/train/plugins/profile/2021_09_25_10_30_42/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-103040/train/plugins/profile/2021_09_25_10_30_42/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 4s - loss: 0.6923 - acc: 0.4846 - precision_2: 0.2288 - auc_2: 0.7158 - recall_2: 0.7595WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0649s vs `on_train_batch_end` time: 0.2468s). Check your callbacks.
33/33 [==============================] - 3s 104ms/step - loss: 0.5333 - acc: 0.8589 - precision_2: 0.5896 - auc_2: 0.7142 - recall_2: 0.4499 - val_loss: 0.3082 - val_acc: 0.8878 - val_precision_2: 0.6294 - val_auc_2: 0.8900 - val_recall_2: 0.6975
Epoch 2/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2583 - acc: 0.9035 - precision_2: 0.7485 - auc_2: 0.9061 - recall_2: 0.6176 - val_loss: 0.2386 - val_acc: 0.9179 - val_precision_2: 0.7831 - val_auc_2: 0.9196 - val_recall_2: 0.6335
Epoch 3/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2703 - acc: 0.9018 - precision_2: 0.7193 - auc_2: 0.9008 - recall_2: 0.6548 - val_loss: 0.2601 - val_acc: 0.9124 - val_precision_2: 0.7824 - val_auc_2: 0.9178 - val_recall_2: 0.5992
Epoch 4/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2514 - acc: 0.9071 - precision_2: 0.7613 - auc_2: 0.9030 - recall_2: 0.6291 - val_loss: 0.2829 - val_acc: 0.8865 - val_precision_2: 0.6575 - val_auc_2: 0.9013 - val_recall_2: 0.6847
Epoch 5/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2356 - acc: 0.9096 - precision_2: 0.7669 - auc_2: 0.9203 - recall_2: 0.6426 - val_loss: 0.2093 - val_acc: 0.9168 - val_precision_2: 0.7645 - val_auc_2: 0.9369 - val_recall_2: 0.6673
Epoch 6/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2217 - acc: 0.9142 - precision_2: 0.7980 - auc_2: 0.9302 - recall_2: 0.6358 - val_loss: 0.1939 - val_acc: 0.9233 - val_precision_2: 0.8167 - val_auc_2: 0.9478 - val_recall_2: 0.6630
Epoch 7/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2131 - acc: 0.9159 - precision_2: 0.7872 - auc_2: 0.9384 - recall_2: 0.6654 - val_loss: 0.1836 - val_acc: 0.9251 - val_precision_2: 0.7859 - val_auc_2: 0.9551 - val_recall_2: 0.6888
Epoch 8/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2005 - acc: 0.9217 - precision_2: 0.8039 - auc_2: 0.9461 - recall_2: 0.6889 - val_loss: 0.2030 - val_acc: 0.9169 - val_precision_2: 0.7976 - val_auc_2: 0.9506 - val_recall_2: 0.6556
Epoch 9/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2110 - acc: 0.9235 - precision_2: 0.8125 - auc_2: 0.9379 - recall_2: 0.6914 - val_loss: 0.2037 - val_acc: 0.9256 - val_precision_2: 0.7840 - val_auc_2: 0.9480 - val_recall_2: 0.7281
Epoch 10/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1908 - acc: 0.9286 - precision_2: 0.8116 - auc_2: 0.9487 - recall_2: 0.7334 - val_loss: 0.1723 - val_acc: 0.9290 - val_precision_2: 0.8762 - val_auc_2: 0.9639 - val_recall_2: 0.6327
Epoch 11/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1898 - acc: 0.9256 - precision_2: 0.7954 - auc_2: 0.9518 - recall_2: 0.7340 - val_loss: 0.1572 - val_acc: 0.9336 - val_precision_2: 0.7773 - val_auc_2: 0.9716 - val_recall_2: 0.8186
Epoch 12/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1678 - acc: 0.9333 - precision_2: 0.8088 - auc_2: 0.9637 - recall_2: 0.7751 - val_loss: 0.1472 - val_acc: 0.9389 - val_precision_2: 0.8860 - val_auc_2: 0.9735 - val_recall_2: 0.6627
Epoch 13/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1461 - acc: 0.9416 - precision_2: 0.8362 - auc_2: 0.9735 - recall_2: 0.7993 - val_loss: 0.1463 - val_acc: 0.9417 - val_precision_2: 0.8017 - val_auc_2: 0.9750 - val_recall_2: 0.8199
Epoch 14/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1336 - acc: 0.9481 - precision_2: 0.8549 - auc_2: 0.9772 - recall_2: 0.8224 - val_loss: 0.1363 - val_acc: 0.9468 - val_precision_2: 0.8772 - val_auc_2: 0.9784 - val_recall_2: 0.7759
Epoch 15/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1289 - acc: 0.9491 - precision_2: 0.8604 - auc_2: 0.9794 - recall_2: 0.8219 - val_loss: 0.1202 - val_acc: 0.9534 - val_precision_2: 0.8740 - val_auc_2: 0.9825 - val_recall_2: 0.8355
Epoch 16/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1162 - acc: 0.9551 - precision_2: 0.8683 - auc_2: 0.9831 - recall_2: 0.8549 - val_loss: 0.1074 - val_acc: 0.9590 - val_precision_2: 0.8750 - val_auc_2: 0.9859 - val_recall_2: 0.8585
Epoch 17/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1027 - acc: 0.9598 - precision_2: 0.8877 - auc_2: 0.9871 - recall_2: 0.8632 - val_loss: 0.1069 - val_acc: 0.9606 - val_precision_2: 0.8592 - val_auc_2: 0.9876 - val_recall_2: 0.8967
Epoch 18/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1045 - acc: 0.9601 - precision_2: 0.8807 - auc_2: 0.9859 - recall_2: 0.8744 - val_loss: 0.1002 - val_acc: 0.9606 - val_precision_2: 0.8558 - val_auc_2: 0.9884 - val_recall_2: 0.9124
Epoch 19/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0976 - acc: 0.9625 - precision_2: 0.8912 - auc_2: 0.9880 - recall_2: 0.8781 - val_loss: 0.0903 - val_acc: 0.9668 - val_precision_2: 0.8975 - val_auc_2: 0.9864 - val_recall_2: 0.8670
Epoch 20/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0845 - acc: 0.9674 - precision_2: 0.9073 - auc_2: 0.9912 - recall_2: 0.8915 - val_loss: 0.0767 - val_acc: 0.9698 - val_precision_2: 0.9022 - val_auc_2: 0.9918 - val_recall_2: 0.9031
Epoch 21/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0808 - acc: 0.9686 - precision_2: 0.9065 - auc_2: 0.9918 - recall_2: 0.9010 - val_loss: 0.0704 - val_acc: 0.9730 - val_precision_2: 0.9163 - val_auc_2: 0.9928 - val_recall_2: 0.8969
Epoch 22/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0724 - acc: 0.9714 - precision_2: 0.9155 - auc_2: 0.9933 - recall_2: 0.9093 - val_loss: 0.0783 - val_acc: 0.9692 - val_precision_2: 0.9012 - val_auc_2: 0.9911 - val_recall_2: 0.8921
Epoch 23/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0673 - acc: 0.9733 - precision_2: 0.9217 - auc_2: 0.9942 - recall_2: 0.9146 - val_loss: 0.0846 - val_acc: 0.9682 - val_precision_2: 0.9008 - val_auc_2: 0.9899 - val_recall_2: 0.9061
Epoch 24/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0664 - acc: 0.9738 - precision_2: 0.9226 - auc_2: 0.9945 - recall_2: 0.9169 - val_loss: 0.0767 - val_acc: 0.9707 - val_precision_2: 0.9013 - val_auc_2: 0.9913 - val_recall_2: 0.8973
Epoch 25/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0935 - acc: 0.9636 - precision_2: 0.8893 - auc_2: 0.9888 - recall_2: 0.8876 - val_loss: 0.0997 - val_acc: 0.9594 - val_precision_2: 0.8350 - val_auc_2: 0.9895 - val_recall_2: 0.9251
Epoch 26/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0792 - acc: 0.9690 - precision_2: 0.9085 - auc_2: 0.9922 - recall_2: 0.9014 - val_loss: 0.0802 - val_acc: 0.9689 - val_precision_2: 0.8966 - val_auc_2: 0.9910 - val_recall_2: 0.9045
Epoch 27/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0713 - acc: 0.9722 - precision_2: 0.9177 - auc_2: 0.9933 - recall_2: 0.9116 - val_loss: 0.0688 - val_acc: 0.9731 - val_precision_2: 0.9110 - val_auc_2: 0.9940 - val_recall_2: 0.9133
Epoch 28/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0637 - acc: 0.9746 - precision_2: 0.9283 - auc_2: 0.9948 - recall_2: 0.9151 - val_loss: 0.0755 - val_acc: 0.9723 - val_precision_2: 0.8986 - val_auc_2: 0.9921 - val_recall_2: 0.9144
Epoch 29/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0594 - acc: 0.9760 - precision_2: 0.9303 - auc_2: 0.9956 - recall_2: 0.9224 - val_loss: 0.0826 - val_acc: 0.9683 - val_precision_2: 0.9199 - val_auc_2: 0.9903 - val_recall_2: 0.8897
Epoch 30/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0559 - acc: 0.9772 - precision_2: 0.9371 - auc_2: 0.9960 - recall_2: 0.9223 - val_loss: 0.0786 - val_acc: 0.9718 - val_precision_2: 0.9023 - val_auc_2: 0.9915 - val_recall_2: 0.9186
Epoch 31/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0588 - acc: 0.9761 - precision_2: 0.9310 - auc_2: 0.9957 - recall_2: 0.9223 - val_loss: 0.0810 - val_acc: 0.9703 - val_precision_2: 0.9181 - val_auc_2: 0.9911 - val_recall_2: 0.8984
Epoch 32/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0528 - acc: 0.9783 - precision_2: 0.9393 - auc_2: 0.9964 - recall_2: 0.9272 - val_loss: 0.0751 - val_acc: 0.9731 - val_precision_2: 0.9040 - val_auc_2: 0.9929 - val_recall_2: 0.9211
Epoch 33/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0501 - acc: 0.9793 - precision_2: 0.9413 - auc_2: 0.9968 - recall_2: 0.9315 - val_loss: 0.0761 - val_acc: 0.9730 - val_precision_2: 0.9353 - val_auc_2: 0.9913 - val_recall_2: 0.8899
Epoch 34/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0496 - acc: 0.9794 - precision_2: 0.9441 - auc_2: 0.9968 - recall_2: 0.9290 - val_loss: 0.0792 - val_acc: 0.9723 - val_precision_2: 0.9309 - val_auc_2: 0.9902 - val_recall_2: 0.8900
Epoch 35/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0482 - acc: 0.9798 - precision_2: 0.9452 - auc_2: 0.9970 - recall_2: 0.9306 - val_loss: 0.0794 - val_acc: 0.9723 - val_precision_2: 0.9250 - val_auc_2: 0.9913 - val_recall_2: 0.9173
Epoch 36/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0455 - acc: 0.9809 - precision_2: 0.9477 - auc_2: 0.9974 - recall_2: 0.9350 - val_loss: 0.0792 - val_acc: 0.9741 - val_precision_2: 0.9237 - val_auc_2: 0.9903 - val_recall_2: 0.9116
Epoch 37/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0447 - acc: 0.9813 - precision_2: 0.9476 - auc_2: 0.9974 - recall_2: 0.9372 - val_loss: 0.0829 - val_acc: 0.9750 - val_precision_2: 0.9043 - val_auc_2: 0.9911 - val_recall_2: 0.9247
Epoch 38/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0591 - acc: 0.9763 - precision_2: 0.9322 - auc_2: 0.9956 - recall_2: 0.9224 - val_loss: 0.0838 - val_acc: 0.9675 - val_precision_2: 0.9080 - val_auc_2: 0.9903 - val_recall_2: 0.8903
Epoch 39/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0536 - acc: 0.9780 - precision_2: 0.9378 - auc_2: 0.9964 - recall_2: 0.9267 - val_loss: 0.0888 - val_acc: 0.9703 - val_precision_2: 0.9051 - val_auc_2: 0.9888 - val_recall_2: 0.9106
Epoch 40/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0530 - acc: 0.9785 - precision_2: 0.9400 - auc_2: 0.9962 - recall_2: 0.9279 - val_loss: 0.0763 - val_acc: 0.9717 - val_precision_2: 0.9090 - val_auc_2: 0.9913 - val_recall_2: 0.9088
Epoch 41/100
32/33 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9807 - precision_2: 0.9461 - auc_2: 0.9972 - recall_2: 0.9352Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 85ms/step - loss: 0.0468 - acc: 0.9806 - precision_2: 0.9461 - auc_2: 0.9972 - recall_2: 0.9347 - val_loss: 0.0774 - val_acc: 0.9741 - val_precision_2: 0.9294 - val_auc_2: 0.9905 - val_recall_2: 0.9039
Epoch 00041: early stopping
6/6 [==============================] - 0s 26ms/step - loss: 0.0780 - acc: 0.9698 - precision_2: 0.9030 - auc_2: 0.9927 - recall_2: 0.9119
---------------TEST METRICS----------------------
jaccard_index 0.8214254774239355
test_sensitivity 0.9222627462776357
test_specifitivity 0.9788802748713564
test_accuracy 0.9696896023220486
test_precision 0.8943176547083476
test_jaccard_score 0.8214254774239355
test_dicecoef 0.9080752559349661
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-103253.h5
[1.93567369 1.63987983 1.79908385 1.80534876 1.96092887] [0.9696896  0.82142548 0.89431765 0.92226275 0.97888027]

-------------------------
Averaged metrics for Baseline - bacteria: [0.96845443 0.8204351  0.8978005  0.90920383 0.97993638]
-------------------------


-------------------------
RUN: Baseline + Augumentations - bacteria, PARAMS: {'augumentation': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-25 10:32:54.309702: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:32:54.309806: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied normalization
Data augumentation on
Prep done
Training samples: 258, channel mean: 0.20299959845194301,
Validation samples: 63, channel mean: 0.20179995867161976
Model built.
Model: "functional_7"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_4 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 256, 256, 32) 320         input_4[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_57[0][0]
__________________________________________________________________________________________________
max_pooling2d_12 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_58[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d_12[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_59[0][0]
__________________________________________________________________________________________________
max_pooling2d_13 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_60[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_13[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_61[0][0]
__________________________________________________________________________________________________
max_pooling2d_14 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_62[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_14[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_63[0][0]
__________________________________________________________________________________________________
max_pooling2d_15 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_64[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_15[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 16, 16, 512)  2359808     conv2d_65[0][0]
__________________________________________________________________________________________________
up_sampling2d_12 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_66[0][0]
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_12[0][0]
                                                                 conv2d_64[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate_12[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_67[0][0]
__________________________________________________________________________________________________
up_sampling2d_13 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_68[0][0]
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_13[0][0]
                                                                 conv2d_62[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_13[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_69[0][0]
__________________________________________________________________________________________________
up_sampling2d_14 (UpSampling2D) (None, 128, 128, 128 0           conv2d_70[0][0]
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_14[0][0]
                                                                 conv2d_60[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_14[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_71[0][0]
__________________________________________________________________________________________________
up_sampling2d_15 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_72[0][0]
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_15[0][0]
                                                                 conv2d_58[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 256, 256, 32) 27680       concatenate_15[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_73[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 256, 256, 1)  33          conv2d_74[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6930 - acc: 0.6527 - precision_3: 0.1484 - auc_3: 0.3642 - recall_3: 0.25732021-09-25 10:32:56.437253: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:32:56.437307: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 10:32:56.708322: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 10:32:56.717311: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-103254/train/plugins/profile/2021_09_25_10_32_56
2021-09-25 10:32:56.721117: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-103254/train/plugins/profile/2021_09_25_10_32_56/xeon-e-v100.trace.json.gz
2021-09-25 10:32:56.738902: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-103254/train/plugins/profile/2021_09_25_10_32_56
2021-09-25 10:32:56.747416: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-103254/train/plugins/profile/2021_09_25_10_32_56/xeon-e-v100.memory_profile.json.gz
2021-09-25 10:32:56.765994: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-103254/train/plugins/profile/2021_09_25_10_32_56Dumped tool data for xplane.pb to logs/fit/20210925-103254/train/plugins/profile/2021_09_25_10_32_56/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-103254/train/plugins/profile/2021_09_25_10_32_56/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-103254/train/plugins/profile/2021_09_25_10_32_56/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-103254/train/plugins/profile/2021_09_25_10_32_56/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-103254/train/plugins/profile/2021_09_25_10_32_56/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6908 - acc: 0.7219 - precision_3: 0.3400 - auc_3: 0.6111 - recall_3: 0.4808WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0647s vs `on_train_batch_end` time: 0.2653s). Check your callbacks.
33/33 [==============================] - 4s 108ms/step - loss: 0.5450 - acc: 0.8511 - precision_3: 0.5800 - auc_3: 0.7001 - recall_3: 0.3635 - val_loss: 0.3060 - val_acc: 0.8754 - val_precision_3: 0.5796 - val_auc_3: 0.8969 - val_recall_3: 0.7587
Epoch 2/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2419 - acc: 0.8969 - precision_3: 0.7564 - auc_3: 0.9144 - recall_3: 0.6448 - val_loss: 0.2874 - val_acc: 0.9108 - val_precision_3: 0.8549 - val_auc_3: 0.8911 - val_recall_3: 0.4945
Epoch 3/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2139 - acc: 0.9058 - precision_3: 0.8105 - auc_3: 0.9333 - recall_3: 0.6272 - val_loss: 0.2358 - val_acc: 0.9132 - val_precision_3: 0.7324 - val_auc_3: 0.9309 - val_recall_3: 0.6905
Epoch 4/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2092 - acc: 0.9093 - precision_3: 0.8264 - auc_3: 0.9321 - recall_3: 0.6366 - val_loss: 0.2480 - val_acc: 0.9042 - val_precision_3: 0.7523 - val_auc_3: 0.9158 - val_recall_3: 0.6449
Epoch 5/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1938 - acc: 0.9143 - precision_3: 0.8333 - auc_3: 0.9415 - recall_3: 0.6664 - val_loss: 0.1906 - val_acc: 0.9267 - val_precision_3: 0.8214 - val_auc_3: 0.9466 - val_recall_3: 0.6714
Epoch 6/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1927 - acc: 0.9134 - precision_3: 0.8450 - auc_3: 0.9411 - recall_3: 0.6300 - val_loss: 0.2216 - val_acc: 0.9193 - val_precision_3: 0.7845 - val_auc_3: 0.9201 - val_recall_3: 0.6739
Epoch 7/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1884 - acc: 0.9140 - precision_3: 0.8371 - auc_3: 0.9464 - recall_3: 0.6546 - val_loss: 0.1886 - val_acc: 0.9253 - val_precision_3: 0.7722 - val_auc_3: 0.9484 - val_recall_3: 0.7122
Epoch 8/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1842 - acc: 0.9164 - precision_3: 0.8380 - auc_3: 0.9481 - recall_3: 0.6731 - val_loss: 0.2162 - val_acc: 0.9153 - val_precision_3: 0.7798 - val_auc_3: 0.9365 - val_recall_3: 0.6686
Epoch 9/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1798 - acc: 0.9184 - precision_3: 0.8460 - auc_3: 0.9504 - recall_3: 0.6811 - val_loss: 0.2207 - val_acc: 0.9153 - val_precision_3: 0.7012 - val_auc_3: 0.9503 - val_recall_3: 0.8043
Epoch 10/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1776 - acc: 0.9177 - precision_3: 0.8334 - auc_3: 0.9539 - recall_3: 0.6900 - val_loss: 0.1745 - val_acc: 0.9290 - val_precision_3: 0.8174 - val_auc_3: 0.9605 - val_recall_3: 0.6999
Epoch 11/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1663 - acc: 0.9212 - precision_3: 0.8458 - auc_3: 0.9592 - recall_3: 0.7033 - val_loss: 0.1667 - val_acc: 0.9274 - val_precision_3: 0.7254 - val_auc_3: 0.9724 - val_recall_3: 0.8771
Epoch 12/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1590 - acc: 0.9244 - precision_3: 0.8385 - auc_3: 0.9618 - recall_3: 0.7348 - val_loss: 0.1495 - val_acc: 0.9450 - val_precision_3: 0.8137 - val_auc_3: 0.9703 - val_recall_3: 0.8032
Epoch 13/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1521 - acc: 0.9279 - precision_3: 0.8562 - auc_3: 0.9661 - recall_3: 0.7419 - val_loss: 0.1360 - val_acc: 0.9457 - val_precision_3: 0.7850 - val_auc_3: 0.9804 - val_recall_3: 0.8859
Epoch 14/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1264 - acc: 0.9370 - precision_3: 0.8776 - auc_3: 0.9770 - recall_3: 0.7859 - val_loss: 0.1331 - val_acc: 0.9468 - val_precision_3: 0.8364 - val_auc_3: 0.9771 - val_recall_3: 0.8292
Epoch 15/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1204 - acc: 0.9388 - precision_3: 0.8822 - auc_3: 0.9777 - recall_3: 0.7919 - val_loss: 0.1308 - val_acc: 0.9480 - val_precision_3: 0.8421 - val_auc_3: 0.9786 - val_recall_3: 0.8398
Epoch 16/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1137 - acc: 0.9428 - precision_3: 0.8946 - auc_3: 0.9799 - recall_3: 0.8066 - val_loss: 0.9856 - val_acc: 0.7774 - val_precision_3: 0.2891 - val_auc_3: 0.5946 - val_recall_3: 0.2972
Epoch 17/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1132 - acc: 0.9440 - precision_3: 0.9047 - auc_3: 0.9799 - recall_3: 0.7948 - val_loss: 0.1279 - val_acc: 0.9532 - val_precision_3: 0.8359 - val_auc_3: 0.9820 - val_recall_3: 0.8739
Epoch 18/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1200 - acc: 0.9423 - precision_3: 0.8955 - auc_3: 0.9757 - recall_3: 0.7977 - val_loss: 0.1071 - val_acc: 0.9589 - val_precision_3: 0.8524 - val_auc_3: 0.9852 - val_recall_3: 0.9045
Epoch 19/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0948 - acc: 0.9491 - precision_3: 0.9119 - auc_3: 0.9862 - recall_3: 0.8267 - val_loss: 0.0968 - val_acc: 0.9637 - val_precision_3: 0.9189 - val_auc_3: 0.9849 - val_recall_3: 0.8188
Epoch 20/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1377 - acc: 0.9415 - precision_3: 0.8934 - auc_3: 0.9687 - recall_3: 0.7962 - val_loss: 0.0810 - val_acc: 0.9682 - val_precision_3: 0.8930 - val_auc_3: 0.9914 - val_recall_3: 0.9026
Epoch 21/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1022 - acc: 0.9483 - precision_3: 0.9071 - auc_3: 0.9825 - recall_3: 0.8280 - val_loss: 0.0809 - val_acc: 0.9693 - val_precision_3: 0.9143 - val_auc_3: 0.9903 - val_recall_3: 0.8713
Epoch 22/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1115 - acc: 0.9482 - precision_3: 0.9109 - auc_3: 0.9780 - recall_3: 0.8210 - val_loss: 0.0881 - val_acc: 0.9668 - val_precision_3: 0.9108 - val_auc_3: 0.9885 - val_recall_3: 0.8626
Epoch 23/100
33/33 [==============================] - 3s 88ms/step - loss: 0.0830 - acc: 0.9533 - precision_3: 0.9216 - auc_3: 0.9896 - recall_3: 0.8481 - val_loss: 0.0934 - val_acc: 0.9655 - val_precision_3: 0.9110 - val_auc_3: 0.9869 - val_recall_3: 0.8753
Epoch 24/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0939 - acc: 0.9520 - precision_3: 0.9218 - auc_3: 0.9855 - recall_3: 0.8411 - val_loss: 0.0771 - val_acc: 0.9695 - val_precision_3: 0.8860 - val_auc_3: 0.9915 - val_recall_3: 0.9079
Epoch 25/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0772 - acc: 0.9552 - precision_3: 0.9321 - auc_3: 0.9910 - recall_3: 0.8520 - val_loss: 0.0818 - val_acc: 0.9681 - val_precision_3: 0.9097 - val_auc_3: 0.9902 - val_recall_3: 0.8855
Epoch 26/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0846 - acc: 0.9547 - precision_3: 0.9274 - auc_3: 0.9882 - recall_3: 0.8551 - val_loss: 0.0810 - val_acc: 0.9683 - val_precision_3: 0.8913 - val_auc_3: 0.9903 - val_recall_3: 0.9075
Epoch 27/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0856 - acc: 0.9540 - precision_3: 0.9257 - auc_3: 0.9872 - recall_3: 0.8472 - val_loss: 0.0672 - val_acc: 0.9734 - val_precision_3: 0.9171 - val_auc_3: 0.9938 - val_recall_3: 0.9079
Epoch 28/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0769 - acc: 0.9553 - precision_3: 0.9298 - auc_3: 0.9910 - recall_3: 0.8525 - val_loss: 0.0770 - val_acc: 0.9696 - val_precision_3: 0.8936 - val_auc_3: 0.9911 - val_recall_3: 0.9001
Epoch 29/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0730 - acc: 0.9566 - precision_3: 0.9337 - auc_3: 0.9920 - recall_3: 0.8572 - val_loss: 0.0841 - val_acc: 0.9667 - val_precision_3: 0.9071 - val_auc_3: 0.9910 - val_recall_3: 0.8948
Epoch 30/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0972 - acc: 0.9513 - precision_3: 0.9268 - auc_3: 0.9843 - recall_3: 0.8269 - val_loss: 0.0819 - val_acc: 0.9691 - val_precision_3: 0.8768 - val_auc_3: 0.9929 - val_recall_3: 0.9324
Epoch 31/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0887 - acc: 0.9528 - precision_3: 0.9213 - auc_3: 0.9880 - recall_3: 0.8483 - val_loss: 0.0773 - val_acc: 0.9697 - val_precision_3: 0.9165 - val_auc_3: 0.9917 - val_recall_3: 0.8964
Epoch 32/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0973 - acc: 0.9518 - precision_3: 0.9161 - auc_3: 0.9832 - recall_3: 0.8427 - val_loss: 0.0789 - val_acc: 0.9695 - val_precision_3: 0.9198 - val_auc_3: 0.9912 - val_recall_3: 0.8757
Epoch 33/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0780 - acc: 0.9561 - precision_3: 0.9369 - auc_3: 0.9900 - recall_3: 0.8512 - val_loss: 0.0750 - val_acc: 0.9707 - val_precision_3: 0.9296 - val_auc_3: 0.9920 - val_recall_3: 0.8807
Epoch 34/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0799 - acc: 0.9544 - precision_3: 0.9252 - auc_3: 0.9902 - recall_3: 0.8530 - val_loss: 0.0759 - val_acc: 0.9706 - val_precision_3: 0.9123 - val_auc_3: 0.9920 - val_recall_3: 0.9001
Epoch 35/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0894 - acc: 0.9541 - precision_3: 0.9294 - auc_3: 0.9847 - recall_3: 0.8409 - val_loss: 0.0804 - val_acc: 0.9683 - val_precision_3: 0.9080 - val_auc_3: 0.9917 - val_recall_3: 0.9128
Epoch 36/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0797 - acc: 0.9561 - precision_3: 0.9373 - auc_3: 0.9902 - recall_3: 0.8488 - val_loss: 0.0700 - val_acc: 0.9724 - val_precision_3: 0.9150 - val_auc_3: 0.9927 - val_recall_3: 0.9103
Epoch 37/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0764 - acc: 0.9580 - precision_3: 0.9420 - auc_3: 0.9894 - recall_3: 0.8541 - val_loss: 0.0747 - val_acc: 0.9712 - val_precision_3: 0.8854 - val_auc_3: 0.9921 - val_recall_3: 0.9192
Epoch 38/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0674 - acc: 0.9586 - precision_3: 0.9405 - auc_3: 0.9929 - recall_3: 0.8623 - val_loss: 0.0752 - val_acc: 0.9696 - val_precision_3: 0.9237 - val_auc_3: 0.9928 - val_recall_3: 0.8863
Epoch 39/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0763 - acc: 0.9571 - precision_3: 0.9381 - auc_3: 0.9902 - recall_3: 0.8544 - val_loss: 0.0817 - val_acc: 0.9692 - val_precision_3: 0.9228 - val_auc_3: 0.9899 - val_recall_3: 0.8823
Epoch 40/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0714 - acc: 0.9581 - precision_3: 0.9408 - auc_3: 0.9918 - recall_3: 0.8621 - val_loss: 0.0770 - val_acc: 0.9692 - val_precision_3: 0.8804 - val_auc_3: 0.9928 - val_recall_3: 0.9281
Epoch 41/100
32/33 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9539 - precision_3: 0.9271 - auc_3: 0.9834 - recall_3: 0.8382Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 87ms/step - loss: 0.0943 - acc: 0.9538 - precision_3: 0.9265 - auc_3: 0.9835 - recall_3: 0.8380 - val_loss: 0.0732 - val_acc: 0.9707 - val_precision_3: 0.8926 - val_auc_3: 0.9927 - val_recall_3: 0.9253
Epoch 00041: early stopping
6/6 [==============================] - 0s 25ms/step - loss: 0.0728 - acc: 0.9712 - precision_3: 0.9070 - auc_3: 0.9934 - recall_3: 0.9165
---------------TEST METRICS----------------------
jaccard_index 0.8279341770710889
test_sensitivity 0.9261710198693204
test_specifitivity 0.979865543606035
test_accuracy 0.9711493598090277
test_precision 0.8991328753011413
test_jaccard_score 0.8279341770710889
test_dicecoef 0.9124516901750472
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-103509.h5
[0. 0. 0. 0. 0.] [0.97114936 0.82793418 0.89913288 0.92617102 0.97986554]

-------------------------
Rep: 1
-------------------------

2021-09-25 10:35:09.992144: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:35:09.992204: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied normalization
Data augumentation on
Prep done
Training samples: 258, channel mean: 0.20299959845194301,
Validation samples: 63, channel mean: 0.20179995867161976
Model built.
Model: "functional_9"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_5 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 256, 256, 32) 320         input_5[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_16 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_77[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d_16[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_78[0][0]
__________________________________________________________________________________________________
max_pooling2d_17 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_79[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_17[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_80[0][0]
__________________________________________________________________________________________________
max_pooling2d_18 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_81[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_18[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_82[0][0]
__________________________________________________________________________________________________
max_pooling2d_19 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_83[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_19[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 16, 16, 512)  2359808     conv2d_84[0][0]
__________________________________________________________________________________________________
up_sampling2d_16 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_85[0][0]
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_16[0][0]
                                                                 conv2d_83[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate_16[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_86[0][0]
__________________________________________________________________________________________________
up_sampling2d_17 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_87[0][0]
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_17[0][0]
                                                                 conv2d_81[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_17[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_88[0][0]
__________________________________________________________________________________________________
up_sampling2d_18 (UpSampling2D) (None, 128, 128, 128 0           conv2d_89[0][0]
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_18[0][0]
                                                                 conv2d_79[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_18[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_90[0][0]
__________________________________________________________________________________________________
up_sampling2d_19 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_91[0][0]
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_19[0][0]
                                                                 conv2d_77[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 256, 256, 32) 27680       concatenate_19[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_92[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 256, 256, 1)  33          conv2d_93[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6931 - acc: 0.6715 - precision_4: 0.2081 - auc_4: 0.4628 - recall_4: 0.38232021-09-25 10:35:12.130359: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:35:12.130414: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 10:35:12.395795: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 10:35:12.404571: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-103509/train/plugins/profile/2021_09_25_10_35_12
2021-09-25 10:35:12.408568: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-103509/train/plugins/profile/2021_09_25_10_35_12/xeon-e-v100.trace.json.gz
2021-09-25 10:35:12.425419: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-103509/train/plugins/profile/2021_09_25_10_35_12
2021-09-25 10:35:12.432572: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-103509/train/plugins/profile/2021_09_25_10_35_12/xeon-e-v100.memory_profile.json.gz
2021-09-25 10:35:12.454957: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-103509/train/plugins/profile/2021_09_25_10_35_12Dumped tool data for xplane.pb to logs/fit/20210925-103509/train/plugins/profile/2021_09_25_10_35_12/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-103509/train/plugins/profile/2021_09_25_10_35_12/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-103509/train/plugins/profile/2021_09_25_10_35_12/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-103509/train/plugins/profile/2021_09_25_10_35_12/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-103509/train/plugins/profile/2021_09_25_10_35_12/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6906 - acc: 0.7544 - precision_4: 0.3905 - auc_4: 0.6547 - recall_4: 0.4868WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0665s vs `on_train_batch_end` time: 0.2595s). Check your callbacks.
33/33 [==============================] - 4s 113ms/step - loss: 0.4846 - acc: 0.8790 - precision_4: 0.7207 - auc_4: 0.7534 - recall_4: 0.4793 - val_loss: 0.2890 - val_acc: 0.8917 - val_precision_4: 0.6498 - val_auc_4: 0.8891 - val_recall_4: 0.6768
Epoch 2/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2351 - acc: 0.8979 - precision_4: 0.7924 - auc_4: 0.9116 - recall_4: 0.5760 - val_loss: 0.2333 - val_acc: 0.9128 - val_precision_4: 0.7406 - val_auc_4: 0.9192 - val_recall_4: 0.6528
Epoch 3/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2282 - acc: 0.9014 - precision_4: 0.8040 - auc_4: 0.9200 - recall_4: 0.5966 - val_loss: 0.2677 - val_acc: 0.9113 - val_precision_4: 0.7635 - val_auc_4: 0.9164 - val_recall_4: 0.6170
Epoch 4/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2191 - acc: 0.9044 - precision_4: 0.8170 - auc_4: 0.9264 - recall_4: 0.6001 - val_loss: 0.2638 - val_acc: 0.8940 - val_precision_4: 0.7027 - val_auc_4: 0.9091 - val_recall_4: 0.6457
Epoch 5/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2094 - acc: 0.9071 - precision_4: 0.8199 - auc_4: 0.9319 - recall_4: 0.6266 - val_loss: 0.1925 - val_acc: 0.9236 - val_precision_4: 0.8078 - val_auc_4: 0.9490 - val_recall_4: 0.6635
Epoch 6/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1924 - acc: 0.9122 - precision_4: 0.8404 - auc_4: 0.9446 - recall_4: 0.6250 - val_loss: 0.1936 - val_acc: 0.9252 - val_precision_4: 0.7976 - val_auc_4: 0.9463 - val_recall_4: 0.7052
Epoch 7/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1906 - acc: 0.9134 - precision_4: 0.8345 - auc_4: 0.9439 - recall_4: 0.6551 - val_loss: 0.2069 - val_acc: 0.9213 - val_precision_4: 0.8087 - val_auc_4: 0.9412 - val_recall_4: 0.6234
Epoch 8/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1944 - acc: 0.9112 - precision_4: 0.8250 - auc_4: 0.9446 - recall_4: 0.6485 - val_loss: 0.1997 - val_acc: 0.9175 - val_precision_4: 0.7817 - val_auc_4: 0.9498 - val_recall_4: 0.6840
Epoch 9/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1897 - acc: 0.9158 - precision_4: 0.8369 - auc_4: 0.9444 - recall_4: 0.6716 - val_loss: 0.2187 - val_acc: 0.9139 - val_precision_4: 0.6838 - val_auc_4: 0.9558 - val_recall_4: 0.8432
Epoch 10/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1725 - acc: 0.9189 - precision_4: 0.8315 - auc_4: 0.9565 - recall_4: 0.7049 - val_loss: 0.1735 - val_acc: 0.9337 - val_precision_4: 0.8299 - val_auc_4: 0.9575 - val_recall_4: 0.7215
Epoch 11/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1603 - acc: 0.9227 - precision_4: 0.8524 - auc_4: 0.9641 - recall_4: 0.7099 - val_loss: 0.1651 - val_acc: 0.9291 - val_precision_4: 0.7323 - val_auc_4: 0.9726 - val_recall_4: 0.8761
Epoch 12/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1613 - acc: 0.9243 - precision_4: 0.8395 - auc_4: 0.9597 - recall_4: 0.7362 - val_loss: 0.1459 - val_acc: 0.9419 - val_precision_4: 0.8645 - val_auc_4: 0.9714 - val_recall_4: 0.7091
Epoch 13/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1610 - acc: 0.9247 - precision_4: 0.8528 - auc_4: 0.9619 - recall_4: 0.7273 - val_loss: 0.1469 - val_acc: 0.9399 - val_precision_4: 0.7554 - val_auc_4: 0.9784 - val_recall_4: 0.8952
Epoch 14/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1240 - acc: 0.9365 - precision_4: 0.8739 - auc_4: 0.9778 - recall_4: 0.7931 - val_loss: 0.1323 - val_acc: 0.9496 - val_precision_4: 0.8820 - val_auc_4: 0.9780 - val_recall_4: 0.7907
Epoch 15/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1291 - acc: 0.9360 - precision_4: 0.8799 - auc_4: 0.9751 - recall_4: 0.7768 - val_loss: 0.1466 - val_acc: 0.9451 - val_precision_4: 0.8237 - val_auc_4: 0.9761 - val_recall_4: 0.8453
Epoch 16/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1236 - acc: 0.9397 - precision_4: 0.8865 - auc_4: 0.9761 - recall_4: 0.7925 - val_loss: 0.1207 - val_acc: 0.9565 - val_precision_4: 0.8329 - val_auc_4: 0.9857 - val_recall_4: 0.9002
Epoch 17/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1134 - acc: 0.9425 - precision_4: 0.8947 - auc_4: 0.9798 - recall_4: 0.7985 - val_loss: 0.1048 - val_acc: 0.9592 - val_precision_4: 0.8590 - val_auc_4: 0.9869 - val_recall_4: 0.8864
Epoch 18/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1183 - acc: 0.9423 - precision_4: 0.8908 - auc_4: 0.9770 - recall_4: 0.8042 - val_loss: 0.1009 - val_acc: 0.9606 - val_precision_4: 0.8729 - val_auc_4: 0.9875 - val_recall_4: 0.8881
Epoch 19/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0962 - acc: 0.9479 - precision_4: 0.9102 - auc_4: 0.9863 - recall_4: 0.8220 - val_loss: 0.0983 - val_acc: 0.9629 - val_precision_4: 0.8959 - val_auc_4: 0.9855 - val_recall_4: 0.8386
Epoch 20/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1190 - acc: 0.9451 - precision_4: 0.9063 - auc_4: 0.9757 - recall_4: 0.8063 - val_loss: 0.0876 - val_acc: 0.9668 - val_precision_4: 0.8972 - val_auc_4: 0.9892 - val_recall_4: 0.8876
Epoch 21/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0984 - acc: 0.9480 - precision_4: 0.9074 - auc_4: 0.9849 - recall_4: 0.8267 - val_loss: 0.0914 - val_acc: 0.9652 - val_precision_4: 0.9305 - val_auc_4: 0.9897 - val_recall_4: 0.8229
Epoch 22/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1096 - acc: 0.9473 - precision_4: 0.9123 - auc_4: 0.9801 - recall_4: 0.8154 - val_loss: 0.0872 - val_acc: 0.9661 - val_precision_4: 0.8866 - val_auc_4: 0.9898 - val_recall_4: 0.8866
Epoch 23/100
33/33 [==============================] - 3s 88ms/step - loss: 0.1007 - acc: 0.9490 - precision_4: 0.9085 - auc_4: 0.9839 - recall_4: 0.8323 - val_loss: 0.0898 - val_acc: 0.9650 - val_precision_4: 0.8961 - val_auc_4: 0.9896 - val_recall_4: 0.8898
Epoch 24/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0820 - acc: 0.9534 - precision_4: 0.9248 - auc_4: 0.9899 - recall_4: 0.8493 - val_loss: 0.0754 - val_acc: 0.9705 - val_precision_4: 0.8971 - val_auc_4: 0.9924 - val_recall_4: 0.9009
Epoch 25/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0954 - acc: 0.9509 - precision_4: 0.9212 - auc_4: 0.9853 - recall_4: 0.8335 - val_loss: 0.0975 - val_acc: 0.9616 - val_precision_4: 0.8555 - val_auc_4: 0.9891 - val_recall_4: 0.9094
Epoch 26/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0962 - acc: 0.9499 - precision_4: 0.9127 - auc_4: 0.9847 - recall_4: 0.8387 - val_loss: 0.0820 - val_acc: 0.9678 - val_precision_4: 0.8976 - val_auc_4: 0.9910 - val_recall_4: 0.8953
Epoch 27/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0947 - acc: 0.9503 - precision_4: 0.9172 - auc_4: 0.9851 - recall_4: 0.8322 - val_loss: 0.0738 - val_acc: 0.9715 - val_precision_4: 0.9020 - val_auc_4: 0.9933 - val_recall_4: 0.9125
Epoch 28/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1047 - acc: 0.9491 - precision_4: 0.9117 - auc_4: 0.9809 - recall_4: 0.8300 - val_loss: 0.0820 - val_acc: 0.9695 - val_precision_4: 0.9211 - val_auc_4: 0.9896 - val_recall_4: 0.8665
Epoch 29/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1082 - acc: 0.9482 - precision_4: 0.9034 - auc_4: 0.9793 - recall_4: 0.8313 - val_loss: 0.0888 - val_acc: 0.9662 - val_precision_4: 0.9044 - val_auc_4: 0.9900 - val_recall_4: 0.8947
Epoch 30/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0808 - acc: 0.9545 - precision_4: 0.9305 - auc_4: 0.9897 - recall_4: 0.8469 - val_loss: 0.0773 - val_acc: 0.9705 - val_precision_4: 0.9049 - val_auc_4: 0.9921 - val_recall_4: 0.9058
Epoch 31/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0868 - acc: 0.9535 - precision_4: 0.9222 - auc_4: 0.9884 - recall_4: 0.8523 - val_loss: 0.0755 - val_acc: 0.9703 - val_precision_4: 0.9165 - val_auc_4: 0.9928 - val_recall_4: 0.9002
Epoch 32/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0793 - acc: 0.9549 - precision_4: 0.9299 - auc_4: 0.9902 - recall_4: 0.8489 - val_loss: 0.0786 - val_acc: 0.9697 - val_precision_4: 0.9283 - val_auc_4: 0.9908 - val_recall_4: 0.8676
Epoch 33/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0803 - acc: 0.9553 - precision_4: 0.9320 - auc_4: 0.9895 - recall_4: 0.8534 - val_loss: 0.0753 - val_acc: 0.9706 - val_precision_4: 0.9330 - val_auc_4: 0.9926 - val_recall_4: 0.8764
Epoch 34/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0937 - acc: 0.9521 - precision_4: 0.9215 - auc_4: 0.9855 - recall_4: 0.8408 - val_loss: 0.0806 - val_acc: 0.9678 - val_precision_4: 0.8747 - val_auc_4: 0.9927 - val_recall_4: 0.9287
Epoch 35/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0832 - acc: 0.9549 - precision_4: 0.9316 - auc_4: 0.9878 - recall_4: 0.8471 - val_loss: 0.0779 - val_acc: 0.9693 - val_precision_4: 0.9133 - val_auc_4: 0.9927 - val_recall_4: 0.9123
Epoch 36/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0806 - acc: 0.9561 - precision_4: 0.9358 - auc_4: 0.9897 - recall_4: 0.8524 - val_loss: 0.0674 - val_acc: 0.9736 - val_precision_4: 0.9202 - val_auc_4: 0.9936 - val_recall_4: 0.9121
Epoch 37/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0735 - acc: 0.9579 - precision_4: 0.9397 - auc_4: 0.9912 - recall_4: 0.8588 - val_loss: 0.0714 - val_acc: 0.9731 - val_precision_4: 0.9029 - val_auc_4: 0.9926 - val_recall_4: 0.9112
Epoch 38/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0746 - acc: 0.9575 - precision_4: 0.9370 - auc_4: 0.9910 - recall_4: 0.8585 - val_loss: 0.0797 - val_acc: 0.9685 - val_precision_4: 0.9230 - val_auc_4: 0.9917 - val_recall_4: 0.8801
Epoch 39/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0776 - acc: 0.9567 - precision_4: 0.9379 - auc_4: 0.9895 - recall_4: 0.8520 - val_loss: 0.0749 - val_acc: 0.9715 - val_precision_4: 0.9317 - val_auc_4: 0.9919 - val_recall_4: 0.8877
Epoch 40/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0696 - acc: 0.9582 - precision_4: 0.9411 - auc_4: 0.9924 - recall_4: 0.8618 - val_loss: 0.0678 - val_acc: 0.9733 - val_precision_4: 0.9199 - val_auc_4: 0.9939 - val_recall_4: 0.9076
Epoch 41/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0705 - acc: 0.9581 - precision_4: 0.9413 - auc_4: 0.9920 - recall_4: 0.8557 - val_loss: 0.0673 - val_acc: 0.9738 - val_precision_4: 0.9229 - val_auc_4: 0.9938 - val_recall_4: 0.9092
Epoch 42/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0745 - acc: 0.9573 - precision_4: 0.9411 - auc_4: 0.9910 - recall_4: 0.8533 - val_loss: 0.0773 - val_acc: 0.9698 - val_precision_4: 0.8806 - val_auc_4: 0.9934 - val_recall_4: 0.9324
Epoch 43/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0843 - acc: 0.9526 - precision_4: 0.9202 - auc_4: 0.9885 - recall_4: 0.8435 - val_loss: 0.0783 - val_acc: 0.9700 - val_precision_4: 0.9016 - val_auc_4: 0.9926 - val_recall_4: 0.9143
Epoch 44/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0736 - acc: 0.9566 - precision_4: 0.9345 - auc_4: 0.9915 - recall_4: 0.8573 - val_loss: 0.0688 - val_acc: 0.9723 - val_precision_4: 0.9328 - val_auc_4: 0.9935 - val_recall_4: 0.8800
Epoch 45/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1229 - acc: 0.9495 - precision_4: 0.9110 - auc_4: 0.9728 - recall_4: 0.8276 - val_loss: 0.0805 - val_acc: 0.9685 - val_precision_4: 0.8892 - val_auc_4: 0.9910 - val_recall_4: 0.9075
Epoch 46/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0844 - acc: 0.9552 - precision_4: 0.9318 - auc_4: 0.9882 - recall_4: 0.8481 - val_loss: 0.0722 - val_acc: 0.9714 - val_precision_4: 0.8964 - val_auc_4: 0.9936 - val_recall_4: 0.9125
Epoch 47/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0757 - acc: 0.9570 - precision_4: 0.9355 - auc_4: 0.9909 - recall_4: 0.8610 - val_loss: 0.0766 - val_acc: 0.9700 - val_precision_4: 0.9167 - val_auc_4: 0.9919 - val_recall_4: 0.8986
Epoch 48/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0933 - acc: 0.9552 - precision_4: 0.9315 - auc_4: 0.9843 - recall_4: 0.8523 - val_loss: 0.0709 - val_acc: 0.9718 - val_precision_4: 0.8923 - val_auc_4: 0.9940 - val_recall_4: 0.9320
Epoch 49/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0686 - acc: 0.9583 - precision_4: 0.9400 - auc_4: 0.9930 - recall_4: 0.8586 - val_loss: 0.0753 - val_acc: 0.9695 - val_precision_4: 0.8817 - val_auc_4: 0.9928 - val_recall_4: 0.9314
Epoch 50/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0760 - acc: 0.9567 - precision_4: 0.9324 - auc_4: 0.9906 - recall_4: 0.8610 - val_loss: 0.0668 - val_acc: 0.9737 - val_precision_4: 0.9083 - val_auc_4: 0.9941 - val_recall_4: 0.9247
Epoch 51/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0731 - acc: 0.9585 - precision_4: 0.9458 - auc_4: 0.9904 - recall_4: 0.8549 - val_loss: 0.0711 - val_acc: 0.9716 - val_precision_4: 0.9284 - val_auc_4: 0.9934 - val_recall_4: 0.8918
Epoch 52/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0732 - acc: 0.9581 - precision_4: 0.9429 - auc_4: 0.9914 - recall_4: 0.8579 - val_loss: 0.0677 - val_acc: 0.9730 - val_precision_4: 0.9133 - val_auc_4: 0.9949 - val_recall_4: 0.9288
Epoch 53/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0671 - acc: 0.9589 - precision_4: 0.9399 - auc_4: 0.9928 - recall_4: 0.8658 - val_loss: 0.0777 - val_acc: 0.9689 - val_precision_4: 0.8548 - val_auc_4: 0.9924 - val_recall_4: 0.9343
Epoch 54/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0672 - acc: 0.9586 - precision_4: 0.9394 - auc_4: 0.9931 - recall_4: 0.8656 - val_loss: 0.0687 - val_acc: 0.9725 - val_precision_4: 0.9123 - val_auc_4: 0.9941 - val_recall_4: 0.9263
Epoch 55/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0673 - acc: 0.9589 - precision_4: 0.9465 - auc_4: 0.9931 - recall_4: 0.8584 - val_loss: 0.0687 - val_acc: 0.9731 - val_precision_4: 0.9187 - val_auc_4: 0.9925 - val_recall_4: 0.8959
Epoch 56/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0721 - acc: 0.9586 - precision_4: 0.9461 - auc_4: 0.9912 - recall_4: 0.8580 - val_loss: 0.0694 - val_acc: 0.9730 - val_precision_4: 0.9319 - val_auc_4: 0.9934 - val_recall_4: 0.8976
Epoch 57/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0804 - acc: 0.9578 - precision_4: 0.9409 - auc_4: 0.9874 - recall_4: 0.8562 - val_loss: 0.0698 - val_acc: 0.9722 - val_precision_4: 0.9056 - val_auc_4: 0.9934 - val_recall_4: 0.9192
Epoch 58/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0703 - acc: 0.9581 - precision_4: 0.9413 - auc_4: 0.9925 - recall_4: 0.8619 - val_loss: 0.0725 - val_acc: 0.9712 - val_precision_4: 0.9048 - val_auc_4: 0.9936 - val_recall_4: 0.9255
Epoch 59/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0721 - acc: 0.9583 - precision_4: 0.9427 - auc_4: 0.9918 - recall_4: 0.8610 - val_loss: 0.0648 - val_acc: 0.9741 - val_precision_4: 0.9255 - val_auc_4: 0.9937 - val_recall_4: 0.8925
Epoch 60/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0644 - acc: 0.9596 - precision_4: 0.9486 - auc_4: 0.9935 - recall_4: 0.8643 - val_loss: 0.0667 - val_acc: 0.9733 - val_precision_4: 0.9090 - val_auc_4: 0.9941 - val_recall_4: 0.9256
Epoch 61/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0744 - acc: 0.9580 - precision_4: 0.9437 - auc_4: 0.9907 - recall_4: 0.8562 - val_loss: 0.0687 - val_acc: 0.9729 - val_precision_4: 0.9314 - val_auc_4: 0.9932 - val_recall_4: 0.9009
Epoch 62/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0699 - acc: 0.9589 - precision_4: 0.9443 - auc_4: 0.9916 - recall_4: 0.8627 - val_loss: 0.0696 - val_acc: 0.9729 - val_precision_4: 0.9174 - val_auc_4: 0.9935 - val_recall_4: 0.9140
Epoch 63/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0646 - acc: 0.9600 - precision_4: 0.9448 - auc_4: 0.9935 - recall_4: 0.8702 - val_loss: 0.0652 - val_acc: 0.9748 - val_precision_4: 0.9153 - val_auc_4: 0.9942 - val_recall_4: 0.9299
Epoch 64/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0759 - acc: 0.9577 - precision_4: 0.9429 - auc_4: 0.9901 - recall_4: 0.8542 - val_loss: 0.0718 - val_acc: 0.9721 - val_precision_4: 0.8864 - val_auc_4: 0.9942 - val_recall_4: 0.9368
Epoch 65/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0851 - acc: 0.9567 - precision_4: 0.9358 - auc_4: 0.9865 - recall_4: 0.8519 - val_loss: 0.0672 - val_acc: 0.9738 - val_precision_4: 0.9318 - val_auc_4: 0.9934 - val_recall_4: 0.8995
Epoch 66/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0715 - acc: 0.9584 - precision_4: 0.9404 - auc_4: 0.9915 - recall_4: 0.8646 - val_loss: 0.0925 - val_acc: 0.9654 - val_precision_4: 0.9342 - val_auc_4: 0.9876 - val_recall_4: 0.8314
Epoch 67/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0830 - acc: 0.9566 - precision_4: 0.9360 - auc_4: 0.9876 - recall_4: 0.8505 - val_loss: 0.0680 - val_acc: 0.9730 - val_precision_4: 0.9015 - val_auc_4: 0.9932 - val_recall_4: 0.9211
Epoch 68/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0730 - acc: 0.9583 - precision_4: 0.9418 - auc_4: 0.9905 - recall_4: 0.8598 - val_loss: 0.0665 - val_acc: 0.9742 - val_precision_4: 0.9235 - val_auc_4: 0.9938 - val_recall_4: 0.9198
Epoch 69/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0760 - acc: 0.9586 - precision_4: 0.9465 - auc_4: 0.9891 - recall_4: 0.8590 - val_loss: 0.0665 - val_acc: 0.9736 - val_precision_4: 0.9319 - val_auc_4: 0.9934 - val_recall_4: 0.9033
Epoch 70/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0779 - acc: 0.9568 - precision_4: 0.9323 - auc_4: 0.9902 - recall_4: 0.8585 - val_loss: 0.0647 - val_acc: 0.9743 - val_precision_4: 0.9101 - val_auc_4: 0.9939 - val_recall_4: 0.9189
Epoch 71/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0689 - acc: 0.9590 - precision_4: 0.9433 - auc_4: 0.9922 - recall_4: 0.8684 - val_loss: 0.0648 - val_acc: 0.9743 - val_precision_4: 0.9161 - val_auc_4: 0.9939 - val_recall_4: 0.9158
Epoch 72/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0644 - acc: 0.9602 - precision_4: 0.9484 - auc_4: 0.9932 - recall_4: 0.8622 - val_loss: 0.0649 - val_acc: 0.9752 - val_precision_4: 0.9229 - val_auc_4: 0.9939 - val_recall_4: 0.9249
Epoch 73/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0807 - acc: 0.9579 - precision_4: 0.9413 - auc_4: 0.9879 - recall_4: 0.8602 - val_loss: 0.0645 - val_acc: 0.9750 - val_precision_4: 0.9292 - val_auc_4: 0.9939 - val_recall_4: 0.9104
Epoch 74/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0633 - acc: 0.9609 - precision_4: 0.9499 - auc_4: 0.9933 - recall_4: 0.8715 - val_loss: 0.0642 - val_acc: 0.9749 - val_precision_4: 0.9207 - val_auc_4: 0.9943 - val_recall_4: 0.9243
Epoch 75/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0699 - acc: 0.9596 - precision_4: 0.9474 - auc_4: 0.9917 - recall_4: 0.8631 - val_loss: 0.0636 - val_acc: 0.9752 - val_precision_4: 0.9094 - val_auc_4: 0.9940 - val_recall_4: 0.9287
Epoch 76/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0734 - acc: 0.9591 - precision_4: 0.9451 - auc_4: 0.9904 - recall_4: 0.8632 - val_loss: 0.0613 - val_acc: 0.9759 - val_precision_4: 0.9223 - val_auc_4: 0.9941 - val_recall_4: 0.9139
Epoch 77/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0742 - acc: 0.9578 - precision_4: 0.9380 - auc_4: 0.9904 - recall_4: 0.8608 - val_loss: 0.0636 - val_acc: 0.9750 - val_precision_4: 0.9099 - val_auc_4: 0.9946 - val_recall_4: 0.9262
Epoch 78/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0630 - acc: 0.9608 - precision_4: 0.9479 - auc_4: 0.9936 - recall_4: 0.8695 - val_loss: 0.0676 - val_acc: 0.9730 - val_precision_4: 0.9204 - val_auc_4: 0.9946 - val_recall_4: 0.9163
Epoch 79/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0698 - acc: 0.9590 - precision_4: 0.9452 - auc_4: 0.9917 - recall_4: 0.8640 - val_loss: 0.0667 - val_acc: 0.9739 - val_precision_4: 0.9333 - val_auc_4: 0.9933 - val_recall_4: 0.9018
Epoch 80/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0674 - acc: 0.9605 - precision_4: 0.9532 - auc_4: 0.9920 - recall_4: 0.8609 - val_loss: 0.0622 - val_acc: 0.9751 - val_precision_4: 0.9110 - val_auc_4: 0.9950 - val_recall_4: 0.9308
Epoch 81/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0911 - acc: 0.9570 - precision_4: 0.9382 - auc_4: 0.9826 - recall_4: 0.8593 - val_loss: 0.0711 - val_acc: 0.9718 - val_precision_4: 0.8852 - val_auc_4: 0.9943 - val_recall_4: 0.9434
Epoch 82/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0812 - acc: 0.9539 - precision_4: 0.9259 - auc_4: 0.9900 - recall_4: 0.8475 - val_loss: 0.0835 - val_acc: 0.9659 - val_precision_4: 0.8564 - val_auc_4: 0.9940 - val_recall_4: 0.9474
Epoch 83/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0738 - acc: 0.9577 - precision_4: 0.9365 - auc_4: 0.9902 - recall_4: 0.8636 - val_loss: 0.0681 - val_acc: 0.9733 - val_precision_4: 0.9180 - val_auc_4: 0.9939 - val_recall_4: 0.9211
Epoch 84/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0733 - acc: 0.9585 - precision_4: 0.9448 - auc_4: 0.9909 - recall_4: 0.8585 - val_loss: 0.0613 - val_acc: 0.9761 - val_precision_4: 0.8963 - val_auc_4: 0.9942 - val_recall_4: 0.9319
Epoch 85/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0838 - acc: 0.9578 - precision_4: 0.9402 - auc_4: 0.9874 - recall_4: 0.8598 - val_loss: 0.0694 - val_acc: 0.9730 - val_precision_4: 0.9362 - val_auc_4: 0.9930 - val_recall_4: 0.8946
Epoch 86/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0719 - acc: 0.9593 - precision_4: 0.9472 - auc_4: 0.9903 - recall_4: 0.8594 - val_loss: 0.0632 - val_acc: 0.9751 - val_precision_4: 0.9179 - val_auc_4: 0.9941 - val_recall_4: 0.9237
Epoch 87/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0654 - acc: 0.9608 - precision_4: 0.9509 - auc_4: 0.9926 - recall_4: 0.8697 - val_loss: 0.0662 - val_acc: 0.9737 - val_precision_4: 0.9348 - val_auc_4: 0.9936 - val_recall_4: 0.9001
Epoch 88/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0807 - acc: 0.9582 - precision_4: 0.9435 - auc_4: 0.9868 - recall_4: 0.8560 - val_loss: 0.0641 - val_acc: 0.9746 - val_precision_4: 0.9348 - val_auc_4: 0.9942 - val_recall_4: 0.9124
Epoch 89/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0681 - acc: 0.9604 - precision_4: 0.9480 - auc_4: 0.9916 - recall_4: 0.8672 - val_loss: 0.0638 - val_acc: 0.9748 - val_precision_4: 0.9194 - val_auc_4: 0.9939 - val_recall_4: 0.9208
Epoch 90/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0872 - acc: 0.9571 - precision_4: 0.9382 - auc_4: 0.9849 - recall_4: 0.8629 - val_loss: 0.0692 - val_acc: 0.9726 - val_precision_4: 0.9171 - val_auc_4: 0.9929 - val_recall_4: 0.9014
Epoch 91/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0769 - acc: 0.9589 - precision_4: 0.9421 - auc_4: 0.9880 - recall_4: 0.8637 - val_loss: 0.0641 - val_acc: 0.9748 - val_precision_4: 0.9242 - val_auc_4: 0.9945 - val_recall_4: 0.9248
Epoch 92/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0748 - acc: 0.9592 - precision_4: 0.9446 - auc_4: 0.9894 - recall_4: 0.8644 - val_loss: 0.0684 - val_acc: 0.9736 - val_precision_4: 0.9356 - val_auc_4: 0.9926 - val_recall_4: 0.8911
Epoch 93/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0778 - acc: 0.9583 - precision_4: 0.9419 - auc_4: 0.9880 - recall_4: 0.8609 - val_loss: 0.0741 - val_acc: 0.9710 - val_precision_4: 0.9377 - val_auc_4: 0.9919 - val_recall_4: 0.8831
Epoch 94/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0692 - acc: 0.9588 - precision_4: 0.9467 - auc_4: 0.9919 - recall_4: 0.8620 - val_loss: 0.0655 - val_acc: 0.9740 - val_precision_4: 0.9245 - val_auc_4: 0.9941 - val_recall_4: 0.9167
Epoch 95/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0763 - acc: 0.9590 - precision_4: 0.9468 - auc_4: 0.9885 - recall_4: 0.8592 - val_loss: 0.0622 - val_acc: 0.9762 - val_precision_4: 0.9064 - val_auc_4: 0.9947 - val_recall_4: 0.9349
Epoch 96/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0662 - acc: 0.9604 - precision_4: 0.9507 - auc_4: 0.9921 - recall_4: 0.8644 - val_loss: 0.0648 - val_acc: 0.9738 - val_precision_4: 0.9082 - val_auc_4: 0.9940 - val_recall_4: 0.9242
Epoch 97/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0578 - acc: 0.9626 - precision_4: 0.9557 - auc_4: 0.9942 - recall_4: 0.8758 - val_loss: 0.0609 - val_acc: 0.9757 - val_precision_4: 0.9367 - val_auc_4: 0.9944 - val_recall_4: 0.9072
Epoch 98/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0558 - acc: 0.9627 - precision_4: 0.9581 - auc_4: 0.9951 - recall_4: 0.8723 - val_loss: 0.0668 - val_acc: 0.9730 - val_precision_4: 0.9394 - val_auc_4: 0.9932 - val_recall_4: 0.8813
Epoch 99/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0693 - acc: 0.9605 - precision_4: 0.9504 - auc_4: 0.9912 - recall_4: 0.8644 - val_loss: 0.0604 - val_acc: 0.9760 - val_precision_4: 0.9284 - val_auc_4: 0.9949 - val_recall_4: 0.9154
Epoch 100/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0559 - acc: 0.9626 - precision_4: 0.9584 - auc_4: 0.9946 - recall_4: 0.8734 - val_loss: 0.0617 - val_acc: 0.9759 - val_precision_4: 0.9271 - val_auc_4: 0.9939 - val_recall_4: 0.9184
6/6 [==============================] - 0s 25ms/step - loss: 0.0614 - acc: 0.9754 - precision_4: 0.9128 - auc_4: 0.9955 - recall_4: 0.9379
WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbe6035b670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8519929953094271
test_sensitivity 0.9433644992563627
test_specifitivity 0.9810179922862444
test_accuracy 0.9749057345920139
test_precision 0.9059333049820665
test_jaccard_score 0.8519929953094271
test_dicecoef 0.9242700841966526
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-104017.h5
[0.97114936 0.82793418 0.89913288 0.92617102 0.97986554] [0.97490573 0.851993   0.9059333  0.9433645  0.98101799]

-------------------------
Rep: 2
-------------------------

2021-09-25 10:40:17.832837: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:40:17.832892: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied normalization
Data augumentation on
Prep done
Training samples: 258, channel mean: 0.20299959845194301,
Validation samples: 63, channel mean: 0.20179995867161976
Model built.
Model: "functional_11"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_6 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_95 (Conv2D)              (None, 256, 256, 32) 320         input_6[0][0]
__________________________________________________________________________________________________
conv2d_96 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_95[0][0]
__________________________________________________________________________________________________
max_pooling2d_20 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_96[0][0]
__________________________________________________________________________________________________
conv2d_97 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d_20[0][0]
__________________________________________________________________________________________________
conv2d_98 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_97[0][0]
__________________________________________________________________________________________________
max_pooling2d_21 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_98[0][0]
__________________________________________________________________________________________________
conv2d_99 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_21[0][0]
__________________________________________________________________________________________________
conv2d_100 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_99[0][0]
__________________________________________________________________________________________________
max_pooling2d_22 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_100[0][0]
__________________________________________________________________________________________________
conv2d_101 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_22[0][0]
__________________________________________________________________________________________________
conv2d_102 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_101[0][0]
__________________________________________________________________________________________________
max_pooling2d_23 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_102[0][0]
__________________________________________________________________________________________________
conv2d_103 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_23[0][0]
__________________________________________________________________________________________________
conv2d_104 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_103[0][0]
__________________________________________________________________________________________________
up_sampling2d_20 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_104[0][0]
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_20[0][0]
                                                                 conv2d_102[0][0]
__________________________________________________________________________________________________
conv2d_105 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_20[0][0]
__________________________________________________________________________________________________
conv2d_106 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_105[0][0]
__________________________________________________________________________________________________
up_sampling2d_21 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_106[0][0]
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_21[0][0]
                                                                 conv2d_100[0][0]
__________________________________________________________________________________________________
conv2d_107 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_21[0][0]
__________________________________________________________________________________________________
conv2d_108 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_107[0][0]
__________________________________________________________________________________________________
up_sampling2d_22 (UpSampling2D) (None, 128, 128, 128 0           conv2d_108[0][0]
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_22[0][0]
                                                                 conv2d_98[0][0]
__________________________________________________________________________________________________
conv2d_109 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_22[0][0]
__________________________________________________________________________________________________
conv2d_110 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_109[0][0]
__________________________________________________________________________________________________
up_sampling2d_23 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_110[0][0]
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_23[0][0]
                                                                 conv2d_96[0][0]
__________________________________________________________________________________________________
conv2d_111 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_23[0][0]
__________________________________________________________________________________________________
conv2d_112 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_111[0][0]
__________________________________________________________________________________________________
conv2d_113 (Conv2D)             (None, 256, 256, 1)  33          conv2d_112[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6931 - acc: 0.1346 - precision_5: 0.1530 - auc_5: 0.8557 - recall_5: 0.99982021-09-25 10:40:20.240542: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:40:20.240596: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 10:40:20.506663: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 10:40:20.515674: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-104017/train/plugins/profile/2021_09_25_10_40_20
2021-09-25 10:40:20.534795: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-104017/train/plugins/profile/2021_09_25_10_40_20/xeon-e-v100.trace.json.gz
2021-09-25 10:40:20.553491: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-104017/train/plugins/profile/2021_09_25_10_40_20
2021-09-25 10:40:20.561232: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-104017/train/plugins/profile/2021_09_25_10_40_20/xeon-e-v100.memory_profile.json.gz
2021-09-25 10:40:20.584842: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-104017/train/plugins/profile/2021_09_25_10_40_20Dumped tool data for xplane.pb to logs/fit/20210925-104017/train/plugins/profile/2021_09_25_10_40_20/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-104017/train/plugins/profile/2021_09_25_10_40_20/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-104017/train/plugins/profile/2021_09_25_10_40_20/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-104017/train/plugins/profile/2021_09_25_10_40_20/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-104017/train/plugins/profile/2021_09_25_10_40_20/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6922 - acc: 0.4464 - precision_5: 0.2404 - auc_5: 0.7922 - recall_5: 0.8840WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0650s vs `on_train_batch_end` time: 0.2811s). Check your callbacks.
33/33 [==============================] - 4s 109ms/step - loss: 0.4446 - acc: 0.8478 - precision_5: 0.5640 - auc_5: 0.7277 - recall_5: 0.3359 - val_loss: 0.2862 - val_acc: 0.8838 - val_precision_5: 0.6106 - val_auc_5: 0.8991 - val_recall_5: 0.7224
Epoch 2/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2276 - acc: 0.9013 - precision_5: 0.7990 - auc_5: 0.9183 - recall_5: 0.6071 - val_loss: 0.2301 - val_acc: 0.9103 - val_precision_5: 0.7174 - val_auc_5: 0.9234 - val_recall_5: 0.6724
Epoch 3/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2122 - acc: 0.9063 - precision_5: 0.8304 - auc_5: 0.9307 - recall_5: 0.6049 - val_loss: 0.2421 - val_acc: 0.9145 - val_precision_5: 0.7849 - val_auc_5: 0.9205 - val_recall_5: 0.6151
Epoch 4/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2081 - acc: 0.9088 - precision_5: 0.8337 - auc_5: 0.9330 - recall_5: 0.6191 - val_loss: 0.2709 - val_acc: 0.8915 - val_precision_5: 0.6684 - val_auc_5: 0.9116 - val_recall_5: 0.7095
Epoch 5/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2096 - acc: 0.9065 - precision_5: 0.8134 - auc_5: 0.9336 - recall_5: 0.6328 - val_loss: 0.2094 - val_acc: 0.9185 - val_precision_5: 0.8469 - val_auc_5: 0.9419 - val_recall_5: 0.5773
Epoch 6/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2000 - acc: 0.9093 - precision_5: 0.8280 - auc_5: 0.9381 - recall_5: 0.6159 - val_loss: 0.2098 - val_acc: 0.9206 - val_precision_5: 0.7666 - val_auc_5: 0.9345 - val_recall_5: 0.7147
Epoch 7/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1922 - acc: 0.9129 - precision_5: 0.8365 - auc_5: 0.9431 - recall_5: 0.6445 - val_loss: 0.1902 - val_acc: 0.9267 - val_precision_5: 0.8111 - val_auc_5: 0.9487 - val_recall_5: 0.6667
Epoch 8/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1842 - acc: 0.9169 - precision_5: 0.8513 - auc_5: 0.9462 - recall_5: 0.6626 - val_loss: 0.2686 - val_acc: 0.8928 - val_precision_5: 0.6394 - val_auc_5: 0.9232 - val_recall_5: 0.7836
Epoch 9/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2145 - acc: 0.9047 - precision_5: 0.8031 - auc_5: 0.9296 - recall_5: 0.6317 - val_loss: 0.2572 - val_acc: 0.9071 - val_precision_5: 0.6967 - val_auc_5: 0.9267 - val_recall_5: 0.7254
Epoch 10/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2013 - acc: 0.9119 - precision_5: 0.8267 - auc_5: 0.9370 - recall_5: 0.6526 - val_loss: 0.2164 - val_acc: 0.9198 - val_precision_5: 0.8307 - val_auc_5: 0.9318 - val_recall_5: 0.6082
Epoch 11/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1826 - acc: 0.9181 - precision_5: 0.8478 - auc_5: 0.9440 - recall_5: 0.6731 - val_loss: 0.1721 - val_acc: 0.9281 - val_precision_5: 0.7932 - val_auc_5: 0.9644 - val_recall_5: 0.7436
Epoch 12/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1734 - acc: 0.9207 - precision_5: 0.8365 - auc_5: 0.9505 - recall_5: 0.6997 - val_loss: 0.1625 - val_acc: 0.9417 - val_precision_5: 0.8147 - val_auc_5: 0.9571 - val_recall_5: 0.7719
Epoch 13/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1579 - acc: 0.9270 - precision_5: 0.8645 - auc_5: 0.9590 - recall_5: 0.7210 - val_loss: 0.1681 - val_acc: 0.9309 - val_precision_5: 0.7238 - val_auc_5: 0.9727 - val_recall_5: 0.8826
Epoch 14/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1401 - acc: 0.9333 - precision_5: 0.8661 - auc_5: 0.9663 - recall_5: 0.7697 - val_loss: 0.1419 - val_acc: 0.9469 - val_precision_5: 0.8973 - val_auc_5: 0.9738 - val_recall_5: 0.7537
Epoch 15/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1332 - acc: 0.9358 - precision_5: 0.8817 - auc_5: 0.9707 - recall_5: 0.7660 - val_loss: 0.1338 - val_acc: 0.9496 - val_precision_5: 0.8478 - val_auc_5: 0.9759 - val_recall_5: 0.8429
Epoch 16/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1257 - acc: 0.9396 - precision_5: 0.8883 - auc_5: 0.9739 - recall_5: 0.7879 - val_loss: 0.1194 - val_acc: 0.9543 - val_precision_5: 0.8297 - val_auc_5: 0.9844 - val_recall_5: 0.8882
Epoch 17/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1286 - acc: 0.9382 - precision_5: 0.8888 - auc_5: 0.9736 - recall_5: 0.7694 - val_loss: 0.1096 - val_acc: 0.9556 - val_precision_5: 0.8350 - val_auc_5: 0.9856 - val_recall_5: 0.8950
Epoch 18/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1233 - acc: 0.9410 - precision_5: 0.8911 - auc_5: 0.9758 - recall_5: 0.7924 - val_loss: 0.1069 - val_acc: 0.9573 - val_precision_5: 0.8467 - val_auc_5: 0.9876 - val_recall_5: 0.9019
Epoch 19/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1015 - acc: 0.9468 - precision_5: 0.9074 - auc_5: 0.9845 - recall_5: 0.8196 - val_loss: 0.1008 - val_acc: 0.9630 - val_precision_5: 0.8836 - val_auc_5: 0.9853 - val_recall_5: 0.8546
Epoch 20/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1221 - acc: 0.9424 - precision_5: 0.8967 - auc_5: 0.9747 - recall_5: 0.7946 - val_loss: 0.0874 - val_acc: 0.9680 - val_precision_5: 0.8940 - val_auc_5: 0.9887 - val_recall_5: 0.9003
Epoch 21/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1120 - acc: 0.9456 - precision_5: 0.9009 - auc_5: 0.9799 - recall_5: 0.8204 - val_loss: 0.0836 - val_acc: 0.9680 - val_precision_5: 0.9013 - val_auc_5: 0.9902 - val_recall_5: 0.8767
Epoch 22/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1249 - acc: 0.9433 - precision_5: 0.9025 - auc_5: 0.9731 - recall_5: 0.7972 - val_loss: 0.0965 - val_acc: 0.9635 - val_precision_5: 0.8734 - val_auc_5: 0.9864 - val_recall_5: 0.8838
Epoch 23/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0922 - acc: 0.9509 - precision_5: 0.9188 - auc_5: 0.9867 - recall_5: 0.8348 - val_loss: 0.0921 - val_acc: 0.9652 - val_precision_5: 0.8852 - val_auc_5: 0.9882 - val_recall_5: 0.9051
Epoch 24/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0863 - acc: 0.9521 - precision_5: 0.9233 - auc_5: 0.9888 - recall_5: 0.8432 - val_loss: 0.0757 - val_acc: 0.9703 - val_precision_5: 0.8859 - val_auc_5: 0.9923 - val_recall_5: 0.9145
Epoch 25/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0833 - acc: 0.9533 - precision_5: 0.9299 - auc_5: 0.9893 - recall_5: 0.8419 - val_loss: 0.0789 - val_acc: 0.9692 - val_precision_5: 0.9082 - val_auc_5: 0.9910 - val_recall_5: 0.8949
Epoch 26/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0905 - acc: 0.9528 - precision_5: 0.9225 - auc_5: 0.9861 - recall_5: 0.8489 - val_loss: 0.0814 - val_acc: 0.9676 - val_precision_5: 0.8826 - val_auc_5: 0.9911 - val_recall_5: 0.9137
Epoch 27/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0854 - acc: 0.9522 - precision_5: 0.9198 - auc_5: 0.9885 - recall_5: 0.8437 - val_loss: 0.0741 - val_acc: 0.9715 - val_precision_5: 0.9160 - val_auc_5: 0.9927 - val_recall_5: 0.8955
Epoch 28/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1054 - acc: 0.9496 - precision_5: 0.9166 - auc_5: 0.9814 - recall_5: 0.8281 - val_loss: 0.0821 - val_acc: 0.9686 - val_precision_5: 0.8838 - val_auc_5: 0.9911 - val_recall_5: 0.9050
Epoch 29/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0898 - acc: 0.9520 - precision_5: 0.9190 - auc_5: 0.9870 - recall_5: 0.8431 - val_loss: 0.0862 - val_acc: 0.9666 - val_precision_5: 0.9013 - val_auc_5: 0.9912 - val_recall_5: 0.9011
Epoch 30/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0855 - acc: 0.9539 - precision_5: 0.9330 - auc_5: 0.9878 - recall_5: 0.8403 - val_loss: 0.0787 - val_acc: 0.9702 - val_precision_5: 0.8920 - val_auc_5: 0.9921 - val_recall_5: 0.9199
Epoch 31/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0899 - acc: 0.9523 - precision_5: 0.9194 - auc_5: 0.9872 - recall_5: 0.8470 - val_loss: 0.0791 - val_acc: 0.9688 - val_precision_5: 0.9163 - val_auc_5: 0.9919 - val_recall_5: 0.8908
Epoch 32/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0888 - acc: 0.9525 - precision_5: 0.9268 - auc_5: 0.9868 - recall_5: 0.8356 - val_loss: 0.0762 - val_acc: 0.9700 - val_precision_5: 0.9220 - val_auc_5: 0.9919 - val_recall_5: 0.8773
Epoch 33/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0762 - acc: 0.9562 - precision_5: 0.9360 - auc_5: 0.9909 - recall_5: 0.8576 - val_loss: 0.0815 - val_acc: 0.9683 - val_precision_5: 0.9003 - val_auc_5: 0.9920 - val_recall_5: 0.8981
Epoch 34/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0868 - acc: 0.9533 - precision_5: 0.9258 - auc_5: 0.9884 - recall_5: 0.8457 - val_loss: 0.0763 - val_acc: 0.9710 - val_precision_5: 0.9151 - val_auc_5: 0.9920 - val_recall_5: 0.8993
Epoch 35/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0923 - acc: 0.9535 - precision_5: 0.9263 - auc_5: 0.9839 - recall_5: 0.8447 - val_loss: 0.0798 - val_acc: 0.9684 - val_precision_5: 0.8997 - val_auc_5: 0.9928 - val_recall_5: 0.9240
Epoch 36/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0883 - acc: 0.9546 - precision_5: 0.9304 - auc_5: 0.9870 - recall_5: 0.8472 - val_loss: 0.0726 - val_acc: 0.9718 - val_precision_5: 0.9208 - val_auc_5: 0.9920 - val_recall_5: 0.8993
Epoch 37/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0746 - acc: 0.9570 - precision_5: 0.9371 - auc_5: 0.9910 - recall_5: 0.8554 - val_loss: 0.0794 - val_acc: 0.9699 - val_precision_5: 0.8761 - val_auc_5: 0.9917 - val_recall_5: 0.9219
Epoch 38/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0858 - acc: 0.9544 - precision_5: 0.9241 - auc_5: 0.9885 - recall_5: 0.8499 - val_loss: 0.0806 - val_acc: 0.9679 - val_precision_5: 0.9095 - val_auc_5: 0.9918 - val_recall_5: 0.8917
Epoch 39/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0792 - acc: 0.9558 - precision_5: 0.9345 - auc_5: 0.9891 - recall_5: 0.8504 - val_loss: 0.0850 - val_acc: 0.9672 - val_precision_5: 0.9287 - val_auc_5: 0.9909 - val_recall_5: 0.8621
Epoch 40/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0786 - acc: 0.9554 - precision_5: 0.9315 - auc_5: 0.9903 - recall_5: 0.8550 - val_loss: 0.0735 - val_acc: 0.9705 - val_precision_5: 0.8936 - val_auc_5: 0.9932 - val_recall_5: 0.9200
Epoch 41/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0800 - acc: 0.9564 - precision_5: 0.9332 - auc_5: 0.9889 - recall_5: 0.8530 - val_loss: 0.0707 - val_acc: 0.9724 - val_precision_5: 0.9120 - val_auc_5: 0.9927 - val_recall_5: 0.9123
Epoch 42/100
33/33 [==============================] - 3s 88ms/step - loss: 0.0752 - acc: 0.9578 - precision_5: 0.9416 - auc_5: 0.9905 - recall_5: 0.8570 - val_loss: 0.0663 - val_acc: 0.9740 - val_precision_5: 0.9092 - val_auc_5: 0.9939 - val_recall_5: 0.9253
Epoch 43/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0801 - acc: 0.9561 - precision_5: 0.9359 - auc_5: 0.9898 - recall_5: 0.8514 - val_loss: 0.0861 - val_acc: 0.9682 - val_precision_5: 0.8772 - val_auc_5: 0.9925 - val_recall_5: 0.9342
Epoch 44/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0825 - acc: 0.9544 - precision_5: 0.9263 - auc_5: 0.9889 - recall_5: 0.8512 - val_loss: 0.0707 - val_acc: 0.9720 - val_precision_5: 0.9026 - val_auc_5: 0.9933 - val_recall_5: 0.9127
Epoch 45/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0892 - acc: 0.9536 - precision_5: 0.9285 - auc_5: 0.9856 - recall_5: 0.8407 - val_loss: 0.0743 - val_acc: 0.9710 - val_precision_5: 0.8886 - val_auc_5: 0.9934 - val_recall_5: 0.9270
Epoch 46/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0846 - acc: 0.9558 - precision_5: 0.9323 - auc_5: 0.9874 - recall_5: 0.8527 - val_loss: 0.0723 - val_acc: 0.9712 - val_precision_5: 0.8880 - val_auc_5: 0.9938 - val_recall_5: 0.9223
Epoch 47/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0778 - acc: 0.9570 - precision_5: 0.9340 - auc_5: 0.9895 - recall_5: 0.8636 - val_loss: 0.0779 - val_acc: 0.9698 - val_precision_5: 0.9285 - val_auc_5: 0.9914 - val_recall_5: 0.8833
Epoch 48/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0850 - acc: 0.9567 - precision_5: 0.9361 - auc_5: 0.9861 - recall_5: 0.8567 - val_loss: 0.0695 - val_acc: 0.9725 - val_precision_5: 0.8974 - val_auc_5: 0.9939 - val_recall_5: 0.9300
Epoch 49/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0653 - acc: 0.9593 - precision_5: 0.9442 - auc_5: 0.9934 - recall_5: 0.8632 - val_loss: 0.0749 - val_acc: 0.9700 - val_precision_5: 0.8780 - val_auc_5: 0.9935 - val_recall_5: 0.9401
Epoch 50/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0860 - acc: 0.9551 - precision_5: 0.9273 - auc_5: 0.9875 - recall_5: 0.8528 - val_loss: 0.0676 - val_acc: 0.9729 - val_precision_5: 0.9012 - val_auc_5: 0.9940 - val_recall_5: 0.9278
Epoch 51/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0646 - acc: 0.9597 - precision_5: 0.9438 - auc_5: 0.9936 - recall_5: 0.8699 - val_loss: 0.0729 - val_acc: 0.9710 - val_precision_5: 0.9218 - val_auc_5: 0.9934 - val_recall_5: 0.8950
Epoch 52/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0723 - acc: 0.9586 - precision_5: 0.9428 - auc_5: 0.9912 - recall_5: 0.8612 - val_loss: 0.0671 - val_acc: 0.9727 - val_precision_5: 0.9152 - val_auc_5: 0.9948 - val_recall_5: 0.9244
Epoch 53/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0692 - acc: 0.9594 - precision_5: 0.9457 - auc_5: 0.9920 - recall_5: 0.8623 - val_loss: 0.0720 - val_acc: 0.9716 - val_precision_5: 0.8621 - val_auc_5: 0.9937 - val_recall_5: 0.9467
Epoch 54/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0721 - acc: 0.9590 - precision_5: 0.9401 - auc_5: 0.9911 - recall_5: 0.8672 - val_loss: 0.0679 - val_acc: 0.9736 - val_precision_5: 0.9155 - val_auc_5: 0.9936 - val_recall_5: 0.9295
Epoch 55/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0692 - acc: 0.9594 - precision_5: 0.9456 - auc_5: 0.9917 - recall_5: 0.8634 - val_loss: 0.0709 - val_acc: 0.9716 - val_precision_5: 0.8938 - val_auc_5: 0.9929 - val_recall_5: 0.9154
Epoch 56/100
32/33 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9588 - precision_5: 0.9409 - auc_5: 0.9930 - recall_5: 0.8664Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 86ms/step - loss: 0.0672 - acc: 0.9586 - precision_5: 0.9412 - auc_5: 0.9930 - recall_5: 0.8658 - val_loss: 0.0681 - val_acc: 0.9732 - val_precision_5: 0.9195 - val_auc_5: 0.9939 - val_recall_5: 0.9130
Epoch 00056: early stopping
6/6 [==============================] - 0s 25ms/step - loss: 0.0693 - acc: 0.9727 - precision_5: 0.8963 - auc_5: 0.9946 - recall_5: 0.9408
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbdc8292790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8376362497925243
test_sensitivity 0.9472727728480473
test_specifitivity 0.9769749092451724
test_accuracy 0.9721533881293403
test_precision 0.8885488285880553
test_jaccard_score 0.8376362497925243
test_dicecoef 0.9169715750256547
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-104317.h5
[1.94605509 1.67992717 1.80506618 1.86953552 1.96088354] [0.97215339 0.83763625 0.88854883 0.94727277 0.97697491]

-------------------------
Averaged metrics for Baseline + Augumentations - bacteria: [0.97273616 0.83918781 0.89787167 0.9389361  0.97928615]
-------------------------


-------------------------
RUN: Baseline + Histogram Equalization - bacteria, PARAMS: {'histogram_equalization': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-25 10:43:17.764495: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:43:17.764551: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied histogram equalization on all input images
Applied normalization
Data augumentation off
Prep done
Training samples: 258, channel mean: 0.1526593703632206,
Validation samples: 63, channel mean: 0.14989169631611324
Model built.
Model: "functional_13"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_7 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_114 (Conv2D)             (None, 256, 256, 32) 320         input_7[0][0]
__________________________________________________________________________________________________
conv2d_115 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_114[0][0]
__________________________________________________________________________________________________
max_pooling2d_24 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_115[0][0]
__________________________________________________________________________________________________
conv2d_116 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_24[0][0]
__________________________________________________________________________________________________
conv2d_117 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_116[0][0]
__________________________________________________________________________________________________
max_pooling2d_25 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_117[0][0]
__________________________________________________________________________________________________
conv2d_118 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_25[0][0]
__________________________________________________________________________________________________
conv2d_119 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_118[0][0]
__________________________________________________________________________________________________
max_pooling2d_26 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_119[0][0]
__________________________________________________________________________________________________
conv2d_120 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_26[0][0]
__________________________________________________________________________________________________
conv2d_121 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_120[0][0]
__________________________________________________________________________________________________
max_pooling2d_27 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_121[0][0]
__________________________________________________________________________________________________
conv2d_122 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_27[0][0]
__________________________________________________________________________________________________
conv2d_123 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_122[0][0]
__________________________________________________________________________________________________
up_sampling2d_24 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_123[0][0]
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_24[0][0]
                                                                 conv2d_121[0][0]
__________________________________________________________________________________________________
conv2d_124 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_24[0][0]
__________________________________________________________________________________________________
conv2d_125 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_124[0][0]
__________________________________________________________________________________________________
up_sampling2d_25 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_125[0][0]
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_25[0][0]
                                                                 conv2d_119[0][0]
__________________________________________________________________________________________________
conv2d_126 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_25[0][0]
__________________________________________________________________________________________________
conv2d_127 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_126[0][0]
__________________________________________________________________________________________________
up_sampling2d_26 (UpSampling2D) (None, 128, 128, 128 0           conv2d_127[0][0]
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_26[0][0]
                                                                 conv2d_117[0][0]
__________________________________________________________________________________________________
conv2d_128 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_26[0][0]
__________________________________________________________________________________________________
conv2d_129 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_128[0][0]
__________________________________________________________________________________________________
up_sampling2d_27 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_129[0][0]
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_27[0][0]
                                                                 conv2d_115[0][0]
__________________________________________________________________________________________________
conv2d_130 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_27[0][0]
__________________________________________________________________________________________________
conv2d_131 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_130[0][0]
__________________________________________________________________________________________________
conv2d_132 (Conv2D)             (None, 256, 256, 1)  33          conv2d_131[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6932 - acc: 0.6696 - precision_6: 0.1034 - auc_6: 0.4454 - recall_6: 0.14682021-09-25 10:43:22.478594: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:43:22.478660: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 10:43:22.753085: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 10:43:22.761915: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-104317/train/plugins/profile/2021_09_25_10_43_22
2021-09-25 10:43:22.765105: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-104317/train/plugins/profile/2021_09_25_10_43_22/xeon-e-v100.trace.json.gz
2021-09-25 10:43:22.783527: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-104317/train/plugins/profile/2021_09_25_10_43_22
2021-09-25 10:43:22.791100: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-104317/train/plugins/profile/2021_09_25_10_43_22/xeon-e-v100.memory_profile.json.gz
2021-09-25 10:43:22.811648: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-104317/train/plugins/profile/2021_09_25_10_43_22Dumped tool data for xplane.pb to logs/fit/20210925-104317/train/plugins/profile/2021_09_25_10_43_22/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-104317/train/plugins/profile/2021_09_25_10_43_22/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-104317/train/plugins/profile/2021_09_25_10_43_22/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-104317/train/plugins/profile/2021_09_25_10_43_22/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-104317/train/plugins/profile/2021_09_25_10_43_22/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6902 - acc: 0.7342 - precision_6: 0.3242 - auc_6: 0.6593 - recall_6: 0.4100WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0659s vs `on_train_batch_end` time: 0.2685s). Check your callbacks.
33/33 [==============================] - 3s 105ms/step - loss: 0.7139 - acc: 0.8640 - precision_6: 0.5831 - auc_6: 0.7774 - recall_6: 0.5901 - val_loss: 0.2668 - val_acc: 0.9012 - val_precision_6: 0.6867 - val_auc_6: 0.9135 - val_recall_6: 0.6848
Epoch 2/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2449 - acc: 0.9063 - precision_6: 0.7471 - auc_6: 0.9174 - recall_6: 0.6457 - val_loss: 0.2167 - val_acc: 0.9112 - val_precision_6: 0.6976 - val_auc_6: 0.9448 - val_recall_6: 0.7303
Epoch 3/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2144 - acc: 0.9137 - precision_6: 0.7730 - auc_6: 0.9401 - recall_6: 0.6685 - val_loss: 0.2164 - val_acc: 0.9143 - val_precision_6: 0.8657 - val_auc_6: 0.9425 - val_recall_6: 0.5268
Epoch 4/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2079 - acc: 0.9184 - precision_6: 0.7950 - auc_6: 0.9418 - recall_6: 0.6751 - val_loss: 0.2174 - val_acc: 0.9113 - val_precision_6: 0.7561 - val_auc_6: 0.9421 - val_recall_6: 0.7006
Epoch 5/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1942 - acc: 0.9227 - precision_6: 0.7962 - auc_6: 0.9513 - recall_6: 0.7084 - val_loss: 0.2022 - val_acc: 0.9218 - val_precision_6: 0.8526 - val_auc_6: 0.9505 - val_recall_6: 0.5973
Epoch 6/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1959 - acc: 0.9206 - precision_6: 0.7903 - auc_6: 0.9506 - recall_6: 0.7000 - val_loss: 0.1864 - val_acc: 0.9271 - val_precision_6: 0.8096 - val_auc_6: 0.9512 - val_recall_6: 0.7042
Epoch 7/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1894 - acc: 0.9226 - precision_6: 0.7993 - auc_6: 0.9552 - recall_6: 0.7029 - val_loss: 0.1877 - val_acc: 0.9324 - val_precision_6: 0.8245 - val_auc_6: 0.9563 - val_recall_6: 0.6982
Epoch 8/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1881 - acc: 0.9251 - precision_6: 0.8010 - auc_6: 0.9547 - recall_6: 0.7207 - val_loss: 0.2023 - val_acc: 0.9207 - val_precision_6: 0.8335 - val_auc_6: 0.9541 - val_recall_6: 0.6405
Epoch 9/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1892 - acc: 0.9246 - precision_6: 0.8003 - auc_6: 0.9548 - recall_6: 0.7178 - val_loss: 0.1868 - val_acc: 0.9272 - val_precision_6: 0.7767 - val_auc_6: 0.9626 - val_recall_6: 0.7540
Epoch 10/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1745 - acc: 0.9281 - precision_6: 0.7999 - auc_6: 0.9624 - recall_6: 0.7474 - val_loss: 0.1609 - val_acc: 0.9312 - val_precision_6: 0.7927 - val_auc_6: 0.9711 - val_recall_6: 0.7555
Epoch 11/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1699 - acc: 0.9293 - precision_6: 0.8049 - auc_6: 0.9643 - recall_6: 0.7494 - val_loss: 0.1650 - val_acc: 0.9306 - val_precision_6: 0.7470 - val_auc_6: 0.9726 - val_recall_6: 0.8550
Epoch 12/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1482 - acc: 0.9395 - precision_6: 0.8152 - auc_6: 0.9730 - recall_6: 0.8146 - val_loss: 0.1342 - val_acc: 0.9468 - val_precision_6: 0.8560 - val_auc_6: 0.9766 - val_recall_6: 0.7599
Epoch 13/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1383 - acc: 0.9424 - precision_6: 0.8264 - auc_6: 0.9770 - recall_6: 0.8196 - val_loss: 0.1306 - val_acc: 0.9505 - val_precision_6: 0.8178 - val_auc_6: 0.9823 - val_recall_6: 0.8683
Epoch 14/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1251 - acc: 0.9502 - precision_6: 0.8451 - auc_6: 0.9806 - recall_6: 0.8518 - val_loss: 0.1291 - val_acc: 0.9503 - val_precision_6: 0.8760 - val_auc_6: 0.9788 - val_recall_6: 0.8029
Epoch 15/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1298 - acc: 0.9499 - precision_6: 0.8534 - auc_6: 0.9790 - recall_6: 0.8375 - val_loss: 0.1194 - val_acc: 0.9543 - val_precision_6: 0.8507 - val_auc_6: 0.9817 - val_recall_6: 0.8738
Epoch 16/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1089 - acc: 0.9566 - precision_6: 0.8689 - auc_6: 0.9850 - recall_6: 0.8648 - val_loss: 0.0959 - val_acc: 0.9640 - val_precision_6: 0.8779 - val_auc_6: 0.9883 - val_recall_6: 0.8918
Epoch 17/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0962 - acc: 0.9620 - precision_6: 0.8903 - auc_6: 0.9885 - recall_6: 0.8754 - val_loss: 0.0948 - val_acc: 0.9641 - val_precision_6: 0.8842 - val_auc_6: 0.9891 - val_recall_6: 0.8882
Epoch 18/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1090 - acc: 0.9579 - precision_6: 0.8783 - auc_6: 0.9842 - recall_6: 0.8617 - val_loss: 0.1057 - val_acc: 0.9595 - val_precision_6: 0.8365 - val_auc_6: 0.9882 - val_recall_6: 0.9342
Epoch 19/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0983 - acc: 0.9625 - precision_6: 0.8921 - auc_6: 0.9875 - recall_6: 0.8769 - val_loss: 0.0929 - val_acc: 0.9655 - val_precision_6: 0.9180 - val_auc_6: 0.9872 - val_recall_6: 0.8337
Epoch 20/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0967 - acc: 0.9626 - precision_6: 0.8917 - auc_6: 0.9884 - recall_6: 0.8776 - val_loss: 0.0807 - val_acc: 0.9692 - val_precision_6: 0.8884 - val_auc_6: 0.9920 - val_recall_6: 0.9161
Epoch 21/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0832 - acc: 0.9681 - precision_6: 0.9049 - auc_6: 0.9911 - recall_6: 0.8994 - val_loss: 0.0690 - val_acc: 0.9741 - val_precision_6: 0.9120 - val_auc_6: 0.9934 - val_recall_6: 0.9101
Epoch 22/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0741 - acc: 0.9711 - precision_6: 0.9170 - auc_6: 0.9931 - recall_6: 0.9049 - val_loss: 0.0764 - val_acc: 0.9713 - val_precision_6: 0.9228 - val_auc_6: 0.9908 - val_recall_6: 0.8822
Epoch 23/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0712 - acc: 0.9718 - precision_6: 0.9136 - auc_6: 0.9937 - recall_6: 0.9139 - val_loss: 0.0824 - val_acc: 0.9694 - val_precision_6: 0.8962 - val_auc_6: 0.9901 - val_recall_6: 0.9197
Epoch 24/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0673 - acc: 0.9735 - precision_6: 0.9217 - auc_6: 0.9942 - recall_6: 0.9158 - val_loss: 0.0741 - val_acc: 0.9715 - val_precision_6: 0.8778 - val_auc_6: 0.9935 - val_recall_6: 0.9346
Epoch 25/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0696 - acc: 0.9724 - precision_6: 0.9184 - auc_6: 0.9938 - recall_6: 0.9124 - val_loss: 0.0740 - val_acc: 0.9712 - val_precision_6: 0.8999 - val_auc_6: 0.9923 - val_recall_6: 0.9192
Epoch 26/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0887 - acc: 0.9653 - precision_6: 0.8914 - auc_6: 0.9903 - recall_6: 0.8973 - val_loss: 0.0862 - val_acc: 0.9691 - val_precision_6: 0.8960 - val_auc_6: 0.9888 - val_recall_6: 0.9068
Epoch 27/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0766 - acc: 0.9707 - precision_6: 0.9149 - auc_6: 0.9924 - recall_6: 0.9051 - val_loss: 0.0697 - val_acc: 0.9733 - val_precision_6: 0.9132 - val_auc_6: 0.9924 - val_recall_6: 0.9120
Epoch 28/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0701 - acc: 0.9725 - precision_6: 0.9214 - auc_6: 0.9939 - recall_6: 0.9096 - val_loss: 0.0755 - val_acc: 0.9719 - val_precision_6: 0.9004 - val_auc_6: 0.9913 - val_recall_6: 0.9091
Epoch 29/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0676 - acc: 0.9735 - precision_6: 0.9210 - auc_6: 0.9941 - recall_6: 0.9165 - val_loss: 0.0795 - val_acc: 0.9694 - val_precision_6: 0.9160 - val_auc_6: 0.9908 - val_recall_6: 0.9014
Epoch 30/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0636 - acc: 0.9749 - precision_6: 0.9302 - auc_6: 0.9948 - recall_6: 0.9154 - val_loss: 0.0744 - val_acc: 0.9720 - val_precision_6: 0.9027 - val_auc_6: 0.9916 - val_recall_6: 0.9194
Epoch 31/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0591 - acc: 0.9761 - precision_6: 0.9295 - auc_6: 0.9956 - recall_6: 0.9241 - val_loss: 0.0750 - val_acc: 0.9714 - val_precision_6: 0.9290 - val_auc_6: 0.9915 - val_recall_6: 0.8936
Epoch 32/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0555 - acc: 0.9776 - precision_6: 0.9364 - auc_6: 0.9961 - recall_6: 0.9256 - val_loss: 0.0700 - val_acc: 0.9735 - val_precision_6: 0.9039 - val_auc_6: 0.9933 - val_recall_6: 0.9244
Epoch 33/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0524 - acc: 0.9788 - precision_6: 0.9387 - auc_6: 0.9964 - recall_6: 0.9310 - val_loss: 0.0695 - val_acc: 0.9739 - val_precision_6: 0.9255 - val_auc_6: 0.9928 - val_recall_6: 0.9075
Epoch 34/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0508 - acc: 0.9792 - precision_6: 0.9437 - auc_6: 0.9968 - recall_6: 0.9281 - val_loss: 0.0744 - val_acc: 0.9737 - val_precision_6: 0.9307 - val_auc_6: 0.9907 - val_recall_6: 0.9002
Epoch 35/100
32/33 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9800 - precision_6: 0.9453 - auc_6: 0.9968 - recall_6: 0.9323Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 85ms/step - loss: 0.0488 - acc: 0.9800 - precision_6: 0.9451 - auc_6: 0.9968 - recall_6: 0.9321 - val_loss: 0.0829 - val_acc: 0.9723 - val_precision_6: 0.9352 - val_auc_6: 0.9900 - val_recall_6: 0.9057
Epoch 00035: early stopping
6/6 [==============================] - 0s 26ms/step - loss: 0.0823 - acc: 0.9680 - precision_6: 0.8961 - auc_6: 0.9920 - recall_6: 0.9081
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbdc82f6040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8161393954941695
test_sensitivity 0.9244706806370214
test_specifitivity 0.9764033400367229
test_accuracy 0.967973158094618
test_precision 0.8836148807158843
test_jaccard_score 0.8161393954941695
test_dicecoef 0.9035811884755985
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-104516.h5
[0. 0. 0. 0. 0.] [0.96797316 0.8161394  0.88361488 0.92447068 0.97640334]

-------------------------
Rep: 1
-------------------------

2021-09-25 10:45:17.109042: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:45:17.109102: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied histogram equalization on all input images
Applied normalization
Data augumentation off
Prep done
Training samples: 258, channel mean: 0.1526593703632206,
Validation samples: 63, channel mean: 0.14989169631611324
Model built.
Model: "functional_15"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_8 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_133 (Conv2D)             (None, 256, 256, 32) 320         input_8[0][0]
__________________________________________________________________________________________________
conv2d_134 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_133[0][0]
__________________________________________________________________________________________________
max_pooling2d_28 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_134[0][0]
__________________________________________________________________________________________________
conv2d_135 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_28[0][0]
__________________________________________________________________________________________________
conv2d_136 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_135[0][0]
__________________________________________________________________________________________________
max_pooling2d_29 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_136[0][0]
__________________________________________________________________________________________________
conv2d_137 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_29[0][0]
__________________________________________________________________________________________________
conv2d_138 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_137[0][0]
__________________________________________________________________________________________________
max_pooling2d_30 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_138[0][0]
__________________________________________________________________________________________________
conv2d_139 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_30[0][0]
__________________________________________________________________________________________________
conv2d_140 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_139[0][0]
__________________________________________________________________________________________________
max_pooling2d_31 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_140[0][0]
__________________________________________________________________________________________________
conv2d_141 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_31[0][0]
__________________________________________________________________________________________________
conv2d_142 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_141[0][0]
__________________________________________________________________________________________________
up_sampling2d_28 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_142[0][0]
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_28[0][0]
                                                                 conv2d_140[0][0]
__________________________________________________________________________________________________
conv2d_143 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_28[0][0]
__________________________________________________________________________________________________
conv2d_144 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_143[0][0]
__________________________________________________________________________________________________
up_sampling2d_29 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_144[0][0]
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_29[0][0]
                                                                 conv2d_138[0][0]
__________________________________________________________________________________________________
conv2d_145 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_29[0][0]
__________________________________________________________________________________________________
conv2d_146 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_145[0][0]
__________________________________________________________________________________________________
up_sampling2d_30 (UpSampling2D) (None, 128, 128, 128 0           conv2d_146[0][0]
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_30[0][0]
                                                                 conv2d_136[0][0]
__________________________________________________________________________________________________
conv2d_147 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_30[0][0]
__________________________________________________________________________________________________
conv2d_148 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_147[0][0]
__________________________________________________________________________________________________
up_sampling2d_31 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_148[0][0]
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_31[0][0]
                                                                 conv2d_134[0][0]
__________________________________________________________________________________________________
conv2d_149 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_31[0][0]
__________________________________________________________________________________________________
conv2d_150 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_149[0][0]
__________________________________________________________________________________________________
conv2d_151 (Conv2D)             (None, 256, 256, 1)  33          conv2d_150[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6920 - acc: 0.1718 - precision_7: 0.1580 - auc_7: 0.9011 - recall_7: 0.99982021-09-25 10:45:21.960124: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:45:21.960183: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 10:45:22.222690: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 10:45:22.231106: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-104517/train/plugins/profile/2021_09_25_10_45_22
2021-09-25 10:45:22.234600: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-104517/train/plugins/profile/2021_09_25_10_45_22/xeon-e-v100.trace.json.gz
2021-09-25 10:45:22.252512: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-104517/train/plugins/profile/2021_09_25_10_45_22
2021-09-25 10:45:22.260267: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-104517/train/plugins/profile/2021_09_25_10_45_22/xeon-e-v100.memory_profile.json.gz
2021-09-25 10:45:22.275891: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-104517/train/plugins/profile/2021_09_25_10_45_22Dumped tool data for xplane.pb to logs/fit/20210925-104517/train/plugins/profile/2021_09_25_10_45_22/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-104517/train/plugins/profile/2021_09_25_10_45_22/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-104517/train/plugins/profile/2021_09_25_10_45_22/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-104517/train/plugins/profile/2021_09_25_10_45_22/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-104517/train/plugins/profile/2021_09_25_10_45_22/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 4s - loss: 0.6898 - acc: 0.4503 - precision_7: 0.2433 - auc_7: 0.8610 - recall_7: 0.9417WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0646s vs `on_train_batch_end` time: 0.2525s). Check your callbacks.
33/33 [==============================] - 4s 112ms/step - loss: 0.6509 - acc: 0.8618 - precision_7: 0.5849 - auc_7: 0.7577 - recall_7: 0.5333 - val_loss: 0.2593 - val_acc: 0.9018 - val_precision_7: 0.6933 - val_auc_7: 0.9183 - val_recall_7: 0.6749
Epoch 2/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2416 - acc: 0.9061 - precision_7: 0.7569 - auc_7: 0.9189 - recall_7: 0.6271 - val_loss: 0.2095 - val_acc: 0.9159 - val_precision_7: 0.7724 - val_auc_7: 0.9363 - val_recall_7: 0.6306
Epoch 3/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2316 - acc: 0.9086 - precision_7: 0.7717 - auc_7: 0.9292 - recall_7: 0.6261 - val_loss: 0.2307 - val_acc: 0.9123 - val_precision_7: 0.7344 - val_auc_7: 0.9388 - val_recall_7: 0.6774
Epoch 4/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2173 - acc: 0.9125 - precision_7: 0.7928 - auc_7: 0.9372 - recall_7: 0.6292 - val_loss: 0.2550 - val_acc: 0.8955 - val_precision_7: 0.6796 - val_auc_7: 0.9281 - val_recall_7: 0.7209
Epoch 5/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2200 - acc: 0.9124 - precision_7: 0.7815 - auc_7: 0.9363 - recall_7: 0.6444 - val_loss: 0.2017 - val_acc: 0.9158 - val_precision_7: 0.8702 - val_auc_7: 0.9500 - val_recall_7: 0.5348
Epoch 6/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2216 - acc: 0.9111 - precision_7: 0.7629 - auc_7: 0.9353 - recall_7: 0.6619 - val_loss: 0.2123 - val_acc: 0.9184 - val_precision_7: 0.7848 - val_auc_7: 0.9356 - val_recall_7: 0.6655
Epoch 7/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2073 - acc: 0.9174 - precision_7: 0.7926 - auc_7: 0.9435 - recall_7: 0.6698 - val_loss: 0.1841 - val_acc: 0.9262 - val_precision_7: 0.7830 - val_auc_7: 0.9525 - val_recall_7: 0.7035
Epoch 8/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2011 - acc: 0.9214 - precision_7: 0.8097 - auc_7: 0.9460 - recall_7: 0.6786 - val_loss: 0.2064 - val_acc: 0.9193 - val_precision_7: 0.8273 - val_auc_7: 0.9415 - val_recall_7: 0.6375
Epoch 9/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1987 - acc: 0.9228 - precision_7: 0.8120 - auc_7: 0.9473 - recall_7: 0.6864 - val_loss: 0.2117 - val_acc: 0.9167 - val_precision_7: 0.7144 - val_auc_7: 0.9502 - val_recall_7: 0.7847
Epoch 10/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1912 - acc: 0.9238 - precision_7: 0.7884 - auc_7: 0.9533 - recall_7: 0.7297 - val_loss: 0.1750 - val_acc: 0.9295 - val_precision_7: 0.8303 - val_auc_7: 0.9589 - val_recall_7: 0.6870
Epoch 11/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1718 - acc: 0.9315 - precision_7: 0.8186 - auc_7: 0.9617 - recall_7: 0.7465 - val_loss: 0.1660 - val_acc: 0.9365 - val_precision_7: 0.7644 - val_auc_7: 0.9752 - val_recall_7: 0.8703
Epoch 12/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1646 - acc: 0.9362 - precision_7: 0.8116 - auc_7: 0.9635 - recall_7: 0.7940 - val_loss: 0.1304 - val_acc: 0.9524 - val_precision_7: 0.9029 - val_auc_7: 0.9736 - val_recall_7: 0.7513
Epoch 13/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1488 - acc: 0.9429 - precision_7: 0.8418 - auc_7: 0.9707 - recall_7: 0.8017 - val_loss: 0.1608 - val_acc: 0.9375 - val_precision_7: 0.8258 - val_auc_7: 0.9663 - val_recall_7: 0.7465
Epoch 14/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1428 - acc: 0.9453 - precision_7: 0.8412 - auc_7: 0.9729 - recall_7: 0.8203 - val_loss: 0.1340 - val_acc: 0.9471 - val_precision_7: 0.9022 - val_auc_7: 0.9785 - val_recall_7: 0.7502
Epoch 15/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1331 - acc: 0.9487 - precision_7: 0.8625 - auc_7: 0.9767 - recall_7: 0.8166 - val_loss: 0.1165 - val_acc: 0.9553 - val_precision_7: 0.8720 - val_auc_7: 0.9823 - val_recall_7: 0.8516
Epoch 16/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1053 - acc: 0.9587 - precision_7: 0.8817 - auc_7: 0.9859 - recall_7: 0.8629 - val_loss: 0.0932 - val_acc: 0.9640 - val_precision_7: 0.8789 - val_auc_7: 0.9893 - val_recall_7: 0.8908
Epoch 17/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0997 - acc: 0.9609 - precision_7: 0.8905 - auc_7: 0.9877 - recall_7: 0.8674 - val_loss: 0.0983 - val_acc: 0.9609 - val_precision_7: 0.8515 - val_auc_7: 0.9887 - val_recall_7: 0.9100
Epoch 18/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1125 - acc: 0.9551 - precision_7: 0.8641 - auc_7: 0.9843 - recall_7: 0.8608 - val_loss: 0.1029 - val_acc: 0.9604 - val_precision_7: 0.8478 - val_auc_7: 0.9886 - val_recall_7: 0.9229
Epoch 19/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0940 - acc: 0.9632 - precision_7: 0.8929 - auc_7: 0.9891 - recall_7: 0.8806 - val_loss: 0.0870 - val_acc: 0.9671 - val_precision_7: 0.8958 - val_auc_7: 0.9885 - val_recall_7: 0.8719
Epoch 20/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0901 - acc: 0.9654 - precision_7: 0.9032 - auc_7: 0.9898 - recall_7: 0.8828 - val_loss: 0.0785 - val_acc: 0.9697 - val_precision_7: 0.9169 - val_auc_7: 0.9912 - val_recall_7: 0.8847
Epoch 21/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0882 - acc: 0.9662 - precision_7: 0.9014 - auc_7: 0.9900 - recall_7: 0.8909 - val_loss: 0.0697 - val_acc: 0.9731 - val_precision_7: 0.9061 - val_auc_7: 0.9936 - val_recall_7: 0.9098
Epoch 22/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0770 - acc: 0.9702 - precision_7: 0.9149 - auc_7: 0.9926 - recall_7: 0.9016 - val_loss: 0.0797 - val_acc: 0.9700 - val_precision_7: 0.9295 - val_auc_7: 0.9908 - val_recall_7: 0.8649
Epoch 23/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0727 - acc: 0.9715 - precision_7: 0.9149 - auc_7: 0.9935 - recall_7: 0.9106 - val_loss: 0.0836 - val_acc: 0.9686 - val_precision_7: 0.9045 - val_auc_7: 0.9898 - val_recall_7: 0.9038
Epoch 24/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0703 - acc: 0.9726 - precision_7: 0.9203 - auc_7: 0.9938 - recall_7: 0.9115 - val_loss: 0.0869 - val_acc: 0.9653 - val_precision_7: 0.8435 - val_auc_7: 0.9920 - val_recall_7: 0.9359
Epoch 25/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0742 - acc: 0.9709 - precision_7: 0.9128 - auc_7: 0.9931 - recall_7: 0.9086 - val_loss: 0.0751 - val_acc: 0.9707 - val_precision_7: 0.8976 - val_auc_7: 0.9923 - val_recall_7: 0.9189
Epoch 26/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0637 - acc: 0.9748 - precision_7: 0.9248 - auc_7: 0.9949 - recall_7: 0.9208 - val_loss: 0.0760 - val_acc: 0.9715 - val_precision_7: 0.9028 - val_auc_7: 0.9913 - val_recall_7: 0.9159
Epoch 27/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0600 - acc: 0.9761 - precision_7: 0.9306 - auc_7: 0.9954 - recall_7: 0.9228 - val_loss: 0.0632 - val_acc: 0.9754 - val_precision_7: 0.9133 - val_auc_7: 0.9943 - val_recall_7: 0.9273
Epoch 28/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0570 - acc: 0.9770 - precision_7: 0.9351 - auc_7: 0.9959 - recall_7: 0.9236 - val_loss: 0.0748 - val_acc: 0.9725 - val_precision_7: 0.9084 - val_auc_7: 0.9918 - val_recall_7: 0.9036
Epoch 29/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0565 - acc: 0.9772 - precision_7: 0.9348 - auc_7: 0.9959 - recall_7: 0.9251 - val_loss: 0.0842 - val_acc: 0.9670 - val_precision_7: 0.8972 - val_auc_7: 0.9905 - val_recall_7: 0.9087
Epoch 30/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1125 - acc: 0.9597 - precision_7: 0.8891 - auc_7: 0.9838 - recall_7: 0.8611 - val_loss: 0.1478 - val_acc: 0.9401 - val_precision_7: 0.7453 - val_auc_7: 0.9841 - val_recall_7: 0.9350
Epoch 31/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1024 - acc: 0.9597 - precision_7: 0.8781 - auc_7: 0.9870 - recall_7: 0.8752 - val_loss: 0.0857 - val_acc: 0.9670 - val_precision_7: 0.9262 - val_auc_7: 0.9901 - val_recall_7: 0.8672
Epoch 32/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0818 - acc: 0.9682 - precision_7: 0.9084 - auc_7: 0.9916 - recall_7: 0.8958 - val_loss: 0.0881 - val_acc: 0.9676 - val_precision_7: 0.9008 - val_auc_7: 0.9876 - val_recall_7: 0.8847
Epoch 33/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0801 - acc: 0.9689 - precision_7: 0.9101 - auc_7: 0.9919 - recall_7: 0.8988 - val_loss: 0.0759 - val_acc: 0.9714 - val_precision_7: 0.9210 - val_auc_7: 0.9921 - val_recall_7: 0.8952
Epoch 34/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0726 - acc: 0.9720 - precision_7: 0.9198 - auc_7: 0.9934 - recall_7: 0.9079 - val_loss: 0.0745 - val_acc: 0.9716 - val_precision_7: 0.9098 - val_auc_7: 0.9921 - val_recall_7: 0.9098
Epoch 35/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0683 - acc: 0.9732 - precision_7: 0.9229 - auc_7: 0.9940 - recall_7: 0.9123 - val_loss: 0.0757 - val_acc: 0.9701 - val_precision_7: 0.9024 - val_auc_7: 0.9930 - val_recall_7: 0.9315
Epoch 36/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0640 - acc: 0.9746 - precision_7: 0.9282 - auc_7: 0.9948 - recall_7: 0.9153 - val_loss: 0.0702 - val_acc: 0.9741 - val_precision_7: 0.9154 - val_auc_7: 0.9920 - val_recall_7: 0.9212
Epoch 37/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0572 - acc: 0.9771 - precision_7: 0.9343 - auc_7: 0.9958 - recall_7: 0.9248 - val_loss: 0.0714 - val_acc: 0.9738 - val_precision_7: 0.8987 - val_auc_7: 0.9922 - val_recall_7: 0.9224
Epoch 38/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0538 - acc: 0.9783 - precision_7: 0.9396 - auc_7: 0.9962 - recall_7: 0.9266 - val_loss: 0.0691 - val_acc: 0.9727 - val_precision_7: 0.9191 - val_auc_7: 0.9933 - val_recall_7: 0.9124
Epoch 39/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0514 - acc: 0.9791 - precision_7: 0.9419 - auc_7: 0.9966 - recall_7: 0.9293 - val_loss: 0.0781 - val_acc: 0.9731 - val_precision_7: 0.9156 - val_auc_7: 0.9912 - val_recall_7: 0.9172
Epoch 40/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0498 - acc: 0.9795 - precision_7: 0.9422 - auc_7: 0.9968 - recall_7: 0.9318 - val_loss: 0.0746 - val_acc: 0.9738 - val_precision_7: 0.9049 - val_auc_7: 0.9927 - val_recall_7: 0.9293
Epoch 41/100
32/33 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9807 - precision_7: 0.9484 - auc_7: 0.9972 - recall_7: 0.9332Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 85ms/step - loss: 0.0466 - acc: 0.9807 - precision_7: 0.9483 - auc_7: 0.9972 - recall_7: 0.9329 - val_loss: 0.0741 - val_acc: 0.9743 - val_precision_7: 0.9391 - val_auc_7: 0.9913 - val_recall_7: 0.8948
Epoch 00041: early stopping
6/6 [==============================] - 0s 25ms/step - loss: 0.0717 - acc: 0.9730 - precision_7: 0.9099 - auc_7: 0.9934 - recall_7: 0.9255
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb61233b670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8414190059219986
test_sensitivity 0.9307665313079662
test_specifitivity 0.9810293265198398
test_accuracy 0.9728702121310764
test_precision 0.9048327847147624
test_jaccard_score 0.8414190059219986
test_dicecoef 0.9176164592036261
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-104732.h5
[0.96797316 0.8161394  0.88361488 0.92447068 0.97640334] [0.97287021 0.84141901 0.90483278 0.93076653 0.98102933]

-------------------------
Rep: 2
-------------------------

2021-09-25 10:47:33.148038: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:47:33.148096: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied histogram equalization on all input images
Applied normalization
Data augumentation off
Prep done
Training samples: 258, channel mean: 0.1526593703632206,
Validation samples: 63, channel mean: 0.14989169631611324
Model built.
Model: "functional_17"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_9 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_152 (Conv2D)             (None, 256, 256, 32) 320         input_9[0][0]
__________________________________________________________________________________________________
conv2d_153 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_152[0][0]
__________________________________________________________________________________________________
max_pooling2d_32 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_153[0][0]
__________________________________________________________________________________________________
conv2d_154 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_32[0][0]
__________________________________________________________________________________________________
conv2d_155 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_154[0][0]
__________________________________________________________________________________________________
max_pooling2d_33 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_155[0][0]
__________________________________________________________________________________________________
conv2d_156 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_33[0][0]
__________________________________________________________________________________________________
conv2d_157 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_156[0][0]
__________________________________________________________________________________________________
max_pooling2d_34 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_157[0][0]
__________________________________________________________________________________________________
conv2d_158 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_34[0][0]
__________________________________________________________________________________________________
conv2d_159 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_158[0][0]
__________________________________________________________________________________________________
max_pooling2d_35 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_159[0][0]
__________________________________________________________________________________________________
conv2d_160 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_35[0][0]
__________________________________________________________________________________________________
conv2d_161 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_160[0][0]
__________________________________________________________________________________________________
up_sampling2d_32 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_161[0][0]
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_32[0][0]
                                                                 conv2d_159[0][0]
__________________________________________________________________________________________________
conv2d_162 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_32[0][0]
__________________________________________________________________________________________________
conv2d_163 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_162[0][0]
__________________________________________________________________________________________________
up_sampling2d_33 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_163[0][0]
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_33[0][0]
                                                                 conv2d_157[0][0]
__________________________________________________________________________________________________
conv2d_164 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_33[0][0]
__________________________________________________________________________________________________
conv2d_165 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_164[0][0]
__________________________________________________________________________________________________
up_sampling2d_34 (UpSampling2D) (None, 128, 128, 128 0           conv2d_165[0][0]
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_34[0][0]
                                                                 conv2d_155[0][0]
__________________________________________________________________________________________________
conv2d_166 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_34[0][0]
__________________________________________________________________________________________________
conv2d_167 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_166[0][0]
__________________________________________________________________________________________________
up_sampling2d_35 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_167[0][0]
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_35[0][0]
                                                                 conv2d_153[0][0]
__________________________________________________________________________________________________
conv2d_168 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_35[0][0]
__________________________________________________________________________________________________
conv2d_169 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_168[0][0]
__________________________________________________________________________________________________
conv2d_170 (Conv2D)             (None, 256, 256, 1)  33          conv2d_169[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6935 - acc: 0.5946 - precision_8: 0.1021 - auc_8: 0.3192 - recall_8: 0.20632021-09-25 10:47:38.102711: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:47:38.102774: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 10:47:38.373421: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 10:47:38.383461: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-104733/train/plugins/profile/2021_09_25_10_47_38
2021-09-25 10:47:38.391248: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-104733/train/plugins/profile/2021_09_25_10_47_38/xeon-e-v100.trace.json.gz
2021-09-25 10:47:38.411723: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-104733/train/plugins/profile/2021_09_25_10_47_38
2021-09-25 10:47:38.419167: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-104733/train/plugins/profile/2021_09_25_10_47_38/xeon-e-v100.memory_profile.json.gz
2021-09-25 10:47:38.441041: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-104733/train/plugins/profile/2021_09_25_10_47_38Dumped tool data for xplane.pb to logs/fit/20210925-104733/train/plugins/profile/2021_09_25_10_47_38/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-104733/train/plugins/profile/2021_09_25_10_47_38/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-104733/train/plugins/profile/2021_09_25_10_47_38/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-104733/train/plugins/profile/2021_09_25_10_47_38/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-104733/train/plugins/profile/2021_09_25_10_47_38/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6903 - acc: 0.6769 - precision_8: 0.3009 - auc_8: 0.6675 - recall_8: 0.5715WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0647s vs `on_train_batch_end` time: 0.2753s). Check your callbacks.
33/33 [==============================] - 4s 106ms/step - loss: 0.8099 - acc: 0.8542 - precision_8: 0.5507 - auc_8: 0.7597 - recall_8: 0.5868 - val_loss: 0.2716 - val_acc: 0.8988 - val_precision_8: 0.6798 - val_auc_8: 0.9098 - val_recall_8: 0.6745
Epoch 2/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2282 - acc: 0.9080 - precision_8: 0.7626 - auc_8: 0.9298 - recall_8: 0.6349 - val_loss: 0.2189 - val_acc: 0.9155 - val_precision_8: 0.8771 - val_auc_8: 0.9393 - val_recall_8: 0.5139
Epoch 3/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2196 - acc: 0.9105 - precision_8: 0.7588 - auc_8: 0.9373 - recall_8: 0.6635 - val_loss: 0.2258 - val_acc: 0.9140 - val_precision_8: 0.7380 - val_auc_8: 0.9390 - val_recall_8: 0.6874
Epoch 4/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2161 - acc: 0.9156 - precision_8: 0.7970 - auc_8: 0.9374 - recall_8: 0.6488 - val_loss: 0.2233 - val_acc: 0.9091 - val_precision_8: 0.7644 - val_auc_8: 0.9377 - val_recall_8: 0.6676
Epoch 5/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1986 - acc: 0.9207 - precision_8: 0.7933 - auc_8: 0.9485 - recall_8: 0.6964 - val_loss: 0.1732 - val_acc: 0.9281 - val_precision_8: 0.8236 - val_auc_8: 0.9610 - val_recall_8: 0.6805
Epoch 6/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2008 - acc: 0.9193 - precision_8: 0.7950 - auc_8: 0.9471 - recall_8: 0.6825 - val_loss: 0.1955 - val_acc: 0.9246 - val_precision_8: 0.7861 - val_auc_8: 0.9461 - val_recall_8: 0.7176
Epoch 7/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1921 - acc: 0.9219 - precision_8: 0.7944 - auc_8: 0.9528 - recall_8: 0.7045 - val_loss: 0.1701 - val_acc: 0.9327 - val_precision_8: 0.8051 - val_auc_8: 0.9604 - val_recall_8: 0.7278
Epoch 8/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1896 - acc: 0.9242 - precision_8: 0.7982 - auc_8: 0.9536 - recall_8: 0.7176 - val_loss: 0.2017 - val_acc: 0.9208 - val_precision_8: 0.8045 - val_auc_8: 0.9473 - val_recall_8: 0.6783
Epoch 9/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1868 - acc: 0.9247 - precision_8: 0.8009 - auc_8: 0.9548 - recall_8: 0.7177 - val_loss: 0.1987 - val_acc: 0.9225 - val_precision_8: 0.7289 - val_auc_8: 0.9603 - val_recall_8: 0.8081
Epoch 10/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1811 - acc: 0.9260 - precision_8: 0.7895 - auc_8: 0.9585 - recall_8: 0.7464 - val_loss: 0.1655 - val_acc: 0.9310 - val_precision_8: 0.8319 - val_auc_8: 0.9662 - val_recall_8: 0.6973
Epoch 11/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1784 - acc: 0.9280 - precision_8: 0.8069 - auc_8: 0.9582 - recall_8: 0.7361 - val_loss: 0.1738 - val_acc: 0.9236 - val_precision_8: 0.7483 - val_auc_8: 0.9668 - val_recall_8: 0.7854
Epoch 12/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1746 - acc: 0.9281 - precision_8: 0.7924 - auc_8: 0.9612 - recall_8: 0.7593 - val_loss: 0.1614 - val_acc: 0.9364 - val_precision_8: 0.8805 - val_auc_8: 0.9668 - val_recall_8: 0.6479
Epoch 13/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1602 - acc: 0.9328 - precision_8: 0.8120 - auc_8: 0.9690 - recall_8: 0.7664 - val_loss: 0.1345 - val_acc: 0.9412 - val_precision_8: 0.8103 - val_auc_8: 0.9768 - val_recall_8: 0.8007
Epoch 14/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1527 - acc: 0.9389 - precision_8: 0.8265 - auc_8: 0.9717 - recall_8: 0.7926 - val_loss: 0.1627 - val_acc: 0.9356 - val_precision_8: 0.8643 - val_auc_8: 0.9686 - val_recall_8: 0.7080
Epoch 15/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1622 - acc: 0.9341 - precision_8: 0.8079 - auc_8: 0.9669 - recall_8: 0.7827 - val_loss: 0.1380 - val_acc: 0.9467 - val_precision_8: 0.8428 - val_auc_8: 0.9756 - val_recall_8: 0.8288
Epoch 16/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1314 - acc: 0.9489 - precision_8: 0.8479 - auc_8: 0.9778 - recall_8: 0.8374 - val_loss: 0.1034 - val_acc: 0.9597 - val_precision_8: 0.8587 - val_auc_8: 0.9863 - val_recall_8: 0.8865
Epoch 17/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1173 - acc: 0.9537 - precision_8: 0.8700 - auc_8: 0.9828 - recall_8: 0.8424 - val_loss: 0.1117 - val_acc: 0.9567 - val_precision_8: 0.8888 - val_auc_8: 0.9837 - val_recall_8: 0.8287
Epoch 18/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1143 - acc: 0.9548 - precision_8: 0.8660 - auc_8: 0.9837 - recall_8: 0.8563 - val_loss: 0.1001 - val_acc: 0.9617 - val_precision_8: 0.8641 - val_auc_8: 0.9890 - val_recall_8: 0.9078
Epoch 19/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1006 - acc: 0.9608 - precision_8: 0.8870 - auc_8: 0.9876 - recall_8: 0.8711 - val_loss: 0.0891 - val_acc: 0.9662 - val_precision_8: 0.8936 - val_auc_8: 0.9873 - val_recall_8: 0.8675
Epoch 20/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0964 - acc: 0.9615 - precision_8: 0.8867 - auc_8: 0.9886 - recall_8: 0.8768 - val_loss: 0.0906 - val_acc: 0.9660 - val_precision_8: 0.9123 - val_auc_8: 0.9883 - val_recall_8: 0.8636
Epoch 21/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1140 - acc: 0.9557 - precision_8: 0.8666 - auc_8: 0.9836 - recall_8: 0.8616 - val_loss: 0.1167 - val_acc: 0.9539 - val_precision_8: 0.9267 - val_auc_8: 0.9840 - val_recall_8: 0.7423
Epoch 22/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1078 - acc: 0.9583 - precision_8: 0.8865 - auc_8: 0.9853 - recall_8: 0.8545 - val_loss: 0.0945 - val_acc: 0.9634 - val_precision_8: 0.8654 - val_auc_8: 0.9878 - val_recall_8: 0.8944
Epoch 23/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0958 - acc: 0.9628 - precision_8: 0.8902 - auc_8: 0.9886 - recall_8: 0.8814 - val_loss: 0.0911 - val_acc: 0.9651 - val_precision_8: 0.9014 - val_auc_8: 0.9882 - val_recall_8: 0.8838
Epoch 24/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0880 - acc: 0.9657 - precision_8: 0.8985 - auc_8: 0.9904 - recall_8: 0.8911 - val_loss: 0.0759 - val_acc: 0.9695 - val_precision_8: 0.8822 - val_auc_8: 0.9925 - val_recall_8: 0.9131
Epoch 25/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0866 - acc: 0.9658 - precision_8: 0.9015 - auc_8: 0.9908 - recall_8: 0.8880 - val_loss: 0.0809 - val_acc: 0.9686 - val_precision_8: 0.9019 - val_auc_8: 0.9904 - val_recall_8: 0.8981
Epoch 26/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0819 - acc: 0.9678 - precision_8: 0.9030 - auc_8: 0.9916 - recall_8: 0.8995 - val_loss: 0.0844 - val_acc: 0.9660 - val_precision_8: 0.8697 - val_auc_8: 0.9908 - val_recall_8: 0.9194
Epoch 27/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0938 - acc: 0.9631 - precision_8: 0.8888 - auc_8: 0.9885 - recall_8: 0.8850 - val_loss: 0.0747 - val_acc: 0.9715 - val_precision_8: 0.8960 - val_auc_8: 0.9931 - val_recall_8: 0.9201
Epoch 28/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0919 - acc: 0.9639 - precision_8: 0.8952 - auc_8: 0.9895 - recall_8: 0.8829 - val_loss: 0.0783 - val_acc: 0.9696 - val_precision_8: 0.8965 - val_auc_8: 0.9903 - val_recall_8: 0.8959
Epoch 29/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0826 - acc: 0.9684 - precision_8: 0.9079 - auc_8: 0.9912 - recall_8: 0.8974 - val_loss: 0.0828 - val_acc: 0.9672 - val_precision_8: 0.8975 - val_auc_8: 0.9910 - val_recall_8: 0.9095
Epoch 30/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0796 - acc: 0.9690 - precision_8: 0.9114 - auc_8: 0.9919 - recall_8: 0.8973 - val_loss: 0.0749 - val_acc: 0.9707 - val_precision_8: 0.8922 - val_auc_8: 0.9925 - val_recall_8: 0.9232
Epoch 31/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0772 - acc: 0.9698 - precision_8: 0.9044 - auc_8: 0.9924 - recall_8: 0.9117 - val_loss: 0.0801 - val_acc: 0.9683 - val_precision_8: 0.9286 - val_auc_8: 0.9914 - val_recall_8: 0.8735
Epoch 32/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0801 - acc: 0.9684 - precision_8: 0.9066 - auc_8: 0.9918 - recall_8: 0.8993 - val_loss: 0.0824 - val_acc: 0.9686 - val_precision_8: 0.9288 - val_auc_8: 0.9898 - val_recall_8: 0.8592
Epoch 33/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0762 - acc: 0.9704 - precision_8: 0.9126 - auc_8: 0.9924 - recall_8: 0.9056 - val_loss: 0.0747 - val_acc: 0.9710 - val_precision_8: 0.9019 - val_auc_8: 0.9931 - val_recall_8: 0.9152
Epoch 34/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0753 - acc: 0.9706 - precision_8: 0.9141 - auc_8: 0.9926 - recall_8: 0.9053 - val_loss: 0.0728 - val_acc: 0.9715 - val_precision_8: 0.9103 - val_auc_8: 0.9923 - val_recall_8: 0.9084
Epoch 35/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0729 - acc: 0.9711 - precision_8: 0.9133 - auc_8: 0.9930 - recall_8: 0.9097 - val_loss: 0.0837 - val_acc: 0.9670 - val_precision_8: 0.8791 - val_auc_8: 0.9926 - val_recall_8: 0.9423
Epoch 36/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0699 - acc: 0.9725 - precision_8: 0.9199 - auc_8: 0.9937 - recall_8: 0.9112 - val_loss: 0.0717 - val_acc: 0.9723 - val_precision_8: 0.9197 - val_auc_8: 0.9922 - val_recall_8: 0.9041
Epoch 37/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0678 - acc: 0.9731 - precision_8: 0.9192 - auc_8: 0.9940 - recall_8: 0.9162 - val_loss: 0.0744 - val_acc: 0.9711 - val_precision_8: 0.8780 - val_auc_8: 0.9932 - val_recall_8: 0.9290
Epoch 38/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0691 - acc: 0.9726 - precision_8: 0.9180 - auc_8: 0.9938 - recall_8: 0.9140 - val_loss: 0.0776 - val_acc: 0.9697 - val_precision_8: 0.9253 - val_auc_8: 0.9914 - val_recall_8: 0.8856
Epoch 39/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0664 - acc: 0.9734 - precision_8: 0.9206 - auc_8: 0.9943 - recall_8: 0.9166 - val_loss: 0.0741 - val_acc: 0.9716 - val_precision_8: 0.9296 - val_auc_8: 0.9917 - val_recall_8: 0.8907
Epoch 40/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0630 - acc: 0.9748 - precision_8: 0.9236 - auc_8: 0.9949 - recall_8: 0.9218 - val_loss: 0.0700 - val_acc: 0.9724 - val_precision_8: 0.9073 - val_auc_8: 0.9927 - val_recall_8: 0.9163
Epoch 41/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0610 - acc: 0.9755 - precision_8: 0.9284 - auc_8: 0.9954 - recall_8: 0.9210 - val_loss: 0.0711 - val_acc: 0.9730 - val_precision_8: 0.9183 - val_auc_8: 0.9919 - val_recall_8: 0.9094
Epoch 42/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0588 - acc: 0.9764 - precision_8: 0.9308 - auc_8: 0.9955 - recall_8: 0.9242 - val_loss: 0.0684 - val_acc: 0.9741 - val_precision_8: 0.9187 - val_auc_8: 0.9928 - val_recall_8: 0.9143
Epoch 43/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0619 - acc: 0.9752 - precision_8: 0.9266 - auc_8: 0.9947 - recall_8: 0.9213 - val_loss: 0.0718 - val_acc: 0.9721 - val_precision_8: 0.9157 - val_auc_8: 0.9929 - val_recall_8: 0.9114
Epoch 44/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0586 - acc: 0.9766 - precision_8: 0.9328 - auc_8: 0.9954 - recall_8: 0.9232 - val_loss: 0.0671 - val_acc: 0.9740 - val_precision_8: 0.9056 - val_auc_8: 0.9940 - val_recall_8: 0.9239
Epoch 45/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0580 - acc: 0.9762 - precision_8: 0.9299 - auc_8: 0.9958 - recall_8: 0.9240 - val_loss: 0.0690 - val_acc: 0.9745 - val_precision_8: 0.9271 - val_auc_8: 0.9917 - val_recall_8: 0.9052
Epoch 46/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0534 - acc: 0.9785 - precision_8: 0.9372 - auc_8: 0.9963 - recall_8: 0.9306 - val_loss: 0.0736 - val_acc: 0.9710 - val_precision_8: 0.8818 - val_auc_8: 0.9932 - val_recall_8: 0.9290
Epoch 47/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0602 - acc: 0.9758 - precision_8: 0.9274 - auc_8: 0.9953 - recall_8: 0.9242 - val_loss: 0.0730 - val_acc: 0.9722 - val_precision_8: 0.9144 - val_auc_8: 0.9922 - val_recall_8: 0.9156
Epoch 48/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0567 - acc: 0.9773 - precision_8: 0.9328 - auc_8: 0.9959 - recall_8: 0.9279 - val_loss: 0.0706 - val_acc: 0.9731 - val_precision_8: 0.9137 - val_auc_8: 0.9922 - val_recall_8: 0.9138
Epoch 49/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0534 - acc: 0.9782 - precision_8: 0.9365 - auc_8: 0.9963 - recall_8: 0.9298 - val_loss: 0.0742 - val_acc: 0.9708 - val_precision_8: 0.8815 - val_auc_8: 0.9931 - val_recall_8: 0.9411
Epoch 50/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0540 - acc: 0.9779 - precision_8: 0.9337 - auc_8: 0.9963 - recall_8: 0.9306 - val_loss: 0.0791 - val_acc: 0.9733 - val_precision_8: 0.9329 - val_auc_8: 0.9910 - val_recall_8: 0.8932
Epoch 51/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0559 - acc: 0.9773 - precision_8: 0.9349 - auc_8: 0.9956 - recall_8: 0.9257 - val_loss: 0.0791 - val_acc: 0.9693 - val_precision_8: 0.9347 - val_auc_8: 0.9910 - val_recall_8: 0.8691
Epoch 52/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0514 - acc: 0.9790 - precision_8: 0.9402 - auc_8: 0.9966 - recall_8: 0.9306 - val_loss: 0.0705 - val_acc: 0.9725 - val_precision_8: 0.9095 - val_auc_8: 0.9940 - val_recall_8: 0.9301
Epoch 53/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0478 - acc: 0.9804 - precision_8: 0.9446 - auc_8: 0.9970 - recall_8: 0.9349 - val_loss: 0.0840 - val_acc: 0.9722 - val_precision_8: 0.8873 - val_auc_8: 0.9885 - val_recall_8: 0.9157
Epoch 54/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0515 - acc: 0.9791 - precision_8: 0.9400 - auc_8: 0.9964 - recall_8: 0.9315 - val_loss: 0.0726 - val_acc: 0.9733 - val_precision_8: 0.9149 - val_auc_8: 0.9927 - val_recall_8: 0.9279
Epoch 55/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0486 - acc: 0.9801 - precision_8: 0.9430 - auc_8: 0.9969 - recall_8: 0.9348 - val_loss: 0.0741 - val_acc: 0.9725 - val_precision_8: 0.9062 - val_auc_8: 0.9913 - val_recall_8: 0.9065
Epoch 56/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0461 - acc: 0.9811 - precision_8: 0.9457 - auc_8: 0.9972 - recall_8: 0.9380 - val_loss: 0.0754 - val_acc: 0.9735 - val_precision_8: 0.9211 - val_auc_8: 0.9910 - val_recall_8: 0.9134
Epoch 57/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0521 - acc: 0.9788 - precision_8: 0.9384 - auc_8: 0.9964 - recall_8: 0.9314 - val_loss: 0.0727 - val_acc: 0.9737 - val_precision_8: 0.9158 - val_auc_8: 0.9921 - val_recall_8: 0.9175
Epoch 58/100
32/33 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9807 - precision_8: 0.9430 - auc_8: 0.9971 - recall_8: 0.9385Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 86ms/step - loss: 0.0470 - acc: 0.9807 - precision_8: 0.9429 - auc_8: 0.9971 - recall_8: 0.9385 - val_loss: 0.0774 - val_acc: 0.9720 - val_precision_8: 0.9215 - val_auc_8: 0.9912 - val_recall_8: 0.9103
Epoch 00058: early stopping
6/6 [==============================] - 0s 25ms/step - loss: 0.0857 - acc: 0.9676 - precision_8: 0.8780 - auc_8: 0.9924 - recall_8: 0.9297
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbdc87dfaf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8223530304050617
test_sensitivity 0.9340711218061195
test_specifitivity 0.9735952836634834
test_accuracy 0.9671793619791667
test_precision 0.8726961178463533
test_jaccard_score 0.8223530304050617
test_dicecoef 0.9023411803164891
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-105037.h5
[1.94084337 1.6575584  1.78844767 1.85523721 1.95743267] [0.96717936 0.82235303 0.87269612 0.93407112 0.97359528]

-------------------------
Averaged metrics for Baseline + Histogram Equalization - bacteria: [0.96934091 0.82663714 0.88704793 0.92976944 0.97700932]
-------------------------


-------------------------
RUN: Baseline + Per Channel Normalization - bacteria, PARAMS: {'per_channel_normalization': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-25 10:50:38.328075: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:50:38.328124: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Normalized per channel
Applied normalization
Data augumentation off
Prep done
Training samples: 258, channel mean: -1.389123527662048e-17,
Validation samples: 63, channel mean: -0.0011996397803231908
Model built.
Model: "functional_19"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_10 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_171 (Conv2D)             (None, 256, 256, 32) 320         input_10[0][0]
__________________________________________________________________________________________________
conv2d_172 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_171[0][0]
__________________________________________________________________________________________________
max_pooling2d_36 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_172[0][0]
__________________________________________________________________________________________________
conv2d_173 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_36[0][0]
__________________________________________________________________________________________________
conv2d_174 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_173[0][0]
__________________________________________________________________________________________________
max_pooling2d_37 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_174[0][0]
__________________________________________________________________________________________________
conv2d_175 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_37[0][0]
__________________________________________________________________________________________________
conv2d_176 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_175[0][0]
__________________________________________________________________________________________________
max_pooling2d_38 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_176[0][0]
__________________________________________________________________________________________________
conv2d_177 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_38[0][0]
__________________________________________________________________________________________________
conv2d_178 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_177[0][0]
__________________________________________________________________________________________________
max_pooling2d_39 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_178[0][0]
__________________________________________________________________________________________________
conv2d_179 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_39[0][0]
__________________________________________________________________________________________________
conv2d_180 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_179[0][0]
__________________________________________________________________________________________________
up_sampling2d_36 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_180[0][0]
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_36[0][0]
                                                                 conv2d_178[0][0]
__________________________________________________________________________________________________
conv2d_181 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_36[0][0]
__________________________________________________________________________________________________
conv2d_182 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_181[0][0]
__________________________________________________________________________________________________
up_sampling2d_37 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_182[0][0]
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_37[0][0]
                                                                 conv2d_176[0][0]
__________________________________________________________________________________________________
conv2d_183 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_37[0][0]
__________________________________________________________________________________________________
conv2d_184 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_183[0][0]
__________________________________________________________________________________________________
up_sampling2d_38 (UpSampling2D) (None, 128, 128, 128 0           conv2d_184[0][0]
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_38[0][0]
                                                                 conv2d_174[0][0]
__________________________________________________________________________________________________
conv2d_185 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_38[0][0]
__________________________________________________________________________________________________
conv2d_186 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_185[0][0]
__________________________________________________________________________________________________
up_sampling2d_39 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_186[0][0]
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_39[0][0]
                                                                 conv2d_172[0][0]
__________________________________________________________________________________________________
conv2d_187 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_39[0][0]
__________________________________________________________________________________________________
conv2d_188 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_187[0][0]
__________________________________________________________________________________________________
conv2d_189 (Conv2D)             (None, 256, 256, 1)  33          conv2d_188[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6917 - acc: 0.3232 - precision_9: 0.1718 - auc_9: 0.8284 - recall_9: 0.87782021-09-25 10:50:40.496007: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:50:40.496098: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 10:50:40.775092: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 10:50:40.784090: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-105038/train/plugins/profile/2021_09_25_10_50_40
2021-09-25 10:50:40.787683: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-105038/train/plugins/profile/2021_09_25_10_50_40/xeon-e-v100.trace.json.gz
2021-09-25 10:50:40.806811: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-105038/train/plugins/profile/2021_09_25_10_50_40
2021-09-25 10:50:40.817964: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-105038/train/plugins/profile/2021_09_25_10_50_40/xeon-e-v100.memory_profile.json.gz
2021-09-25 10:50:40.837207: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-105038/train/plugins/profile/2021_09_25_10_50_40Dumped tool data for xplane.pb to logs/fit/20210925-105038/train/plugins/profile/2021_09_25_10_50_40/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-105038/train/plugins/profile/2021_09_25_10_50_40/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-105038/train/plugins/profile/2021_09_25_10_50_40/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-105038/train/plugins/profile/2021_09_25_10_50_40/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-105038/train/plugins/profile/2021_09_25_10_50_40/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6885 - acc: 0.5498 - precision_9: 0.2623 - auc_9: 0.8064 - recall_9: 0.7983WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0650s vs `on_train_batch_end` time: 0.2776s). Check your callbacks.
33/33 [==============================] - 3s 105ms/step - loss: 0.4576 - acc: 0.8711 - precision_9: 0.6254 - auc_9: 0.8012 - recall_9: 0.5282 - val_loss: 0.3077 - val_acc: 0.8991 - val_precision_9: 0.6855 - val_auc_9: 0.8904 - val_recall_9: 0.6635
Epoch 2/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2445 - acc: 0.9045 - precision_9: 0.7476 - auc_9: 0.9137 - recall_9: 0.6279 - val_loss: 0.2626 - val_acc: 0.9083 - val_precision_9: 0.7740 - val_auc_9: 0.8971 - val_recall_9: 0.5569
Epoch 3/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2371 - acc: 0.9084 - precision_9: 0.7765 - auc_9: 0.9194 - recall_9: 0.6170 - val_loss: 0.2557 - val_acc: 0.9138 - val_precision_9: 0.7961 - val_auc_9: 0.9167 - val_recall_9: 0.5941
Epoch 4/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2422 - acc: 0.9125 - precision_9: 0.8031 - auc_9: 0.9094 - recall_9: 0.6158 - val_loss: 0.2636 - val_acc: 0.8972 - val_precision_9: 0.7064 - val_auc_9: 0.9079 - val_recall_9: 0.6697
Epoch 5/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2283 - acc: 0.9142 - precision_9: 0.7868 - auc_9: 0.9216 - recall_9: 0.6515 - val_loss: 0.2018 - val_acc: 0.9221 - val_precision_9: 0.8263 - val_auc_9: 0.9406 - val_recall_9: 0.6278
Epoch 6/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2203 - acc: 0.9181 - precision_9: 0.8100 - auc_9: 0.9248 - recall_9: 0.6523 - val_loss: 0.2134 - val_acc: 0.9224 - val_precision_9: 0.7568 - val_auc_9: 0.9275 - val_recall_9: 0.7498
Epoch 7/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2121 - acc: 0.9203 - precision_9: 0.8069 - auc_9: 0.9328 - recall_9: 0.6737 - val_loss: 0.1767 - val_acc: 0.9323 - val_precision_9: 0.7832 - val_auc_9: 0.9550 - val_recall_9: 0.7594
Epoch 8/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1972 - acc: 0.9267 - precision_9: 0.8198 - auc_9: 0.9425 - recall_9: 0.7069 - val_loss: 0.2198 - val_acc: 0.9245 - val_precision_9: 0.8554 - val_auc_9: 0.9340 - val_recall_9: 0.6450
Epoch 9/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1908 - acc: 0.9305 - precision_9: 0.8246 - auc_9: 0.9457 - recall_9: 0.7305 - val_loss: 0.1631 - val_acc: 0.9436 - val_precision_9: 0.8145 - val_auc_9: 0.9715 - val_recall_9: 0.8307
Epoch 10/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1604 - acc: 0.9410 - precision_9: 0.8385 - auc_9: 0.9611 - recall_9: 0.7914 - val_loss: 0.1441 - val_acc: 0.9471 - val_precision_9: 0.8882 - val_auc_9: 0.9716 - val_recall_9: 0.7553
Epoch 11/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1484 - acc: 0.9450 - precision_9: 0.8442 - auc_9: 0.9673 - recall_9: 0.8137 - val_loss: 0.1213 - val_acc: 0.9533 - val_precision_9: 0.8216 - val_auc_9: 0.9851 - val_recall_9: 0.9039
Epoch 12/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1439 - acc: 0.9498 - precision_9: 0.8582 - auc_9: 0.9690 - recall_9: 0.8305 - val_loss: 0.1614 - val_acc: 0.9421 - val_precision_9: 0.7545 - val_auc_9: 0.9785 - val_recall_9: 0.8887
Epoch 13/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1573 - acc: 0.9400 - precision_9: 0.8309 - auc_9: 0.9661 - recall_9: 0.7951 - val_loss: 0.1436 - val_acc: 0.9444 - val_precision_9: 0.7666 - val_auc_9: 0.9820 - val_recall_9: 0.9122
Epoch 14/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1247 - acc: 0.9521 - precision_9: 0.8646 - auc_9: 0.9789 - recall_9: 0.8383 - val_loss: 0.1022 - val_acc: 0.9610 - val_precision_9: 0.9058 - val_auc_9: 0.9876 - val_recall_9: 0.8437
Epoch 15/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1127 - acc: 0.9569 - precision_9: 0.8825 - auc_9: 0.9829 - recall_9: 0.8497 - val_loss: 0.1040 - val_acc: 0.9602 - val_precision_9: 0.8728 - val_auc_9: 0.9860 - val_recall_9: 0.8856
Epoch 16/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1097 - acc: 0.9578 - precision_9: 0.8757 - auc_9: 0.9843 - recall_9: 0.8647 - val_loss: 0.0937 - val_acc: 0.9653 - val_precision_9: 0.8799 - val_auc_9: 0.9901 - val_recall_9: 0.8989
Epoch 17/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0923 - acc: 0.9639 - precision_9: 0.9016 - auc_9: 0.9893 - recall_9: 0.8744 - val_loss: 0.1063 - val_acc: 0.9585 - val_precision_9: 0.8326 - val_auc_9: 0.9890 - val_recall_9: 0.9217
Epoch 18/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1118 - acc: 0.9569 - precision_9: 0.8703 - auc_9: 0.9832 - recall_9: 0.8652 - val_loss: 0.0971 - val_acc: 0.9628 - val_precision_9: 0.8559 - val_auc_9: 0.9904 - val_recall_9: 0.9282
Epoch 19/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0944 - acc: 0.9637 - precision_9: 0.8961 - auc_9: 0.9887 - recall_9: 0.8798 - val_loss: 0.0837 - val_acc: 0.9678 - val_precision_9: 0.8998 - val_auc_9: 0.9890 - val_recall_9: 0.8720
Epoch 20/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0803 - acc: 0.9684 - precision_9: 0.9048 - auc_9: 0.9919 - recall_9: 0.9014 - val_loss: 0.0710 - val_acc: 0.9722 - val_precision_9: 0.9185 - val_auc_9: 0.9929 - val_recall_9: 0.9005
Epoch 21/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0849 - acc: 0.9669 - precision_9: 0.9012 - auc_9: 0.9910 - recall_9: 0.8954 - val_loss: 0.0669 - val_acc: 0.9736 - val_precision_9: 0.9047 - val_auc_9: 0.9943 - val_recall_9: 0.9152
Epoch 22/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0771 - acc: 0.9699 - precision_9: 0.9130 - auc_9: 0.9924 - recall_9: 0.9020 - val_loss: 0.0778 - val_acc: 0.9701 - val_precision_9: 0.9072 - val_auc_9: 0.9911 - val_recall_9: 0.8908
Epoch 23/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0716 - acc: 0.9719 - precision_9: 0.9148 - auc_9: 0.9936 - recall_9: 0.9133 - val_loss: 0.0816 - val_acc: 0.9689 - val_precision_9: 0.9121 - val_auc_9: 0.9903 - val_recall_9: 0.8966
Epoch 24/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0716 - acc: 0.9720 - precision_9: 0.9173 - auc_9: 0.9934 - recall_9: 0.9107 - val_loss: 0.0814 - val_acc: 0.9673 - val_precision_9: 0.8713 - val_auc_9: 0.9914 - val_recall_9: 0.9103
Epoch 25/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0777 - acc: 0.9696 - precision_9: 0.9089 - auc_9: 0.9922 - recall_9: 0.9046 - val_loss: 0.0717 - val_acc: 0.9713 - val_precision_9: 0.9165 - val_auc_9: 0.9930 - val_recall_9: 0.8993
Epoch 26/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0698 - acc: 0.9719 - precision_9: 0.9146 - auc_9: 0.9939 - recall_9: 0.9137 - val_loss: 0.0825 - val_acc: 0.9681 - val_precision_9: 0.8852 - val_auc_9: 0.9909 - val_recall_9: 0.9135
Epoch 27/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0871 - acc: 0.9667 - precision_9: 0.8982 - auc_9: 0.9899 - recall_9: 0.8983 - val_loss: 0.0702 - val_acc: 0.9735 - val_precision_9: 0.9081 - val_auc_9: 0.9943 - val_recall_9: 0.9199
Epoch 28/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0782 - acc: 0.9705 - precision_9: 0.9175 - auc_9: 0.9920 - recall_9: 0.9004 - val_loss: 0.0773 - val_acc: 0.9718 - val_precision_9: 0.9042 - val_auc_9: 0.9906 - val_recall_9: 0.9037
Epoch 29/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0677 - acc: 0.9735 - precision_9: 0.9224 - auc_9: 0.9941 - recall_9: 0.9151 - val_loss: 0.0811 - val_acc: 0.9672 - val_precision_9: 0.8998 - val_auc_9: 0.9913 - val_recall_9: 0.9073
Epoch 30/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0652 - acc: 0.9741 - precision_9: 0.9258 - auc_9: 0.9947 - recall_9: 0.9152 - val_loss: 0.0684 - val_acc: 0.9730 - val_precision_9: 0.9059 - val_auc_9: 0.9937 - val_recall_9: 0.9228
Epoch 31/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0634 - acc: 0.9751 - precision_9: 0.9260 - auc_9: 0.9947 - recall_9: 0.9213 - val_loss: 0.0807 - val_acc: 0.9693 - val_precision_9: 0.9179 - val_auc_9: 0.9916 - val_recall_9: 0.8922
Epoch 32/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0746 - acc: 0.9719 - precision_9: 0.9215 - auc_9: 0.9918 - recall_9: 0.9054 - val_loss: 0.0856 - val_acc: 0.9672 - val_precision_9: 0.8874 - val_auc_9: 0.9886 - val_recall_9: 0.8983
Epoch 33/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0686 - acc: 0.9735 - precision_9: 0.9217 - auc_9: 0.9932 - recall_9: 0.9158 - val_loss: 0.0710 - val_acc: 0.9723 - val_precision_9: 0.9305 - val_auc_9: 0.9931 - val_recall_9: 0.8905
Epoch 34/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0608 - acc: 0.9756 - precision_9: 0.9290 - auc_9: 0.9953 - recall_9: 0.9214 - val_loss: 0.0741 - val_acc: 0.9719 - val_precision_9: 0.9407 - val_auc_9: 0.9917 - val_recall_9: 0.8770
Epoch 35/100
32/33 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9775 - precision_9: 0.9377 - auc_9: 0.9960 - recall_9: 0.9246Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 85ms/step - loss: 0.0558 - acc: 0.9776 - precision_9: 0.9375 - auc_9: 0.9960 - recall_9: 0.9245 - val_loss: 0.0715 - val_acc: 0.9733 - val_precision_9: 0.9300 - val_auc_9: 0.9928 - val_recall_9: 0.9178
Epoch 00035: early stopping
6/6 [==============================] - 0s 25ms/step - loss: 0.0812 - acc: 0.9673 - precision_9: 0.8925 - auc_9: 0.9921 - recall_9: 0.9081
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb5f8783820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8154667963342322
test_sensitivity 0.9225865209471766
test_specifitivity 0.9754925534085279
test_accuracy 0.9669043646918403
test_precision 0.8794470849901734
test_jaccard_score 0.8154667963342322
test_dicecoef 0.9005004388652098
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-105234.h5
[0. 0. 0. 0. 0.] [0.96690436 0.8154668  0.87944708 0.92258652 0.97549255]

-------------------------
Rep: 1
-------------------------

2021-09-25 10:52:35.124158: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:52:35.124210: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Normalized per channel
Applied normalization
Data augumentation off
Prep done
Training samples: 258, channel mean: -1.389123527662048e-17,
Validation samples: 63, channel mean: -0.0011996397803231908
Model built.
Model: "functional_21"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_11 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_190 (Conv2D)             (None, 256, 256, 32) 320         input_11[0][0]
__________________________________________________________________________________________________
conv2d_191 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_190[0][0]
__________________________________________________________________________________________________
max_pooling2d_40 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_191[0][0]
__________________________________________________________________________________________________
conv2d_192 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_40[0][0]
__________________________________________________________________________________________________
conv2d_193 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_192[0][0]
__________________________________________________________________________________________________
max_pooling2d_41 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_193[0][0]
__________________________________________________________________________________________________
conv2d_194 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_41[0][0]
__________________________________________________________________________________________________
conv2d_195 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_194[0][0]
__________________________________________________________________________________________________
max_pooling2d_42 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_195[0][0]
__________________________________________________________________________________________________
conv2d_196 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_42[0][0]
__________________________________________________________________________________________________
conv2d_197 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_196[0][0]
__________________________________________________________________________________________________
max_pooling2d_43 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_197[0][0]
__________________________________________________________________________________________________
conv2d_198 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_43[0][0]
__________________________________________________________________________________________________
conv2d_199 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_198[0][0]
__________________________________________________________________________________________________
up_sampling2d_40 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_199[0][0]
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_40[0][0]
                                                                 conv2d_197[0][0]
__________________________________________________________________________________________________
conv2d_200 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_40[0][0]
__________________________________________________________________________________________________
conv2d_201 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_200[0][0]
__________________________________________________________________________________________________
up_sampling2d_41 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_201[0][0]
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_41[0][0]
                                                                 conv2d_195[0][0]
__________________________________________________________________________________________________
conv2d_202 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_41[0][0]
__________________________________________________________________________________________________
conv2d_203 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_202[0][0]
__________________________________________________________________________________________________
up_sampling2d_42 (UpSampling2D) (None, 128, 128, 128 0           conv2d_203[0][0]
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_42[0][0]
                                                                 conv2d_193[0][0]
__________________________________________________________________________________________________
conv2d_204 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_42[0][0]
__________________________________________________________________________________________________
conv2d_205 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_204[0][0]
__________________________________________________________________________________________________
up_sampling2d_43 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_205[0][0]
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_43[0][0]
                                                                 conv2d_191[0][0]
__________________________________________________________________________________________________
conv2d_206 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_43[0][0]
__________________________________________________________________________________________________
conv2d_207 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_206[0][0]
__________________________________________________________________________________________________
conv2d_208 (Conv2D)             (None, 256, 256, 1)  33          conv2d_207[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6953 - acc: 0.1144 - precision_10: 0.0706 - auc_10: 0.3797 - recall_10: 0.38602021-09-25 10:52:37.384434: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:52:37.384490: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 10:52:37.658185: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 10:52:37.667276: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-105235/train/plugins/profile/2021_09_25_10_52_37
2021-09-25 10:52:37.670616: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-105235/train/plugins/profile/2021_09_25_10_52_37/xeon-e-v100.trace.json.gz
2021-09-25 10:52:37.690392: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-105235/train/plugins/profile/2021_09_25_10_52_37
2021-09-25 10:52:37.697248: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-105235/train/plugins/profile/2021_09_25_10_52_37/xeon-e-v100.memory_profile.json.gz
2021-09-25 10:52:37.716878: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-105235/train/plugins/profile/2021_09_25_10_52_37Dumped tool data for xplane.pb to logs/fit/20210925-105235/train/plugins/profile/2021_09_25_10_52_37/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-105235/train/plugins/profile/2021_09_25_10_52_37/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-105235/train/plugins/profile/2021_09_25_10_52_37/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-105235/train/plugins/profile/2021_09_25_10_52_37/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-105235/train/plugins/profile/2021_09_25_10_52_37/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6907 - acc: 0.4463 - precision_10: 0.1871 - auc_10: 0.6722 - recall_10: 0.6008WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0327s vs `on_train_batch_end` time: 0.3012s). Check your callbacks.
33/33 [==============================] - 3s 105ms/step - loss: 0.5562 - acc: 0.8585 - precision_10: 0.5767 - auc_10: 0.7339 - recall_10: 0.5049 - val_loss: 0.2669 - val_acc: 0.9050 - val_precision_10: 0.7529 - val_auc_10: 0.8982 - val_recall_10: 0.5904
Epoch 2/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2472 - acc: 0.9074 - precision_10: 0.7808 - auc_10: 0.9050 - recall_10: 0.6030 - val_loss: 0.2360 - val_acc: 0.9103 - val_precision_10: 0.7175 - val_auc_10: 0.9185 - val_recall_10: 0.6723
Epoch 3/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2341 - acc: 0.9089 - precision_10: 0.7824 - auc_10: 0.9214 - recall_10: 0.6134 - val_loss: 0.2442 - val_acc: 0.9112 - val_precision_10: 0.7560 - val_auc_10: 0.9140 - val_recall_10: 0.6276
Epoch 4/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2405 - acc: 0.9109 - precision_10: 0.7979 - auc_10: 0.9105 - recall_10: 0.6096 - val_loss: 0.2659 - val_acc: 0.8999 - val_precision_10: 0.7505 - val_auc_10: 0.9012 - val_recall_10: 0.6101
Epoch 5/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2233 - acc: 0.9158 - precision_10: 0.8115 - auc_10: 0.9249 - recall_10: 0.6318 - val_loss: 0.2201 - val_acc: 0.9131 - val_precision_10: 0.8209 - val_auc_10: 0.9333 - val_recall_10: 0.5602
Epoch 6/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2153 - acc: 0.9198 - precision_10: 0.8093 - auc_10: 0.9295 - recall_10: 0.6666 - val_loss: 0.1971 - val_acc: 0.9278 - val_precision_10: 0.7618 - val_auc_10: 0.9417 - val_recall_10: 0.7895
Epoch 7/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2321 - acc: 0.9113 - precision_10: 0.7720 - auc_10: 0.9206 - recall_10: 0.6492 - val_loss: 0.2091 - val_acc: 0.9221 - val_precision_10: 0.7907 - val_auc_10: 0.9317 - val_recall_10: 0.6543
Epoch 8/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2140 - acc: 0.9199 - precision_10: 0.8136 - auc_10: 0.9310 - recall_10: 0.6613 - val_loss: 0.2306 - val_acc: 0.9163 - val_precision_10: 0.7965 - val_auc_10: 0.9195 - val_recall_10: 0.6528
Epoch 9/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2112 - acc: 0.9249 - precision_10: 0.8253 - auc_10: 0.9300 - recall_10: 0.6856 - val_loss: 0.2642 - val_acc: 0.8945 - val_precision_10: 0.6255 - val_auc_10: 0.9318 - val_recall_10: 0.8211
Epoch 10/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2042 - acc: 0.9247 - precision_10: 0.8102 - auc_10: 0.9386 - recall_10: 0.7048 - val_loss: 0.1922 - val_acc: 0.9291 - val_precision_10: 0.8499 - val_auc_10: 0.9467 - val_recall_10: 0.6605
Epoch 11/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1769 - acc: 0.9349 - precision_10: 0.8324 - auc_10: 0.9538 - recall_10: 0.7532 - val_loss: 0.1605 - val_acc: 0.9419 - val_precision_10: 0.7825 - val_auc_10: 0.9753 - val_recall_10: 0.8808
Epoch 12/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1581 - acc: 0.9417 - precision_10: 0.8380 - auc_10: 0.9638 - recall_10: 0.7976 - val_loss: 0.1506 - val_acc: 0.9488 - val_precision_10: 0.8772 - val_auc_10: 0.9606 - val_recall_10: 0.7512
Epoch 13/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1479 - acc: 0.9453 - precision_10: 0.8499 - auc_10: 0.9687 - recall_10: 0.8083 - val_loss: 0.1296 - val_acc: 0.9496 - val_precision_10: 0.7997 - val_auc_10: 0.9813 - val_recall_10: 0.8922
Epoch 14/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1284 - acc: 0.9512 - precision_10: 0.8594 - auc_10: 0.9777 - recall_10: 0.8390 - val_loss: 0.1180 - val_acc: 0.9557 - val_precision_10: 0.9000 - val_auc_10: 0.9834 - val_recall_10: 0.8133
Epoch 15/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1402 - acc: 0.9489 - precision_10: 0.8617 - auc_10: 0.9732 - recall_10: 0.8185 - val_loss: 0.1225 - val_acc: 0.9538 - val_precision_10: 0.8616 - val_auc_10: 0.9808 - val_recall_10: 0.8547
Epoch 16/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1182 - acc: 0.9550 - precision_10: 0.8702 - auc_10: 0.9813 - recall_10: 0.8517 - val_loss: 0.0965 - val_acc: 0.9617 - val_precision_10: 0.8642 - val_auc_10: 0.9886 - val_recall_10: 0.8939
Epoch 17/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1027 - acc: 0.9597 - precision_10: 0.8861 - auc_10: 0.9866 - recall_10: 0.8648 - val_loss: 0.0961 - val_acc: 0.9622 - val_precision_10: 0.8832 - val_auc_10: 0.9881 - val_recall_10: 0.8754
Epoch 18/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1021 - acc: 0.9605 - precision_10: 0.8850 - auc_10: 0.9865 - recall_10: 0.8713 - val_loss: 0.0903 - val_acc: 0.9643 - val_precision_10: 0.8820 - val_auc_10: 0.9893 - val_recall_10: 0.9016
Epoch 19/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0873 - acc: 0.9654 - precision_10: 0.8969 - auc_10: 0.9905 - recall_10: 0.8910 - val_loss: 0.0896 - val_acc: 0.9672 - val_precision_10: 0.9155 - val_auc_10: 0.9868 - val_recall_10: 0.8498
Epoch 20/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0868 - acc: 0.9659 - precision_10: 0.9029 - auc_10: 0.9906 - recall_10: 0.8868 - val_loss: 0.0881 - val_acc: 0.9682 - val_precision_10: 0.9104 - val_auc_10: 0.9874 - val_recall_10: 0.8819
Epoch 21/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0890 - acc: 0.9657 - precision_10: 0.8981 - auc_10: 0.9894 - recall_10: 0.8911 - val_loss: 0.0763 - val_acc: 0.9711 - val_precision_10: 0.9221 - val_auc_10: 0.9915 - val_recall_10: 0.8758
Epoch 22/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0782 - acc: 0.9693 - precision_10: 0.9116 - auc_10: 0.9924 - recall_10: 0.8991 - val_loss: 0.0777 - val_acc: 0.9697 - val_precision_10: 0.9060 - val_auc_10: 0.9911 - val_recall_10: 0.8898
Epoch 23/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0744 - acc: 0.9705 - precision_10: 0.9122 - auc_10: 0.9932 - recall_10: 0.9066 - val_loss: 0.0836 - val_acc: 0.9672 - val_precision_10: 0.8955 - val_auc_10: 0.9906 - val_recall_10: 0.9058
Epoch 24/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0968 - acc: 0.9635 - precision_10: 0.8925 - auc_10: 0.9872 - recall_10: 0.8832 - val_loss: 0.0839 - val_acc: 0.9673 - val_precision_10: 0.8672 - val_auc_10: 0.9911 - val_recall_10: 0.9165
Epoch 25/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0801 - acc: 0.9687 - precision_10: 0.9082 - auc_10: 0.9920 - recall_10: 0.8994 - val_loss: 0.0782 - val_acc: 0.9693 - val_precision_10: 0.9041 - val_auc_10: 0.9913 - val_recall_10: 0.9004
Epoch 26/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0782 - acc: 0.9693 - precision_10: 0.9066 - auc_10: 0.9923 - recall_10: 0.9055 - val_loss: 0.0840 - val_acc: 0.9680 - val_precision_10: 0.9022 - val_auc_10: 0.9898 - val_recall_10: 0.8911
Epoch 27/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0722 - acc: 0.9716 - precision_10: 0.9190 - auc_10: 0.9933 - recall_10: 0.9062 - val_loss: 0.0698 - val_acc: 0.9740 - val_precision_10: 0.9155 - val_auc_10: 0.9916 - val_recall_10: 0.9143
Epoch 28/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0692 - acc: 0.9727 - precision_10: 0.9193 - auc_10: 0.9939 - recall_10: 0.9128 - val_loss: 0.0810 - val_acc: 0.9709 - val_precision_10: 0.9059 - val_auc_10: 0.9895 - val_recall_10: 0.8944
Epoch 29/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0665 - acc: 0.9741 - precision_10: 0.9229 - auc_10: 0.9944 - recall_10: 0.9184 - val_loss: 0.0812 - val_acc: 0.9688 - val_precision_10: 0.9114 - val_auc_10: 0.9911 - val_recall_10: 0.9031
Epoch 30/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0766 - acc: 0.9704 - precision_10: 0.9156 - auc_10: 0.9921 - recall_10: 0.9017 - val_loss: 0.0745 - val_acc: 0.9721 - val_precision_10: 0.9148 - val_auc_10: 0.9913 - val_recall_10: 0.9056
Epoch 31/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0641 - acc: 0.9745 - precision_10: 0.9245 - auc_10: 0.9949 - recall_10: 0.9192 - val_loss: 0.0765 - val_acc: 0.9710 - val_precision_10: 0.9267 - val_auc_10: 0.9920 - val_recall_10: 0.8935
Epoch 32/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0592 - acc: 0.9765 - precision_10: 0.9329 - auc_10: 0.9955 - recall_10: 0.9224 - val_loss: 0.0696 - val_acc: 0.9731 - val_precision_10: 0.9187 - val_auc_10: 0.9930 - val_recall_10: 0.9029
Epoch 33/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0576 - acc: 0.9770 - precision_10: 0.9336 - auc_10: 0.9957 - recall_10: 0.9249 - val_loss: 0.0716 - val_acc: 0.9729 - val_precision_10: 0.9224 - val_auc_10: 0.9926 - val_recall_10: 0.9038
Epoch 34/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0558 - acc: 0.9775 - precision_10: 0.9366 - auc_10: 0.9961 - recall_10: 0.9253 - val_loss: 0.0734 - val_acc: 0.9731 - val_precision_10: 0.9253 - val_auc_10: 0.9924 - val_recall_10: 0.9022
Epoch 35/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0529 - acc: 0.9784 - precision_10: 0.9398 - auc_10: 0.9964 - recall_10: 0.9275 - val_loss: 0.0825 - val_acc: 0.9713 - val_precision_10: 0.9318 - val_auc_10: 0.9898 - val_recall_10: 0.9034
Epoch 36/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0595 - acc: 0.9761 - precision_10: 0.9333 - auc_10: 0.9954 - recall_10: 0.9193 - val_loss: 0.0701 - val_acc: 0.9734 - val_precision_10: 0.9115 - val_auc_10: 0.9927 - val_recall_10: 0.9211
Epoch 37/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0528 - acc: 0.9785 - precision_10: 0.9394 - auc_10: 0.9964 - recall_10: 0.9287 - val_loss: 0.0759 - val_acc: 0.9741 - val_precision_10: 0.9074 - val_auc_10: 0.9912 - val_recall_10: 0.9140
Epoch 38/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0537 - acc: 0.9781 - precision_10: 0.9405 - auc_10: 0.9962 - recall_10: 0.9246 - val_loss: 0.0751 - val_acc: 0.9709 - val_precision_10: 0.9218 - val_auc_10: 0.9912 - val_recall_10: 0.8973
Epoch 39/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0545 - acc: 0.9780 - precision_10: 0.9386 - auc_10: 0.9960 - recall_10: 0.9261 - val_loss: 0.0740 - val_acc: 0.9726 - val_precision_10: 0.9194 - val_auc_10: 0.9918 - val_recall_10: 0.9094
Epoch 40/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0492 - acc: 0.9797 - precision_10: 0.9448 - auc_10: 0.9968 - recall_10: 0.9305 - val_loss: 0.0737 - val_acc: 0.9739 - val_precision_10: 0.9145 - val_auc_10: 0.9927 - val_recall_10: 0.9180
Epoch 41/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0467 - acc: 0.9808 - precision_10: 0.9484 - auc_10: 0.9972 - recall_10: 0.9331 - val_loss: 0.0738 - val_acc: 0.9746 - val_precision_10: 0.9356 - val_auc_10: 0.9917 - val_recall_10: 0.9009
Epoch 42/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0457 - acc: 0.9812 - precision_10: 0.9508 - auc_10: 0.9973 - recall_10: 0.9331 - val_loss: 0.0782 - val_acc: 0.9750 - val_precision_10: 0.9181 - val_auc_10: 0.9921 - val_recall_10: 0.9214
Epoch 43/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0432 - acc: 0.9821 - precision_10: 0.9519 - auc_10: 0.9975 - recall_10: 0.9379 - val_loss: 0.0822 - val_acc: 0.9741 - val_precision_10: 0.9162 - val_auc_10: 0.9918 - val_recall_10: 0.9240
Epoch 44/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0416 - acc: 0.9826 - precision_10: 0.9537 - auc_10: 0.9977 - recall_10: 0.9394 - val_loss: 0.0686 - val_acc: 0.9752 - val_precision_10: 0.9052 - val_auc_10: 0.9936 - val_recall_10: 0.9330
Epoch 45/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0413 - acc: 0.9828 - precision_10: 0.9520 - auc_10: 0.9978 - recall_10: 0.9421 - val_loss: 0.0793 - val_acc: 0.9747 - val_precision_10: 0.9304 - val_auc_10: 0.9905 - val_recall_10: 0.9023
Epoch 46/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0390 - acc: 0.9837 - precision_10: 0.9555 - auc_10: 0.9980 - recall_10: 0.9440 - val_loss: 0.0863 - val_acc: 0.9749 - val_precision_10: 0.9039 - val_auc_10: 0.9922 - val_recall_10: 0.9292
Epoch 47/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0408 - acc: 0.9830 - precision_10: 0.9512 - auc_10: 0.9978 - recall_10: 0.9446 - val_loss: 0.0792 - val_acc: 0.9734 - val_precision_10: 0.9320 - val_auc_10: 0.9907 - val_recall_10: 0.9036
Epoch 48/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0424 - acc: 0.9824 - precision_10: 0.9509 - auc_10: 0.9977 - recall_10: 0.9409 - val_loss: 0.0800 - val_acc: 0.9750 - val_precision_10: 0.9151 - val_auc_10: 0.9917 - val_recall_10: 0.9260
Epoch 49/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0376 - acc: 0.9843 - precision_10: 0.9570 - auc_10: 0.9982 - recall_10: 0.9462 - val_loss: 0.0839 - val_acc: 0.9743 - val_precision_10: 0.9013 - val_auc_10: 0.9917 - val_recall_10: 0.9393
Epoch 50/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0381 - acc: 0.9839 - precision_10: 0.9544 - auc_10: 0.9981 - recall_10: 0.9470 - val_loss: 0.0842 - val_acc: 0.9764 - val_precision_10: 0.9222 - val_auc_10: 0.9920 - val_recall_10: 0.9266
Epoch 51/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0362 - acc: 0.9848 - precision_10: 0.9561 - auc_10: 0.9983 - recall_10: 0.9505 - val_loss: 0.0875 - val_acc: 0.9743 - val_precision_10: 0.9205 - val_auc_10: 0.9906 - val_recall_10: 0.9192
Epoch 52/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0334 - acc: 0.9858 - precision_10: 0.9591 - auc_10: 0.9986 - recall_10: 0.9538 - val_loss: 0.0830 - val_acc: 0.9745 - val_precision_10: 0.9185 - val_auc_10: 0.9916 - val_recall_10: 0.9320
Epoch 53/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0329 - acc: 0.9860 - precision_10: 0.9593 - auc_10: 0.9986 - recall_10: 0.9547 - val_loss: 0.0930 - val_acc: 0.9758 - val_precision_10: 0.8982 - val_auc_10: 0.9895 - val_recall_10: 0.9314
Epoch 54/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0319 - acc: 0.9864 - precision_10: 0.9597 - auc_10: 0.9987 - recall_10: 0.9568 - val_loss: 0.0943 - val_acc: 0.9751 - val_precision_10: 0.9232 - val_auc_10: 0.9905 - val_recall_10: 0.9295
Epoch 55/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0297 - acc: 0.9873 - precision_10: 0.9628 - auc_10: 0.9989 - recall_10: 0.9593 - val_loss: 0.1104 - val_acc: 0.9734 - val_precision_10: 0.9016 - val_auc_10: 0.9873 - val_recall_10: 0.9192
Epoch 56/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0285 - acc: 0.9878 - precision_10: 0.9633 - auc_10: 0.9990 - recall_10: 0.9622 - val_loss: 0.0993 - val_acc: 0.9752 - val_precision_10: 0.9208 - val_auc_10: 0.9893 - val_recall_10: 0.9253
Epoch 57/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0277 - acc: 0.9881 - precision_10: 0.9650 - auc_10: 0.9990 - recall_10: 0.9621 - val_loss: 0.1125 - val_acc: 0.9739 - val_precision_10: 0.9032 - val_auc_10: 0.9888 - val_recall_10: 0.9342
Epoch 58/100
32/33 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9885 - precision_10: 0.9644 - auc_10: 0.9991 - recall_10: 0.9651Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 85ms/step - loss: 0.0271 - acc: 0.9884 - precision_10: 0.9644 - auc_10: 0.9991 - recall_10: 0.9649 - val_loss: 0.1102 - val_acc: 0.9742 - val_precision_10: 0.9327 - val_auc_10: 0.9863 - val_recall_10: 0.9116
Epoch 00058: early stopping
6/6 [==============================] - 0s 25ms/step - loss: 0.0863 - acc: 0.9721 - precision_10: 0.8865 - auc_10: 0.9933 - recall_10: 0.9500
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb612456c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8379671656333154
test_sensitivity 0.9526787653949633
test_specifitivity 0.9753601857518969
test_accuracy 0.9716783311631945
test_precision 0.8822503017734996
test_jaccard_score 0.8379671656333154
test_dicecoef 0.9161129367904289
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-105537.h5
[0.96690436 0.8154668  0.87944708 0.92258652 0.97549255] [0.97167833 0.83796717 0.8822503  0.95267877 0.97536019]

-------------------------
Rep: 2
-------------------------

2021-09-25 10:55:38.162429: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:55:38.162484: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Normalized per channel
Applied normalization
Data augumentation off
Prep done
Training samples: 258, channel mean: -1.389123527662048e-17,
Validation samples: 63, channel mean: -0.0011996397803231908
Model built.
Model: "functional_23"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_12 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_209 (Conv2D)             (None, 256, 256, 32) 320         input_12[0][0]
__________________________________________________________________________________________________
conv2d_210 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_209[0][0]
__________________________________________________________________________________________________
max_pooling2d_44 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_210[0][0]
__________________________________________________________________________________________________
conv2d_211 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_44[0][0]
__________________________________________________________________________________________________
conv2d_212 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_211[0][0]
__________________________________________________________________________________________________
max_pooling2d_45 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_212[0][0]
__________________________________________________________________________________________________
conv2d_213 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_45[0][0]
__________________________________________________________________________________________________
conv2d_214 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_213[0][0]
__________________________________________________________________________________________________
max_pooling2d_46 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_214[0][0]
__________________________________________________________________________________________________
conv2d_215 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_46[0][0]
__________________________________________________________________________________________________
conv2d_216 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_215[0][0]
__________________________________________________________________________________________________
max_pooling2d_47 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_216[0][0]
__________________________________________________________________________________________________
conv2d_217 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_47[0][0]
__________________________________________________________________________________________________
conv2d_218 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_217[0][0]
__________________________________________________________________________________________________
up_sampling2d_44 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_218[0][0]
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_44[0][0]
                                                                 conv2d_216[0][0]
__________________________________________________________________________________________________
conv2d_219 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_44[0][0]
__________________________________________________________________________________________________
conv2d_220 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_219[0][0]
__________________________________________________________________________________________________
up_sampling2d_45 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_220[0][0]
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_45[0][0]
                                                                 conv2d_214[0][0]
__________________________________________________________________________________________________
conv2d_221 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_45[0][0]
__________________________________________________________________________________________________
conv2d_222 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_221[0][0]
__________________________________________________________________________________________________
up_sampling2d_46 (UpSampling2D) (None, 128, 128, 128 0           conv2d_222[0][0]
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_46[0][0]
                                                                 conv2d_212[0][0]
__________________________________________________________________________________________________
conv2d_223 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_46[0][0]
__________________________________________________________________________________________________
conv2d_224 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_223[0][0]
__________________________________________________________________________________________________
up_sampling2d_47 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_224[0][0]
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_47[0][0]
                                                                 conv2d_210[0][0]
__________________________________________________________________________________________________
conv2d_225 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_47[0][0]
__________________________________________________________________________________________________
conv2d_226 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_225[0][0]
__________________________________________________________________________________________________
conv2d_227 (Conv2D)             (None, 256, 256, 1)  33          conv2d_226[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6925 - acc: 0.1622 - precision_11: 0.1559 - auc_11: 0.8648 - recall_11: 0.99452021-09-25 10:55:40.419349: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:55:40.419417: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 10:55:40.690791: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 10:55:40.699880: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-105538/train/plugins/profile/2021_09_25_10_55_40
2021-09-25 10:55:40.704074: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-105538/train/plugins/profile/2021_09_25_10_55_40/xeon-e-v100.trace.json.gz
2021-09-25 10:55:40.721585: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-105538/train/plugins/profile/2021_09_25_10_55_40
2021-09-25 10:55:40.730296: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-105538/train/plugins/profile/2021_09_25_10_55_40/xeon-e-v100.memory_profile.json.gz
2021-09-25 10:55:40.754411: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-105538/train/plugins/profile/2021_09_25_10_55_40Dumped tool data for xplane.pb to logs/fit/20210925-105538/train/plugins/profile/2021_09_25_10_55_40/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-105538/train/plugins/profile/2021_09_25_10_55_40/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-105538/train/plugins/profile/2021_09_25_10_55_40/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-105538/train/plugins/profile/2021_09_25_10_55_40/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-105538/train/plugins/profile/2021_09_25_10_55_40/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6891 - acc: 0.4685 - precision_11: 0.2378 - auc_11: 0.8304 - recall_11: 0.8563WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0681s vs `on_train_batch_end` time: 0.2684s). Check your callbacks.
33/33 [==============================] - 3s 105ms/step - loss: 0.4395 - acc: 0.8663 - precision_11: 0.5964 - auc_11: 0.8008 - recall_11: 0.5629 - val_loss: 0.2937 - val_acc: 0.9002 - val_precision_11: 0.7051 - val_auc_11: 0.8913 - val_recall_11: 0.6290
Epoch 2/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2475 - acc: 0.9046 - precision_11: 0.7502 - auc_11: 0.9106 - recall_11: 0.6241 - val_loss: 0.2390 - val_acc: 0.9091 - val_precision_11: 0.7130 - val_auc_11: 0.9158 - val_recall_11: 0.6684
Epoch 3/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2344 - acc: 0.9094 - precision_11: 0.7788 - auc_11: 0.9204 - recall_11: 0.6226 - val_loss: 0.2466 - val_acc: 0.9128 - val_precision_11: 0.7792 - val_auc_11: 0.9142 - val_recall_11: 0.6073
Epoch 4/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2395 - acc: 0.9114 - precision_11: 0.7987 - auc_11: 0.9122 - recall_11: 0.6127 - val_loss: 0.2733 - val_acc: 0.8938 - val_precision_11: 0.6991 - val_auc_11: 0.9010 - val_recall_11: 0.6519
Epoch 5/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2273 - acc: 0.9136 - precision_11: 0.7927 - auc_11: 0.9234 - recall_11: 0.6382 - val_loss: 0.1983 - val_acc: 0.9244 - val_precision_11: 0.8260 - val_auc_11: 0.9422 - val_recall_11: 0.6470
Epoch 6/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2183 - acc: 0.9186 - precision_11: 0.8219 - auc_11: 0.9278 - recall_11: 0.6411 - val_loss: 0.2046 - val_acc: 0.9250 - val_precision_11: 0.7857 - val_auc_11: 0.9330 - val_recall_11: 0.7216
Epoch 7/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2115 - acc: 0.9207 - precision_11: 0.8116 - auc_11: 0.9337 - recall_11: 0.6707 - val_loss: 0.1901 - val_acc: 0.9272 - val_precision_11: 0.7834 - val_auc_11: 0.9461 - val_recall_11: 0.7121
Epoch 8/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1918 - acc: 0.9284 - precision_11: 0.8210 - auc_11: 0.9467 - recall_11: 0.7184 - val_loss: 0.2129 - val_acc: 0.9203 - val_precision_11: 0.8388 - val_auc_11: 0.9340 - val_recall_11: 0.6321
Epoch 9/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2013 - acc: 0.9285 - precision_11: 0.8153 - auc_11: 0.9413 - recall_11: 0.7276 - val_loss: 0.1873 - val_acc: 0.9318 - val_precision_11: 0.7357 - val_auc_11: 0.9677 - val_recall_11: 0.8842
Epoch 10/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1623 - acc: 0.9405 - precision_11: 0.8334 - auc_11: 0.9604 - recall_11: 0.7947 - val_loss: 0.1220 - val_acc: 0.9544 - val_precision_11: 0.8807 - val_auc_11: 0.9797 - val_recall_11: 0.8173
Epoch 11/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1426 - acc: 0.9457 - precision_11: 0.8437 - auc_11: 0.9723 - recall_11: 0.8200 - val_loss: 0.1288 - val_acc: 0.9535 - val_precision_11: 0.8637 - val_auc_11: 0.9765 - val_recall_11: 0.8414
Epoch 12/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1268 - acc: 0.9517 - precision_11: 0.8569 - auc_11: 0.9778 - recall_11: 0.8461 - val_loss: 0.1192 - val_acc: 0.9572 - val_precision_11: 0.9146 - val_auc_11: 0.9798 - val_recall_11: 0.7766
Epoch 13/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1187 - acc: 0.9549 - precision_11: 0.8683 - auc_11: 0.9812 - recall_11: 0.8538 - val_loss: 0.0900 - val_acc: 0.9646 - val_precision_11: 0.8644 - val_auc_11: 0.9887 - val_recall_11: 0.9104
Epoch 14/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1070 - acc: 0.9591 - precision_11: 0.8768 - auc_11: 0.9846 - recall_11: 0.8725 - val_loss: 0.0933 - val_acc: 0.9634 - val_precision_11: 0.8938 - val_auc_11: 0.9882 - val_recall_11: 0.8751
Epoch 15/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0882 - acc: 0.9655 - precision_11: 0.8961 - auc_11: 0.9903 - recall_11: 0.8927 - val_loss: 0.0922 - val_acc: 0.9636 - val_precision_11: 0.8854 - val_auc_11: 0.9890 - val_recall_11: 0.8931
Epoch 16/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0874 - acc: 0.9654 - precision_11: 0.8898 - auc_11: 0.9904 - recall_11: 0.8999 - val_loss: 0.0844 - val_acc: 0.9675 - val_precision_11: 0.8824 - val_auc_11: 0.9910 - val_recall_11: 0.9123
Epoch 17/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0827 - acc: 0.9674 - precision_11: 0.9041 - auc_11: 0.9914 - recall_11: 0.8953 - val_loss: 0.0779 - val_acc: 0.9695 - val_precision_11: 0.8915 - val_auc_11: 0.9924 - val_recall_11: 0.9180
Epoch 18/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0795 - acc: 0.9682 - precision_11: 0.9018 - auc_11: 0.9921 - recall_11: 0.9036 - val_loss: 0.0850 - val_acc: 0.9671 - val_precision_11: 0.8717 - val_auc_11: 0.9917 - val_recall_11: 0.9359
Epoch 19/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0741 - acc: 0.9708 - precision_11: 0.9116 - auc_11: 0.9932 - recall_11: 0.9099 - val_loss: 0.0796 - val_acc: 0.9696 - val_precision_11: 0.9119 - val_auc_11: 0.9898 - val_recall_11: 0.8720
Epoch 20/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0704 - acc: 0.9722 - precision_11: 0.9178 - auc_11: 0.9937 - recall_11: 0.9115 - val_loss: 0.0731 - val_acc: 0.9728 - val_precision_11: 0.9132 - val_auc_11: 0.9912 - val_recall_11: 0.9110
Epoch 21/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0725 - acc: 0.9718 - precision_11: 0.9150 - auc_11: 0.9929 - recall_11: 0.9125 - val_loss: 0.0743 - val_acc: 0.9717 - val_precision_11: 0.8805 - val_auc_11: 0.9931 - val_recall_11: 0.9321
Epoch 22/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1049 - acc: 0.9644 - precision_11: 0.9067 - auc_11: 0.9820 - recall_11: 0.8722 - val_loss: 0.0829 - val_acc: 0.9695 - val_precision_11: 0.8908 - val_auc_11: 0.9907 - val_recall_11: 0.9073
Epoch 23/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0773 - acc: 0.9706 - precision_11: 0.9134 - auc_11: 0.9918 - recall_11: 0.9062 - val_loss: 0.0823 - val_acc: 0.9689 - val_precision_11: 0.8892 - val_auc_11: 0.9913 - val_recall_11: 0.9259
Epoch 24/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0698 - acc: 0.9724 - precision_11: 0.9179 - auc_11: 0.9938 - recall_11: 0.9131 - val_loss: 0.0734 - val_acc: 0.9702 - val_precision_11: 0.8761 - val_auc_11: 0.9931 - val_recall_11: 0.9269
Epoch 25/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0653 - acc: 0.9740 - precision_11: 0.9240 - auc_11: 0.9947 - recall_11: 0.9165 - val_loss: 0.0735 - val_acc: 0.9720 - val_precision_11: 0.9116 - val_auc_11: 0.9923 - val_recall_11: 0.9102
Epoch 26/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0583 - acc: 0.9765 - precision_11: 0.9314 - auc_11: 0.9957 - recall_11: 0.9242 - val_loss: 0.0765 - val_acc: 0.9727 - val_precision_11: 0.9057 - val_auc_11: 0.9914 - val_recall_11: 0.9204
Epoch 27/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0563 - acc: 0.9772 - precision_11: 0.9348 - auc_11: 0.9959 - recall_11: 0.9248 - val_loss: 0.0582 - val_acc: 0.9764 - val_precision_11: 0.9157 - val_auc_11: 0.9954 - val_recall_11: 0.9309
Epoch 28/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0545 - acc: 0.9778 - precision_11: 0.9376 - auc_11: 0.9962 - recall_11: 0.9259 - val_loss: 0.0721 - val_acc: 0.9744 - val_precision_11: 0.9103 - val_auc_11: 0.9930 - val_recall_11: 0.9160
Epoch 29/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0526 - acc: 0.9784 - precision_11: 0.9397 - auc_11: 0.9964 - recall_11: 0.9276 - val_loss: 0.0767 - val_acc: 0.9713 - val_precision_11: 0.9198 - val_auc_11: 0.9918 - val_recall_11: 0.9095
Epoch 30/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0501 - acc: 0.9794 - precision_11: 0.9445 - auc_11: 0.9968 - recall_11: 0.9283 - val_loss: 0.0702 - val_acc: 0.9753 - val_precision_11: 0.9106 - val_auc_11: 0.9937 - val_recall_11: 0.9333
Epoch 31/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0494 - acc: 0.9797 - precision_11: 0.9444 - auc_11: 0.9968 - recall_11: 0.9306 - val_loss: 0.0743 - val_acc: 0.9727 - val_precision_11: 0.9277 - val_auc_11: 0.9924 - val_recall_11: 0.9032
Epoch 32/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0490 - acc: 0.9799 - precision_11: 0.9447 - auc_11: 0.9969 - recall_11: 0.9318 - val_loss: 0.0698 - val_acc: 0.9735 - val_precision_11: 0.9291 - val_auc_11: 0.9925 - val_recall_11: 0.8944
Epoch 33/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0466 - acc: 0.9808 - precision_11: 0.9468 - auc_11: 0.9971 - recall_11: 0.9352 - val_loss: 0.0703 - val_acc: 0.9741 - val_precision_11: 0.9412 - val_auc_11: 0.9921 - val_recall_11: 0.8914
Epoch 34/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0452 - acc: 0.9813 - precision_11: 0.9486 - auc_11: 0.9973 - recall_11: 0.9361 - val_loss: 0.0731 - val_acc: 0.9731 - val_precision_11: 0.9387 - val_auc_11: 0.9923 - val_recall_11: 0.8875
Epoch 35/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0446 - acc: 0.9814 - precision_11: 0.9491 - auc_11: 0.9974 - recall_11: 0.9366 - val_loss: 0.0868 - val_acc: 0.9715 - val_precision_11: 0.9384 - val_auc_11: 0.9888 - val_recall_11: 0.8970
Epoch 36/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0507 - acc: 0.9795 - precision_11: 0.9433 - auc_11: 0.9966 - recall_11: 0.9308 - val_loss: 0.0736 - val_acc: 0.9736 - val_precision_11: 0.9177 - val_auc_11: 0.9920 - val_recall_11: 0.9150
Epoch 37/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0569 - acc: 0.9778 - precision_11: 0.9377 - auc_11: 0.9953 - recall_11: 0.9254 - val_loss: 0.0927 - val_acc: 0.9693 - val_precision_11: 0.8754 - val_auc_11: 0.9905 - val_recall_11: 0.9181
Epoch 38/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0623 - acc: 0.9748 - precision_11: 0.9265 - auc_11: 0.9950 - recall_11: 0.9187 - val_loss: 0.0753 - val_acc: 0.9701 - val_precision_11: 0.9013 - val_auc_11: 0.9922 - val_recall_11: 0.9167
Epoch 39/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0498 - acc: 0.9795 - precision_11: 0.9448 - auc_11: 0.9967 - recall_11: 0.9292 - val_loss: 0.0779 - val_acc: 0.9721 - val_precision_11: 0.9135 - val_auc_11: 0.9905 - val_recall_11: 0.9132
Epoch 40/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0505 - acc: 0.9799 - precision_11: 0.9410 - auc_11: 0.9966 - recall_11: 0.9356 - val_loss: 0.0823 - val_acc: 0.9733 - val_precision_11: 0.9203 - val_auc_11: 0.9906 - val_recall_11: 0.9072
Epoch 41/100
32/33 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9819 - precision_11: 0.9509 - auc_11: 0.9976 - recall_11: 0.9378Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 85ms/step - loss: 0.0434 - acc: 0.9819 - precision_11: 0.9507 - auc_11: 0.9976 - recall_11: 0.9376 - val_loss: 0.0702 - val_acc: 0.9755 - val_precision_11: 0.9335 - val_auc_11: 0.9921 - val_recall_11: 0.9092
Epoch 00041: early stopping
6/6 [==============================] - 0s 25ms/step - loss: 0.0702 - acc: 0.9722 - precision_11: 0.9036 - auc_11: 0.9938 - recall_11: 0.9278
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb615716ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8379441250580176
test_sensitivity 0.9345828946708779
test_specifitivity 0.9790284294962095
test_accuracy 0.9718136257595487
test_precision 0.896221898605622
test_jaccard_score 0.8379441250580176
test_dicecoef 0.9150005061623994
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-105751.h5
[1.9385827  1.65343396 1.76169739 1.87526529 1.95085274] [0.97181363 0.83794413 0.8962219  0.93458289 0.97902843]

-------------------------
Averaged metrics for Baseline + Per Channel Normalization - bacteria: [0.97013211 0.83045936 0.8859731  0.93661606 0.97662706]
-------------------------


-------------------------
RUN: Baseline + Gaussian Blur - bacteria, PARAMS: {'per_channel_normalization': True, 'gaussian_blur': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-25 10:57:52.163447: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:57:52.163501: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied gaussian blur on all input images
Normalized per channel
Applied normalization
Data augumentation off
Prep done
Training samples: 258, channel mean: -4.652824206883529e-18,
Validation samples: 63, channel mean: -0.0012084537275524774
Model built.
Model: "functional_25"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_13 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_228 (Conv2D)             (None, 256, 256, 32) 320         input_13[0][0]
__________________________________________________________________________________________________
conv2d_229 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_228[0][0]
__________________________________________________________________________________________________
max_pooling2d_48 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_229[0][0]
__________________________________________________________________________________________________
conv2d_230 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_48[0][0]
__________________________________________________________________________________________________
conv2d_231 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_230[0][0]
__________________________________________________________________________________________________
max_pooling2d_49 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_231[0][0]
__________________________________________________________________________________________________
conv2d_232 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_49[0][0]
__________________________________________________________________________________________________
conv2d_233 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_232[0][0]
__________________________________________________________________________________________________
max_pooling2d_50 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_233[0][0]
__________________________________________________________________________________________________
conv2d_234 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_50[0][0]
__________________________________________________________________________________________________
conv2d_235 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_234[0][0]
__________________________________________________________________________________________________
max_pooling2d_51 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_235[0][0]
__________________________________________________________________________________________________
conv2d_236 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_51[0][0]
__________________________________________________________________________________________________
conv2d_237 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_236[0][0]
__________________________________________________________________________________________________
up_sampling2d_48 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_237[0][0]
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_48[0][0]
                                                                 conv2d_235[0][0]
__________________________________________________________________________________________________
conv2d_238 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_48[0][0]
__________________________________________________________________________________________________
conv2d_239 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_238[0][0]
__________________________________________________________________________________________________
up_sampling2d_49 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_239[0][0]
__________________________________________________________________________________________________
concatenate_49 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_49[0][0]
                                                                 conv2d_233[0][0]
__________________________________________________________________________________________________
conv2d_240 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_49[0][0]
__________________________________________________________________________________________________
conv2d_241 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_240[0][0]
__________________________________________________________________________________________________
up_sampling2d_50 (UpSampling2D) (None, 128, 128, 128 0           conv2d_241[0][0]
__________________________________________________________________________________________________
concatenate_50 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_50[0][0]
                                                                 conv2d_231[0][0]
__________________________________________________________________________________________________
conv2d_242 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_50[0][0]
__________________________________________________________________________________________________
conv2d_243 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_242[0][0]
__________________________________________________________________________________________________
up_sampling2d_51 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_243[0][0]
__________________________________________________________________________________________________
concatenate_51 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_51[0][0]
                                                                 conv2d_229[0][0]
__________________________________________________________________________________________________
conv2d_244 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_51[0][0]
__________________________________________________________________________________________________
conv2d_245 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_244[0][0]
__________________________________________________________________________________________________
conv2d_246 (Conv2D)             (None, 256, 256, 1)  33          conv2d_245[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6848 - acc: 0.8770 - precision_12: 0.5745 - auc_12: 0.8935 - recall_12: 0.80482021-09-25 10:57:57.194557: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 10:57:57.194612: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 10:57:57.459817: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 10:57:57.468326: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-105752/train/plugins/profile/2021_09_25_10_57_57
2021-09-25 10:57:57.472384: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-105752/train/plugins/profile/2021_09_25_10_57_57/xeon-e-v100.trace.json.gz
2021-09-25 10:57:57.489579: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-105752/train/plugins/profile/2021_09_25_10_57_57
2021-09-25 10:57:57.496739: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-105752/train/plugins/profile/2021_09_25_10_57_57/xeon-e-v100.memory_profile.json.gz
2021-09-25 10:57:57.517255: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-105752/train/plugins/profile/2021_09_25_10_57_57Dumped tool data for xplane.pb to logs/fit/20210925-105752/train/plugins/profile/2021_09_25_10_57_57/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-105752/train/plugins/profile/2021_09_25_10_57_57/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-105752/train/plugins/profile/2021_09_25_10_57_57/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-105752/train/plugins/profile/2021_09_25_10_57_57/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-105752/train/plugins/profile/2021_09_25_10_57_57/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6820 - acc: 0.8227 - precision_12: 0.5121 - auc_12: 0.8597 - recall_12: 0.7713WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0658s vs `on_train_batch_end` time: 0.2585s). Check your callbacks.
33/33 [==============================] - 4s 112ms/step - loss: 0.3931 - acc: 0.8879 - precision_12: 0.7091 - auc_12: 0.8266 - recall_12: 0.5327 - val_loss: 0.2908 - val_acc: 0.8999 - val_precision_12: 0.7025 - val_auc_12: 0.8925 - val_recall_12: 0.6318
Epoch 2/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2538 - acc: 0.9064 - precision_12: 0.7688 - auc_12: 0.9047 - recall_12: 0.6114 - val_loss: 0.2412 - val_acc: 0.9097 - val_precision_12: 0.7539 - val_auc_12: 0.9049 - val_recall_12: 0.5994
Epoch 3/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2431 - acc: 0.9063 - precision_12: 0.7741 - auc_12: 0.9144 - recall_12: 0.6029 - val_loss: 0.2389 - val_acc: 0.9098 - val_precision_12: 0.7598 - val_auc_12: 0.9085 - val_recall_12: 0.6082
Epoch 4/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2414 - acc: 0.9081 - precision_12: 0.7951 - auc_12: 0.9127 - recall_12: 0.5901 - val_loss: 0.2751 - val_acc: 0.8957 - val_precision_12: 0.7367 - val_auc_12: 0.8946 - val_recall_12: 0.5949
Epoch 5/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2336 - acc: 0.9115 - precision_12: 0.8085 - auc_12: 0.9190 - recall_12: 0.6010 - val_loss: 0.2146 - val_acc: 0.9199 - val_precision_12: 0.8163 - val_auc_12: 0.9303 - val_recall_12: 0.6215
Epoch 6/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2293 - acc: 0.9123 - precision_12: 0.8210 - auc_12: 0.9222 - recall_12: 0.5931 - val_loss: 0.2316 - val_acc: 0.9171 - val_precision_12: 0.7731 - val_auc_12: 0.9124 - val_recall_12: 0.6724
Epoch 7/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2300 - acc: 0.9117 - precision_12: 0.7990 - auc_12: 0.9221 - recall_12: 0.6143 - val_loss: 0.2210 - val_acc: 0.9200 - val_precision_12: 0.7824 - val_auc_12: 0.9237 - val_recall_12: 0.6469
Epoch 8/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2252 - acc: 0.9167 - precision_12: 0.8128 - auc_12: 0.9226 - recall_12: 0.6372 - val_loss: 0.2462 - val_acc: 0.9105 - val_precision_12: 0.7749 - val_auc_12: 0.9039 - val_recall_12: 0.6342
Epoch 9/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2263 - acc: 0.9181 - precision_12: 0.8202 - auc_12: 0.9205 - recall_12: 0.6393 - val_loss: 0.2565 - val_acc: 0.9001 - val_precision_12: 0.6746 - val_auc_12: 0.9097 - val_recall_12: 0.7058
Epoch 10/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2353 - acc: 0.9126 - precision_12: 0.7887 - auc_12: 0.9185 - recall_12: 0.6359 - val_loss: 0.2385 - val_acc: 0.9128 - val_precision_12: 0.8779 - val_auc_12: 0.9093 - val_recall_12: 0.5107
Epoch 11/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2297 - acc: 0.9161 - precision_12: 0.8164 - auc_12: 0.9189 - recall_12: 0.6279 - val_loss: 0.2410 - val_acc: 0.9093 - val_precision_12: 0.7695 - val_auc_12: 0.9106 - val_recall_12: 0.6168
Epoch 12/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2218 - acc: 0.9175 - precision_12: 0.8046 - auc_12: 0.9248 - recall_12: 0.6547 - val_loss: 0.2083 - val_acc: 0.9287 - val_precision_12: 0.8625 - val_auc_12: 0.9208 - val_recall_12: 0.6029
Epoch 13/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2245 - acc: 0.9171 - precision_12: 0.8070 - auc_12: 0.9226 - recall_12: 0.6483 - val_loss: 0.2474 - val_acc: 0.9106 - val_precision_12: 0.7344 - val_auc_12: 0.9078 - val_recall_12: 0.6461
Epoch 14/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2244 - acc: 0.9175 - precision_12: 0.8015 - auc_12: 0.9232 - recall_12: 0.6587 - val_loss: 0.2284 - val_acc: 0.9171 - val_precision_12: 0.8413 - val_auc_12: 0.9162 - val_recall_12: 0.5933
Epoch 15/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2186 - acc: 0.9188 - precision_12: 0.8076 - auc_12: 0.9269 - recall_12: 0.6607 - val_loss: 0.2458 - val_acc: 0.9127 - val_precision_12: 0.8107 - val_auc_12: 0.9137 - val_recall_12: 0.6082
Epoch 16/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2186 - acc: 0.9208 - precision_12: 0.8190 - auc_12: 0.9258 - recall_12: 0.6616 - val_loss: 0.2226 - val_acc: 0.9206 - val_precision_12: 0.7792 - val_auc_12: 0.9162 - val_recall_12: 0.6822
Epoch 17/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2096 - acc: 0.9224 - precision_12: 0.8095 - auc_12: 0.9336 - recall_12: 0.6869 - val_loss: 0.2328 - val_acc: 0.9215 - val_precision_12: 0.8201 - val_auc_12: 0.9181 - val_recall_12: 0.6416
Epoch 18/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2140 - acc: 0.9217 - precision_12: 0.8052 - auc_12: 0.9296 - recall_12: 0.6871 - val_loss: 0.2494 - val_acc: 0.9068 - val_precision_12: 0.7146 - val_auc_12: 0.9078 - val_recall_12: 0.7141
Epoch 19/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2082 - acc: 0.9224 - precision_12: 0.8032 - auc_12: 0.9344 - recall_12: 0.6962 - val_loss: 0.1739 - val_acc: 0.9367 - val_precision_12: 0.8267 - val_auc_12: 0.9541 - val_recall_12: 0.7061
Epoch 20/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2200 - acc: 0.9136 - precision_12: 0.7762 - auc_12: 0.9335 - recall_12: 0.6626 - val_loss: 0.2554 - val_acc: 0.9125 - val_precision_12: 0.7364 - val_auc_12: 0.9097 - val_recall_12: 0.6779
Epoch 21/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2331 - acc: 0.9117 - precision_12: 0.7649 - auc_12: 0.9196 - recall_12: 0.6638 - val_loss: 0.2098 - val_acc: 0.9268 - val_precision_12: 0.8185 - val_auc_12: 0.9206 - val_recall_12: 0.6402
Epoch 22/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2186 - acc: 0.9168 - precision_12: 0.8045 - auc_12: 0.9288 - recall_12: 0.6484 - val_loss: 0.2036 - val_acc: 0.9239 - val_precision_12: 0.8012 - val_auc_12: 0.9339 - val_recall_12: 0.6532
Epoch 23/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2063 - acc: 0.9211 - precision_12: 0.8038 - auc_12: 0.9383 - recall_12: 0.6846 - val_loss: 0.2123 - val_acc: 0.9213 - val_precision_12: 0.7856 - val_auc_12: 0.9337 - val_recall_12: 0.7153
Epoch 24/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1979 - acc: 0.9255 - precision_12: 0.8113 - auc_12: 0.9438 - recall_12: 0.7095 - val_loss: 0.1838 - val_acc: 0.9296 - val_precision_12: 0.8087 - val_auc_12: 0.9514 - val_recall_12: 0.6775
Epoch 25/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1943 - acc: 0.9273 - precision_12: 0.8233 - auc_12: 0.9446 - recall_12: 0.7071 - val_loss: 0.1842 - val_acc: 0.9293 - val_precision_12: 0.7980 - val_auc_12: 0.9532 - val_recall_12: 0.7378
Epoch 26/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1785 - acc: 0.9314 - precision_12: 0.8208 - auc_12: 0.9567 - recall_12: 0.7422 - val_loss: 0.1468 - val_acc: 0.9422 - val_precision_12: 0.8386 - val_auc_12: 0.9739 - val_recall_12: 0.7793
Epoch 27/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1796 - acc: 0.9313 - precision_12: 0.8191 - auc_12: 0.9556 - recall_12: 0.7442 - val_loss: 0.1556 - val_acc: 0.9399 - val_precision_12: 0.8522 - val_auc_12: 0.9689 - val_recall_12: 0.7337
Epoch 28/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1702 - acc: 0.9350 - precision_12: 0.8379 - auc_12: 0.9612 - recall_12: 0.7469 - val_loss: 0.1422 - val_acc: 0.9429 - val_precision_12: 0.7855 - val_auc_12: 0.9760 - val_recall_12: 0.8406
Epoch 29/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1674 - acc: 0.9368 - precision_12: 0.8288 - auc_12: 0.9616 - recall_12: 0.7734 - val_loss: 0.1348 - val_acc: 0.9485 - val_precision_12: 0.8610 - val_auc_12: 0.9787 - val_recall_12: 0.8291
Epoch 30/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1503 - acc: 0.9426 - precision_12: 0.8507 - auc_12: 0.9702 - recall_12: 0.7868 - val_loss: 0.1559 - val_acc: 0.9438 - val_precision_12: 0.7799 - val_auc_12: 0.9774 - val_recall_12: 0.8906
Epoch 31/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1515 - acc: 0.9419 - precision_12: 0.8346 - auc_12: 0.9696 - recall_12: 0.8040 - val_loss: 0.1443 - val_acc: 0.9399 - val_precision_12: 0.8853 - val_auc_12: 0.9777 - val_recall_12: 0.7265
Epoch 32/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1525 - acc: 0.9404 - precision_12: 0.8469 - auc_12: 0.9701 - recall_12: 0.7757 - val_loss: 0.1558 - val_acc: 0.9371 - val_precision_12: 0.7970 - val_auc_12: 0.9686 - val_recall_12: 0.7866
Epoch 33/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1281 - acc: 0.9500 - precision_12: 0.8589 - auc_12: 0.9795 - recall_12: 0.8306 - val_loss: 0.1037 - val_acc: 0.9594 - val_precision_12: 0.8664 - val_auc_12: 0.9876 - val_recall_12: 0.8773
Epoch 34/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1132 - acc: 0.9555 - precision_12: 0.8737 - auc_12: 0.9840 - recall_12: 0.8508 - val_loss: 0.1245 - val_acc: 0.9503 - val_precision_12: 0.8174 - val_auc_12: 0.9817 - val_recall_12: 0.8817
Epoch 35/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1250 - acc: 0.9515 - precision_12: 0.8646 - auc_12: 0.9799 - recall_12: 0.8342 - val_loss: 0.1060 - val_acc: 0.9589 - val_precision_12: 0.8824 - val_auc_12: 0.9870 - val_recall_12: 0.8852
Epoch 36/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1094 - acc: 0.9569 - precision_12: 0.8771 - auc_12: 0.9846 - recall_12: 0.8566 - val_loss: 0.0919 - val_acc: 0.9633 - val_precision_12: 0.8670 - val_auc_12: 0.9896 - val_recall_12: 0.9074
Epoch 37/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1087 - acc: 0.9576 - precision_12: 0.8774 - auc_12: 0.9850 - recall_12: 0.8611 - val_loss: 0.1445 - val_acc: 0.9462 - val_precision_12: 0.8375 - val_auc_12: 0.9706 - val_recall_12: 0.7777
Epoch 38/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1172 - acc: 0.9534 - precision_12: 0.8680 - auc_12: 0.9828 - recall_12: 0.8432 - val_loss: 0.1076 - val_acc: 0.9572 - val_precision_12: 0.8935 - val_auc_12: 0.9864 - val_recall_12: 0.8367
Epoch 39/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0978 - acc: 0.9607 - precision_12: 0.8846 - auc_12: 0.9882 - recall_12: 0.8733 - val_loss: 0.1089 - val_acc: 0.9583 - val_precision_12: 0.9219 - val_auc_12: 0.9864 - val_recall_12: 0.8092
Epoch 40/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1039 - acc: 0.9593 - precision_12: 0.8806 - auc_12: 0.9860 - recall_12: 0.8686 - val_loss: 0.1030 - val_acc: 0.9607 - val_precision_12: 0.8827 - val_auc_12: 0.9852 - val_recall_12: 0.8616
Epoch 41/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1015 - acc: 0.9604 - precision_12: 0.8822 - auc_12: 0.9868 - recall_12: 0.8748 - val_loss: 0.0913 - val_acc: 0.9642 - val_precision_12: 0.8854 - val_auc_12: 0.9893 - val_recall_12: 0.8872
Epoch 42/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0921 - acc: 0.9635 - precision_12: 0.8933 - auc_12: 0.9895 - recall_12: 0.8821 - val_loss: 0.0848 - val_acc: 0.9679 - val_precision_12: 0.9110 - val_auc_12: 0.9899 - val_recall_12: 0.8796
Epoch 43/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0886 - acc: 0.9656 - precision_12: 0.9031 - auc_12: 0.9900 - recall_12: 0.8847 - val_loss: 0.0867 - val_acc: 0.9662 - val_precision_12: 0.8827 - val_auc_12: 0.9912 - val_recall_12: 0.9120
Epoch 44/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0819 - acc: 0.9678 - precision_12: 0.9054 - auc_12: 0.9917 - recall_12: 0.8967 - val_loss: 0.0775 - val_acc: 0.9691 - val_precision_12: 0.9012 - val_auc_12: 0.9916 - val_recall_12: 0.8928
Epoch 45/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0811 - acc: 0.9678 - precision_12: 0.9016 - auc_12: 0.9919 - recall_12: 0.9014 - val_loss: 0.0859 - val_acc: 0.9667 - val_precision_12: 0.8885 - val_auc_12: 0.9902 - val_recall_12: 0.8949
Epoch 46/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0821 - acc: 0.9678 - precision_12: 0.9090 - auc_12: 0.9914 - recall_12: 0.8924 - val_loss: 0.0901 - val_acc: 0.9652 - val_precision_12: 0.8490 - val_auc_12: 0.9914 - val_recall_12: 0.9305
Epoch 47/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0864 - acc: 0.9662 - precision_12: 0.8974 - auc_12: 0.9905 - recall_12: 0.8955 - val_loss: 0.0859 - val_acc: 0.9663 - val_precision_12: 0.8983 - val_auc_12: 0.9906 - val_recall_12: 0.8957
Epoch 48/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0765 - acc: 0.9695 - precision_12: 0.9077 - auc_12: 0.9927 - recall_12: 0.9056 - val_loss: 0.0802 - val_acc: 0.9689 - val_precision_12: 0.9022 - val_auc_12: 0.9911 - val_recall_12: 0.8979
Epoch 49/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0778 - acc: 0.9693 - precision_12: 0.9111 - auc_12: 0.9921 - recall_12: 0.9003 - val_loss: 0.0942 - val_acc: 0.9631 - val_precision_12: 0.8666 - val_auc_12: 0.9880 - val_recall_12: 0.9048
Epoch 50/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0873 - acc: 0.9659 - precision_12: 0.9001 - auc_12: 0.9902 - recall_12: 0.8903 - val_loss: 0.0852 - val_acc: 0.9671 - val_precision_12: 0.9115 - val_auc_12: 0.9909 - val_recall_12: 0.8743
Epoch 51/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0827 - acc: 0.9670 - precision_12: 0.9062 - auc_12: 0.9916 - recall_12: 0.8900 - val_loss: 0.0931 - val_acc: 0.9642 - val_precision_12: 0.9033 - val_auc_12: 0.9891 - val_recall_12: 0.8702
Epoch 52/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0729 - acc: 0.9713 - precision_12: 0.9154 - auc_12: 0.9932 - recall_12: 0.9081 - val_loss: 0.0840 - val_acc: 0.9670 - val_precision_12: 0.8850 - val_auc_12: 0.9922 - val_recall_12: 0.9247
Epoch 53/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0682 - acc: 0.9730 - precision_12: 0.9230 - auc_12: 0.9940 - recall_12: 0.9110 - val_loss: 0.0826 - val_acc: 0.9688 - val_precision_12: 0.8593 - val_auc_12: 0.9910 - val_recall_12: 0.9267
Epoch 54/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0666 - acc: 0.9735 - precision_12: 0.9221 - auc_12: 0.9943 - recall_12: 0.9155 - val_loss: 0.0847 - val_acc: 0.9690 - val_precision_12: 0.9030 - val_auc_12: 0.9916 - val_recall_12: 0.9146
Epoch 55/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0700 - acc: 0.9721 - precision_12: 0.9183 - auc_12: 0.9937 - recall_12: 0.9102 - val_loss: 0.0843 - val_acc: 0.9680 - val_precision_12: 0.8973 - val_auc_12: 0.9898 - val_recall_12: 0.8832
Epoch 56/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0655 - acc: 0.9740 - precision_12: 0.9261 - auc_12: 0.9945 - recall_12: 0.9139 - val_loss: 0.0816 - val_acc: 0.9698 - val_precision_12: 0.9130 - val_auc_12: 0.9906 - val_recall_12: 0.8978
Epoch 57/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0626 - acc: 0.9750 - precision_12: 0.9264 - auc_12: 0.9948 - recall_12: 0.9202 - val_loss: 0.0926 - val_acc: 0.9691 - val_precision_12: 0.8860 - val_auc_12: 0.9916 - val_recall_12: 0.9220
Epoch 58/100
32/33 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9757 - precision_12: 0.9308 - auc_12: 0.9953 - recall_12: 0.9194Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 85ms/step - loss: 0.0601 - acc: 0.9756 - precision_12: 0.9308 - auc_12: 0.9953 - recall_12: 0.9194 - val_loss: 0.0827 - val_acc: 0.9694 - val_precision_12: 0.9152 - val_auc_12: 0.9907 - val_recall_12: 0.9007
Epoch 00058: early stopping
6/6 [==============================] - 0s 25ms/step - loss: 0.0823 - acc: 0.9677 - precision_12: 0.8844 - auc_12: 0.9920 - recall_12: 0.9213
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbe0005c550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8145506615571116
test_sensitivity 0.9312281713206665
test_specifitivity 0.9742429541546442
test_accuracy 0.9672604031032986
test_precision 0.8750969210988644
test_jaccard_score 0.8145506615571116
test_dicecoef 0.9022904116021344
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-110056.h5
[0. 0. 0. 0. 0.] [0.9672604  0.81455066 0.87509692 0.93122817 0.97424295]

-------------------------
Rep: 1
-------------------------

2021-09-25 11:00:57.100271: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:00:57.100325: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied gaussian blur on all input images
Normalized per channel
Applied normalization
Data augumentation off
Prep done
Training samples: 258, channel mean: -4.652824206883529e-18,
Validation samples: 63, channel mean: -0.0012084537275524774
Model built.
Model: "functional_27"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_14 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_247 (Conv2D)             (None, 256, 256, 32) 320         input_14[0][0]
__________________________________________________________________________________________________
conv2d_248 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_247[0][0]
__________________________________________________________________________________________________
max_pooling2d_52 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_248[0][0]
__________________________________________________________________________________________________
conv2d_249 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_52[0][0]
__________________________________________________________________________________________________
conv2d_250 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_249[0][0]
__________________________________________________________________________________________________
max_pooling2d_53 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_250[0][0]
__________________________________________________________________________________________________
conv2d_251 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_53[0][0]
__________________________________________________________________________________________________
conv2d_252 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_251[0][0]
__________________________________________________________________________________________________
max_pooling2d_54 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_252[0][0]
__________________________________________________________________________________________________
conv2d_253 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_54[0][0]
__________________________________________________________________________________________________
conv2d_254 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_253[0][0]
__________________________________________________________________________________________________
max_pooling2d_55 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_254[0][0]
__________________________________________________________________________________________________
conv2d_255 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_55[0][0]
__________________________________________________________________________________________________
conv2d_256 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_255[0][0]
__________________________________________________________________________________________________
up_sampling2d_52 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_256[0][0]
__________________________________________________________________________________________________
concatenate_52 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_52[0][0]
                                                                 conv2d_254[0][0]
__________________________________________________________________________________________________
conv2d_257 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_52[0][0]
__________________________________________________________________________________________________
conv2d_258 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_257[0][0]
__________________________________________________________________________________________________
up_sampling2d_53 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_258[0][0]
__________________________________________________________________________________________________
concatenate_53 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_53[0][0]
                                                                 conv2d_252[0][0]
__________________________________________________________________________________________________
conv2d_259 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_53[0][0]
__________________________________________________________________________________________________
conv2d_260 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_259[0][0]
__________________________________________________________________________________________________
up_sampling2d_54 (UpSampling2D) (None, 128, 128, 128 0           conv2d_260[0][0]
__________________________________________________________________________________________________
concatenate_54 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_54[0][0]
                                                                 conv2d_250[0][0]
__________________________________________________________________________________________________
conv2d_261 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_54[0][0]
__________________________________________________________________________________________________
conv2d_262 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_261[0][0]
__________________________________________________________________________________________________
up_sampling2d_55 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_262[0][0]
__________________________________________________________________________________________________
concatenate_55 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_55[0][0]
                                                                 conv2d_248[0][0]
__________________________________________________________________________________________________
conv2d_263 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_55[0][0]
__________________________________________________________________________________________________
conv2d_264 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_263[0][0]
__________________________________________________________________________________________________
conv2d_265 (Conv2D)             (None, 256, 256, 1)  33          conv2d_264[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6944 - acc: 0.1578 - precision_13: 0.1555 - auc_13: 0.8083 - recall_13: 0.99712021-09-25 11:01:02.440852: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:01:02.440907: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 11:01:02.744446: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 11:01:02.756090: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-110057/train/plugins/profile/2021_09_25_11_01_02
2021-09-25 11:01:02.759836: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-110057/train/plugins/profile/2021_09_25_11_01_02/xeon-e-v100.trace.json.gz
2021-09-25 11:01:02.782252: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-110057/train/plugins/profile/2021_09_25_11_01_02
2021-09-25 11:01:02.789364: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-110057/train/plugins/profile/2021_09_25_11_01_02/xeon-e-v100.memory_profile.json.gz
2021-09-25 11:01:02.806779: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-110057/train/plugins/profile/2021_09_25_11_01_02Dumped tool data for xplane.pb to logs/fit/20210925-110057/train/plugins/profile/2021_09_25_11_01_02/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-110057/train/plugins/profile/2021_09_25_11_01_02/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-110057/train/plugins/profile/2021_09_25_11_01_02/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-110057/train/plugins/profile/2021_09_25_11_01_02/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-110057/train/plugins/profile/2021_09_25_11_01_02/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6927 - acc: 0.4232 - precision_13: 0.2363 - auc_13: 0.7935 - recall_13: 0.9564WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0652s vs `on_train_batch_end` time: 0.3023s). Check your callbacks.
33/33 [==============================] - 4s 106ms/step - loss: 0.6921 - acc: 0.8574 - precision_13: 0.5592 - auc_13: 0.7536 - recall_13: 0.6025 - val_loss: 0.4487 - val_acc: 0.9005 - val_precision_13: 0.7185 - val_auc_13: 0.8864 - val_recall_13: 0.6047
Epoch 2/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2795 - acc: 0.8993 - precision_13: 0.7602 - auc_13: 0.8801 - recall_13: 0.5614 - val_loss: 0.2397 - val_acc: 0.9090 - val_precision_13: 0.7363 - val_auc_13: 0.9089 - val_recall_13: 0.6213
Epoch 3/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2438 - acc: 0.9046 - precision_13: 0.7703 - auc_13: 0.9137 - recall_13: 0.5937 - val_loss: 0.2475 - val_acc: 0.9113 - val_precision_13: 0.8092 - val_auc_13: 0.9092 - val_recall_13: 0.5567
Epoch 4/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2476 - acc: 0.9041 - precision_13: 0.7738 - auc_13: 0.9082 - recall_13: 0.5842 - val_loss: 0.2746 - val_acc: 0.8925 - val_precision_13: 0.7088 - val_auc_13: 0.8967 - val_recall_13: 0.6167
Epoch 5/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2383 - acc: 0.9063 - precision_13: 0.7712 - auc_13: 0.9173 - recall_13: 0.6065 - val_loss: 0.2361 - val_acc: 0.9054 - val_precision_13: 0.6816 - val_auc_13: 0.9290 - val_recall_13: 0.7274
Epoch 6/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2303 - acc: 0.9104 - precision_13: 0.7937 - auc_13: 0.9221 - recall_13: 0.6104 - val_loss: 0.2228 - val_acc: 0.9178 - val_precision_13: 0.8135 - val_auc_13: 0.9177 - val_recall_13: 0.6220
Epoch 7/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2288 - acc: 0.9101 - precision_13: 0.7852 - auc_13: 0.9247 - recall_13: 0.6194 - val_loss: 0.2358 - val_acc: 0.9084 - val_precision_13: 0.6847 - val_auc_13: 0.9248 - val_recall_13: 0.7223
Epoch 8/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2479 - acc: 0.9045 - precision_13: 0.7370 - auc_13: 0.9209 - recall_13: 0.6463 - val_loss: 0.2747 - val_acc: 0.8957 - val_precision_13: 0.7019 - val_auc_13: 0.8937 - val_recall_13: 0.6242
Epoch 9/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2406 - acc: 0.9090 - precision_13: 0.7935 - auc_13: 0.9126 - recall_13: 0.5992 - val_loss: 0.2447 - val_acc: 0.9070 - val_precision_13: 0.7494 - val_auc_13: 0.9084 - val_recall_13: 0.6146
Epoch 10/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2374 - acc: 0.9102 - precision_13: 0.7934 - auc_13: 0.9174 - recall_13: 0.6095 - val_loss: 0.2404 - val_acc: 0.9111 - val_precision_13: 0.8421 - val_auc_13: 0.9070 - val_recall_13: 0.5275
Epoch 11/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2342 - acc: 0.9112 - precision_13: 0.8088 - auc_13: 0.9184 - recall_13: 0.5985 - val_loss: 0.2527 - val_acc: 0.9027 - val_precision_13: 0.7356 - val_auc_13: 0.9052 - val_recall_13: 0.6100
Epoch 12/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2304 - acc: 0.9130 - precision_13: 0.8083 - auc_13: 0.9207 - recall_13: 0.6131 - val_loss: 0.2118 - val_acc: 0.9243 - val_precision_13: 0.8693 - val_auc_13: 0.9193 - val_recall_13: 0.5604
Epoch 13/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2320 - acc: 0.9145 - precision_13: 0.8126 - auc_13: 0.9189 - recall_13: 0.6200 - val_loss: 0.2481 - val_acc: 0.9087 - val_precision_13: 0.7436 - val_auc_13: 0.9029 - val_recall_13: 0.6107
Epoch 14/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2299 - acc: 0.9151 - precision_13: 0.7952 - auc_13: 0.9205 - recall_13: 0.6472 - val_loss: 0.2309 - val_acc: 0.9155 - val_precision_13: 0.8194 - val_auc_13: 0.9152 - val_recall_13: 0.6047
Epoch 15/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2182 - acc: 0.9195 - precision_13: 0.8248 - auc_13: 0.9274 - recall_13: 0.6443 - val_loss: 0.2272 - val_acc: 0.9204 - val_precision_13: 0.7863 - val_auc_13: 0.9220 - val_recall_13: 0.7051
Epoch 16/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2104 - acc: 0.9223 - precision_13: 0.8059 - auc_13: 0.9340 - recall_13: 0.6915 - val_loss: 0.2216 - val_acc: 0.9240 - val_precision_13: 0.8888 - val_auc_13: 0.9219 - val_recall_13: 0.5835
Epoch 17/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2095 - acc: 0.9214 - precision_13: 0.8120 - auc_13: 0.9364 - recall_13: 0.6754 - val_loss: 0.2012 - val_acc: 0.9276 - val_precision_13: 0.8332 - val_auc_13: 0.9355 - val_recall_13: 0.6753
Epoch 18/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2037 - acc: 0.9242 - precision_13: 0.8131 - auc_13: 0.9384 - recall_13: 0.6961 - val_loss: 0.2155 - val_acc: 0.9229 - val_precision_13: 0.8188 - val_auc_13: 0.9268 - val_recall_13: 0.6772
Epoch 19/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1859 - acc: 0.9300 - precision_13: 0.8254 - auc_13: 0.9501 - recall_13: 0.7251 - val_loss: 0.1612 - val_acc: 0.9334 - val_precision_13: 0.9098 - val_auc_13: 0.9684 - val_recall_13: 0.5939
Epoch 20/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2000 - acc: 0.9235 - precision_13: 0.8159 - auc_13: 0.9433 - recall_13: 0.6870 - val_loss: 0.2244 - val_acc: 0.9147 - val_precision_13: 0.7387 - val_auc_13: 0.9255 - val_recall_13: 0.6962
Epoch 21/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2167 - acc: 0.9172 - precision_13: 0.7945 - auc_13: 0.9308 - recall_13: 0.6657 - val_loss: 0.1863 - val_acc: 0.9343 - val_precision_13: 0.8625 - val_auc_13: 0.9366 - val_recall_13: 0.6538
Epoch 22/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1936 - acc: 0.9257 - precision_13: 0.8222 - auc_13: 0.9467 - recall_13: 0.6963 - val_loss: 0.1680 - val_acc: 0.9369 - val_precision_13: 0.8315 - val_auc_13: 0.9605 - val_recall_13: 0.7250
Epoch 23/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1921 - acc: 0.9258 - precision_13: 0.8068 - auc_13: 0.9506 - recall_13: 0.7184 - val_loss: 0.2451 - val_acc: 0.9141 - val_precision_13: 0.8700 - val_auc_13: 0.9190 - val_recall_13: 0.5596
Epoch 24/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2318 - acc: 0.9117 - precision_13: 0.7794 - auc_13: 0.9217 - recall_13: 0.6416 - val_loss: 0.2126 - val_acc: 0.9215 - val_precision_13: 0.7706 - val_auc_13: 0.9211 - val_recall_13: 0.6579
Epoch 25/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1983 - acc: 0.9264 - precision_13: 0.8283 - auc_13: 0.9412 - recall_13: 0.6934 - val_loss: 0.1857 - val_acc: 0.9309 - val_precision_13: 0.7457 - val_auc_13: 0.9609 - val_recall_13: 0.8510
Epoch 26/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1733 - acc: 0.9345 - precision_13: 0.8379 - auc_13: 0.9574 - recall_13: 0.7428 - val_loss: 0.1301 - val_acc: 0.9507 - val_precision_13: 0.8548 - val_auc_13: 0.9785 - val_recall_13: 0.8232
Epoch 27/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1599 - acc: 0.9398 - precision_13: 0.8411 - auc_13: 0.9626 - recall_13: 0.7791 - val_loss: 0.1237 - val_acc: 0.9546 - val_precision_13: 0.9038 - val_auc_13: 0.9794 - val_recall_13: 0.7866
Epoch 28/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1385 - acc: 0.9473 - precision_13: 0.8664 - auc_13: 0.9746 - recall_13: 0.8014 - val_loss: 0.1564 - val_acc: 0.9356 - val_precision_13: 0.7177 - val_auc_13: 0.9818 - val_recall_13: 0.9251
Epoch 29/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1504 - acc: 0.9435 - precision_13: 0.8504 - auc_13: 0.9690 - recall_13: 0.7942 - val_loss: 0.1297 - val_acc: 0.9514 - val_precision_13: 0.8760 - val_auc_13: 0.9797 - val_recall_13: 0.8302
Epoch 30/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1308 - acc: 0.9505 - precision_13: 0.8738 - auc_13: 0.9770 - recall_13: 0.8151 - val_loss: 0.1138 - val_acc: 0.9564 - val_precision_13: 0.8384 - val_auc_13: 0.9856 - val_recall_13: 0.8921
Epoch 31/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1202 - acc: 0.9537 - precision_13: 0.8643 - auc_13: 0.9815 - recall_13: 0.8504 - val_loss: 0.1228 - val_acc: 0.9496 - val_precision_13: 0.9198 - val_auc_13: 0.9853 - val_recall_13: 0.7577
Epoch 32/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1221 - acc: 0.9521 - precision_13: 0.8652 - auc_13: 0.9809 - recall_13: 0.8373 - val_loss: 0.1285 - val_acc: 0.9529 - val_precision_13: 0.8585 - val_auc_13: 0.9749 - val_recall_13: 0.8264
Epoch 33/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1171 - acc: 0.9557 - precision_13: 0.8835 - auc_13: 0.9815 - recall_13: 0.8397 - val_loss: 0.0994 - val_acc: 0.9605 - val_precision_13: 0.9144 - val_auc_13: 0.9876 - val_recall_13: 0.8261
Epoch 34/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1014 - acc: 0.9600 - precision_13: 0.8882 - auc_13: 0.9874 - recall_13: 0.8641 - val_loss: 0.1058 - val_acc: 0.9589 - val_precision_13: 0.8469 - val_auc_13: 0.9885 - val_recall_13: 0.9022
Epoch 35/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0997 - acc: 0.9604 - precision_13: 0.8865 - auc_13: 0.9877 - recall_13: 0.8689 - val_loss: 0.1002 - val_acc: 0.9601 - val_precision_13: 0.8606 - val_auc_13: 0.9898 - val_recall_13: 0.9234
Epoch 36/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0909 - acc: 0.9641 - precision_13: 0.8958 - auc_13: 0.9899 - recall_13: 0.8828 - val_loss: 0.0861 - val_acc: 0.9660 - val_precision_13: 0.8880 - val_auc_13: 0.9899 - val_recall_13: 0.8982
Epoch 37/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0812 - acc: 0.9677 - precision_13: 0.9038 - auc_13: 0.9918 - recall_13: 0.8982 - val_loss: 0.0850 - val_acc: 0.9666 - val_precision_13: 0.8742 - val_auc_13: 0.9899 - val_recall_13: 0.8981
Epoch 38/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0783 - acc: 0.9688 - precision_13: 0.9039 - auc_13: 0.9924 - recall_13: 0.9055 - val_loss: 0.0840 - val_acc: 0.9664 - val_precision_13: 0.8945 - val_auc_13: 0.9905 - val_recall_13: 0.8995
Epoch 39/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0759 - acc: 0.9699 - precision_13: 0.9103 - auc_13: 0.9928 - recall_13: 0.9053 - val_loss: 0.0892 - val_acc: 0.9666 - val_precision_13: 0.9176 - val_auc_13: 0.9888 - val_recall_13: 0.8707
Epoch 40/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0743 - acc: 0.9704 - precision_13: 0.9114 - auc_13: 0.9930 - recall_13: 0.9075 - val_loss: 0.0829 - val_acc: 0.9677 - val_precision_13: 0.8841 - val_auc_13: 0.9907 - val_recall_13: 0.9115
Epoch 41/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0753 - acc: 0.9703 - precision_13: 0.9104 - auc_13: 0.9929 - recall_13: 0.9074 - val_loss: 0.0791 - val_acc: 0.9687 - val_precision_13: 0.9050 - val_auc_13: 0.9912 - val_recall_13: 0.8952
Epoch 42/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0698 - acc: 0.9723 - precision_13: 0.9187 - auc_13: 0.9938 - recall_13: 0.9109 - val_loss: 0.0812 - val_acc: 0.9702 - val_precision_13: 0.9144 - val_auc_13: 0.9896 - val_recall_13: 0.8919
Epoch 43/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0651 - acc: 0.9744 - precision_13: 0.9263 - auc_13: 0.9945 - recall_13: 0.9160 - val_loss: 0.0835 - val_acc: 0.9686 - val_precision_13: 0.9065 - val_auc_13: 0.9904 - val_recall_13: 0.8985
Epoch 44/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0595 - acc: 0.9760 - precision_13: 0.9325 - auc_13: 0.9954 - recall_13: 0.9201 - val_loss: 0.0801 - val_acc: 0.9694 - val_precision_13: 0.8815 - val_auc_13: 0.9919 - val_recall_13: 0.9209
Epoch 45/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0613 - acc: 0.9754 - precision_13: 0.9291 - auc_13: 0.9951 - recall_13: 0.9200 - val_loss: 0.0940 - val_acc: 0.9647 - val_precision_13: 0.8688 - val_auc_13: 0.9896 - val_recall_13: 0.9067
Epoch 46/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0682 - acc: 0.9730 - precision_13: 0.9246 - auc_13: 0.9940 - recall_13: 0.9092 - val_loss: 0.0842 - val_acc: 0.9678 - val_precision_13: 0.8693 - val_auc_13: 0.9919 - val_recall_13: 0.9215
Epoch 47/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0623 - acc: 0.9749 - precision_13: 0.9251 - auc_13: 0.9949 - recall_13: 0.9212 - val_loss: 0.0874 - val_acc: 0.9673 - val_precision_13: 0.9029 - val_auc_13: 0.9892 - val_recall_13: 0.8968
Epoch 48/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0569 - acc: 0.9769 - precision_13: 0.9321 - auc_13: 0.9959 - recall_13: 0.9261 - val_loss: 0.0810 - val_acc: 0.9695 - val_precision_13: 0.8993 - val_auc_13: 0.9906 - val_recall_13: 0.9061
Epoch 49/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0539 - acc: 0.9780 - precision_13: 0.9397 - auc_13: 0.9963 - recall_13: 0.9248 - val_loss: 0.0830 - val_acc: 0.9693 - val_precision_13: 0.8829 - val_auc_13: 0.9909 - val_recall_13: 0.9280
Epoch 50/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0518 - acc: 0.9788 - precision_13: 0.9397 - auc_13: 0.9965 - recall_13: 0.9297 - val_loss: 0.0892 - val_acc: 0.9703 - val_precision_13: 0.8966 - val_auc_13: 0.9903 - val_recall_13: 0.9149
Epoch 51/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0488 - acc: 0.9800 - precision_13: 0.9437 - auc_13: 0.9969 - recall_13: 0.9334 - val_loss: 0.1049 - val_acc: 0.9672 - val_precision_13: 0.8889 - val_auc_13: 0.9879 - val_recall_13: 0.9093
Epoch 52/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0464 - acc: 0.9809 - precision_13: 0.9470 - auc_13: 0.9971 - recall_13: 0.9352 - val_loss: 0.0813 - val_acc: 0.9714 - val_precision_13: 0.9137 - val_auc_13: 0.9910 - val_recall_13: 0.9175
Epoch 53/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0435 - acc: 0.9819 - precision_13: 0.9497 - auc_13: 0.9975 - recall_13: 0.9393 - val_loss: 0.0950 - val_acc: 0.9692 - val_precision_13: 0.8607 - val_auc_13: 0.9888 - val_recall_13: 0.9286
Epoch 54/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0424 - acc: 0.9822 - precision_13: 0.9480 - auc_13: 0.9977 - recall_13: 0.9427 - val_loss: 0.0884 - val_acc: 0.9711 - val_precision_13: 0.9154 - val_auc_13: 0.9890 - val_recall_13: 0.9129
Epoch 55/100
32/33 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9828 - precision_13: 0.9514 - auc_13: 0.9978 - recall_13: 0.9428Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 85ms/step - loss: 0.0411 - acc: 0.9828 - precision_13: 0.9514 - auc_13: 0.9978 - recall_13: 0.9431 - val_loss: 0.1025 - val_acc: 0.9680 - val_precision_13: 0.8830 - val_auc_13: 0.9855 - val_recall_13: 0.9016
Epoch 00055: early stopping
6/6 [==============================] - 0s 25ms/step - loss: 0.0762 - acc: 0.9698 - precision_13: 0.8944 - auc_13: 0.9931 - recall_13: 0.9228
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb612407a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.820887724798279
test_sensitivity 0.9314663023679417
test_specifitivity 0.9766223336215467
test_accuracy 0.9692921956380208
test_precision 0.8853378495089056
test_jaccard_score 0.820887724798279
test_dicecoef 0.9078164778262193
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-110353.h5
[0.9672604  0.81455066 0.87509692 0.93122817 0.97424295] [0.9692922  0.82088772 0.88533785 0.9314663  0.97662233]

-------------------------
Rep: 2
-------------------------

2021-09-25 11:03:54.445445: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:03:54.445504: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied gaussian blur on all input images
Normalized per channel
Applied normalization
Data augumentation off
Prep done
Training samples: 258, channel mean: -4.652824206883529e-18,
Validation samples: 63, channel mean: -0.0012084537275524774
Model built.
Model: "functional_29"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_15 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_266 (Conv2D)             (None, 256, 256, 32) 320         input_15[0][0]
__________________________________________________________________________________________________
conv2d_267 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_266[0][0]
__________________________________________________________________________________________________
max_pooling2d_56 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_267[0][0]
__________________________________________________________________________________________________
conv2d_268 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_56[0][0]
__________________________________________________________________________________________________
conv2d_269 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_268[0][0]
__________________________________________________________________________________________________
max_pooling2d_57 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_269[0][0]
__________________________________________________________________________________________________
conv2d_270 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_57[0][0]
__________________________________________________________________________________________________
conv2d_271 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_270[0][0]
__________________________________________________________________________________________________
max_pooling2d_58 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_271[0][0]
__________________________________________________________________________________________________
conv2d_272 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_58[0][0]
__________________________________________________________________________________________________
conv2d_273 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_272[0][0]
__________________________________________________________________________________________________
max_pooling2d_59 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_273[0][0]
__________________________________________________________________________________________________
conv2d_274 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_59[0][0]
__________________________________________________________________________________________________
conv2d_275 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_274[0][0]
__________________________________________________________________________________________________
up_sampling2d_56 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_275[0][0]
__________________________________________________________________________________________________
concatenate_56 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_56[0][0]
                                                                 conv2d_273[0][0]
__________________________________________________________________________________________________
conv2d_276 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_56[0][0]
__________________________________________________________________________________________________
conv2d_277 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_276[0][0]
__________________________________________________________________________________________________
up_sampling2d_57 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_277[0][0]
__________________________________________________________________________________________________
concatenate_57 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_57[0][0]
                                                                 conv2d_271[0][0]
__________________________________________________________________________________________________
conv2d_278 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_57[0][0]
__________________________________________________________________________________________________
conv2d_279 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_278[0][0]
__________________________________________________________________________________________________
up_sampling2d_58 (UpSampling2D) (None, 128, 128, 128 0           conv2d_279[0][0]
__________________________________________________________________________________________________
concatenate_58 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_58[0][0]
                                                                 conv2d_269[0][0]
__________________________________________________________________________________________________
conv2d_280 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_58[0][0]
__________________________________________________________________________________________________
conv2d_281 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_280[0][0]
__________________________________________________________________________________________________
up_sampling2d_59 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_281[0][0]
__________________________________________________________________________________________________
concatenate_59 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_59[0][0]
                                                                 conv2d_267[0][0]
__________________________________________________________________________________________________
conv2d_282 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_59[0][0]
__________________________________________________________________________________________________
conv2d_283 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_282[0][0]
__________________________________________________________________________________________________
conv2d_284 (Conv2D)             (None, 256, 256, 1)  33          conv2d_283[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6939 - acc: 0.1365 - precision_14: 0.0691 - auc_14: 0.4937 - recall_14: 0.36542021-09-25 11:03:59.539466: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:03:59.539525: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 11:03:59.823367: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 11:03:59.833388: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-110354/train/plugins/profile/2021_09_25_11_03_59
2021-09-25 11:03:59.837371: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-110354/train/plugins/profile/2021_09_25_11_03_59/xeon-e-v100.trace.json.gz
2021-09-25 11:03:59.856781: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-110354/train/plugins/profile/2021_09_25_11_03_59
2021-09-25 11:03:59.865012: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-110354/train/plugins/profile/2021_09_25_11_03_59/xeon-e-v100.memory_profile.json.gz
2021-09-25 11:03:59.886963: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-110354/train/plugins/profile/2021_09_25_11_03_59Dumped tool data for xplane.pb to logs/fit/20210925-110354/train/plugins/profile/2021_09_25_11_03_59/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-110354/train/plugins/profile/2021_09_25_11_03_59/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-110354/train/plugins/profile/2021_09_25_11_03_59/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-110354/train/plugins/profile/2021_09_25_11_03_59/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-110354/train/plugins/profile/2021_09_25_11_03_59/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6913 - acc: 0.4649 - precision_14: 0.1712 - auc_14: 0.6964 - recall_14: 0.4968WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0671s vs `on_train_batch_end` time: 0.2817s). Check your callbacks.
33/33 [==============================] - 4s 107ms/step - loss: 0.4183 - acc: 0.8626 - precision_14: 0.5942 - auc_14: 0.8049 - recall_14: 0.5039 - val_loss: 0.3027 - val_acc: 0.9006 - val_precision_14: 0.7143 - val_auc_14: 0.8867 - val_recall_14: 0.6137
Epoch 2/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2560 - acc: 0.9030 - precision_14: 0.7433 - auc_14: 0.9047 - recall_14: 0.6216 - val_loss: 0.2378 - val_acc: 0.9112 - val_precision_14: 0.7344 - val_auc_14: 0.9115 - val_recall_14: 0.6480
Epoch 3/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2439 - acc: 0.9071 - precision_14: 0.7655 - auc_14: 0.9130 - recall_14: 0.6221 - val_loss: 0.2623 - val_acc: 0.9070 - val_precision_14: 0.8262 - val_auc_14: 0.9044 - val_recall_14: 0.5039
Epoch 4/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2549 - acc: 0.9087 - precision_14: 0.7857 - auc_14: 0.9014 - recall_14: 0.6073 - val_loss: 0.2935 - val_acc: 0.8863 - val_precision_14: 0.6633 - val_auc_14: 0.8904 - val_recall_14: 0.6639
Epoch 5/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2408 - acc: 0.9093 - precision_14: 0.7690 - auc_14: 0.9144 - recall_14: 0.6367 - val_loss: 0.2173 - val_acc: 0.9177 - val_precision_14: 0.7911 - val_auc_14: 0.9284 - val_recall_14: 0.6355
Epoch 6/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2321 - acc: 0.9130 - precision_14: 0.8038 - auc_14: 0.9186 - recall_14: 0.6186 - val_loss: 0.2326 - val_acc: 0.9165 - val_precision_14: 0.7810 - val_auc_14: 0.9104 - val_recall_14: 0.6546
Epoch 7/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2295 - acc: 0.9129 - precision_14: 0.7957 - auc_14: 0.9218 - recall_14: 0.6285 - val_loss: 0.2211 - val_acc: 0.9163 - val_precision_14: 0.7398 - val_auc_14: 0.9245 - val_recall_14: 0.6821
Epoch 8/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2258 - acc: 0.9148 - precision_14: 0.8058 - auc_14: 0.9235 - recall_14: 0.6313 - val_loss: 0.2440 - val_acc: 0.9131 - val_precision_14: 0.8213 - val_auc_14: 0.9087 - val_recall_14: 0.5958
Epoch 9/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2253 - acc: 0.9190 - precision_14: 0.8088 - auc_14: 0.9224 - recall_14: 0.6609 - val_loss: 0.2822 - val_acc: 0.9021 - val_precision_14: 0.6571 - val_auc_14: 0.9279 - val_recall_14: 0.7905
Epoch 10/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2224 - acc: 0.9191 - precision_14: 0.7836 - auc_14: 0.9246 - recall_14: 0.6982 - val_loss: 0.2194 - val_acc: 0.9193 - val_precision_14: 0.8356 - val_auc_14: 0.9243 - val_recall_14: 0.5987
Epoch 11/100
33/33 [==============================] - 3s 84ms/step - loss: 0.2083 - acc: 0.9242 - precision_14: 0.8193 - auc_14: 0.9333 - recall_14: 0.6878 - val_loss: 0.2252 - val_acc: 0.9143 - val_precision_14: 0.7269 - val_auc_14: 0.9380 - val_recall_14: 0.7419
Epoch 12/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1959 - acc: 0.9285 - precision_14: 0.8133 - auc_14: 0.9406 - recall_14: 0.7305 - val_loss: 0.1895 - val_acc: 0.9338 - val_precision_14: 0.7618 - val_auc_14: 0.9403 - val_recall_14: 0.7887
Epoch 13/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1869 - acc: 0.9326 - precision_14: 0.8248 - auc_14: 0.9467 - recall_14: 0.7465 - val_loss: 0.1854 - val_acc: 0.9277 - val_precision_14: 0.7562 - val_auc_14: 0.9528 - val_recall_14: 0.7740
Epoch 14/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1635 - acc: 0.9388 - precision_14: 0.8298 - auc_14: 0.9601 - recall_14: 0.7873 - val_loss: 0.1784 - val_acc: 0.9364 - val_precision_14: 0.9026 - val_auc_14: 0.9556 - val_recall_14: 0.6751
Epoch 15/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1546 - acc: 0.9414 - precision_14: 0.8473 - auc_14: 0.9660 - recall_14: 0.7824 - val_loss: 0.1347 - val_acc: 0.9488 - val_precision_14: 0.8811 - val_auc_14: 0.9765 - val_recall_14: 0.7943
Epoch 16/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1553 - acc: 0.9406 - precision_14: 0.8390 - auc_14: 0.9678 - recall_14: 0.7876 - val_loss: 0.1329 - val_acc: 0.9504 - val_precision_14: 0.8404 - val_auc_14: 0.9738 - val_recall_14: 0.8404
Epoch 17/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1354 - acc: 0.9487 - precision_14: 0.8614 - auc_14: 0.9747 - recall_14: 0.8176 - val_loss: 0.1290 - val_acc: 0.9503 - val_precision_14: 0.8947 - val_auc_14: 0.9781 - val_recall_14: 0.7752
Epoch 18/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1260 - acc: 0.9512 - precision_14: 0.8663 - auc_14: 0.9795 - recall_14: 0.8296 - val_loss: 0.1124 - val_acc: 0.9548 - val_precision_14: 0.8496 - val_auc_14: 0.9848 - val_recall_14: 0.8785
Epoch 19/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1130 - acc: 0.9553 - precision_14: 0.8754 - auc_14: 0.9841 - recall_14: 0.8470 - val_loss: 0.0939 - val_acc: 0.9639 - val_precision_14: 0.8942 - val_auc_14: 0.9873 - val_recall_14: 0.8483
Epoch 20/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1062 - acc: 0.9581 - precision_14: 0.8869 - auc_14: 0.9859 - recall_14: 0.8521 - val_loss: 0.0904 - val_acc: 0.9648 - val_precision_14: 0.9084 - val_auc_14: 0.9897 - val_recall_14: 0.8592
Epoch 21/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1106 - acc: 0.9557 - precision_14: 0.8726 - auc_14: 0.9846 - recall_14: 0.8539 - val_loss: 0.0814 - val_acc: 0.9681 - val_precision_14: 0.8875 - val_auc_14: 0.9914 - val_recall_14: 0.8943
Epoch 22/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1008 - acc: 0.9602 - precision_14: 0.8898 - auc_14: 0.9875 - recall_14: 0.8634 - val_loss: 0.0918 - val_acc: 0.9641 - val_precision_14: 0.8876 - val_auc_14: 0.9881 - val_recall_14: 0.8698
Epoch 23/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0971 - acc: 0.9613 - precision_14: 0.8888 - auc_14: 0.9883 - recall_14: 0.8726 - val_loss: 0.1007 - val_acc: 0.9622 - val_precision_14: 0.8970 - val_auc_14: 0.9860 - val_recall_14: 0.8692
Epoch 24/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0927 - acc: 0.9633 - precision_14: 0.8942 - auc_14: 0.9893 - recall_14: 0.8797 - val_loss: 0.0842 - val_acc: 0.9669 - val_precision_14: 0.8679 - val_auc_14: 0.9913 - val_recall_14: 0.9119
Epoch 25/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0920 - acc: 0.9632 - precision_14: 0.8941 - auc_14: 0.9896 - recall_14: 0.8791 - val_loss: 0.0910 - val_acc: 0.9644 - val_precision_14: 0.8757 - val_auc_14: 0.9893 - val_recall_14: 0.9015
Epoch 26/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0891 - acc: 0.9648 - precision_14: 0.8921 - auc_14: 0.9901 - recall_14: 0.8923 - val_loss: 0.0894 - val_acc: 0.9668 - val_precision_14: 0.8986 - val_auc_14: 0.9880 - val_recall_14: 0.8871
Epoch 27/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0946 - acc: 0.9623 - precision_14: 0.8876 - auc_14: 0.9888 - recall_14: 0.8812 - val_loss: 0.0737 - val_acc: 0.9713 - val_precision_14: 0.9105 - val_auc_14: 0.9929 - val_recall_14: 0.9007
Epoch 28/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0897 - acc: 0.9642 - precision_14: 0.8970 - auc_14: 0.9900 - recall_14: 0.8821 - val_loss: 0.0824 - val_acc: 0.9683 - val_precision_14: 0.9026 - val_auc_14: 0.9902 - val_recall_14: 0.8790
Epoch 29/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0862 - acc: 0.9654 - precision_14: 0.8945 - auc_14: 0.9907 - recall_14: 0.8937 - val_loss: 0.0923 - val_acc: 0.9647 - val_precision_14: 0.9147 - val_auc_14: 0.9886 - val_recall_14: 0.8722
Epoch 30/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0845 - acc: 0.9665 - precision_14: 0.9035 - auc_14: 0.9910 - recall_14: 0.8903 - val_loss: 0.0830 - val_acc: 0.9676 - val_precision_14: 0.8785 - val_auc_14: 0.9917 - val_recall_14: 0.9195
Epoch 31/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0828 - acc: 0.9674 - precision_14: 0.9000 - auc_14: 0.9914 - recall_14: 0.9006 - val_loss: 0.0833 - val_acc: 0.9674 - val_precision_14: 0.9186 - val_auc_14: 0.9909 - val_recall_14: 0.8785
Epoch 32/100
33/33 [==============================] - 3s 84ms/step - loss: 0.1030 - acc: 0.9582 - precision_14: 0.8708 - auc_14: 0.9870 - recall_14: 0.8742 - val_loss: 0.0976 - val_acc: 0.9604 - val_precision_14: 0.8490 - val_auc_14: 0.9882 - val_recall_14: 0.8996
Epoch 33/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0921 - acc: 0.9637 - precision_14: 0.8938 - auc_14: 0.9891 - recall_14: 0.8828 - val_loss: 0.0850 - val_acc: 0.9677 - val_precision_14: 0.8999 - val_auc_14: 0.9919 - val_recall_14: 0.8941
Epoch 34/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0841 - acc: 0.9667 - precision_14: 0.9034 - auc_14: 0.9911 - recall_14: 0.8919 - val_loss: 0.0826 - val_acc: 0.9672 - val_precision_14: 0.9157 - val_auc_14: 0.9907 - val_recall_14: 0.8721
Epoch 35/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0770 - acc: 0.9690 - precision_14: 0.9085 - auc_14: 0.9926 - recall_14: 0.9015 - val_loss: 0.0851 - val_acc: 0.9674 - val_precision_14: 0.8944 - val_auc_14: 0.9916 - val_recall_14: 0.9243
Epoch 36/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0747 - acc: 0.9703 - precision_14: 0.9135 - auc_14: 0.9930 - recall_14: 0.9038 - val_loss: 0.0756 - val_acc: 0.9701 - val_precision_14: 0.8980 - val_auc_14: 0.9917 - val_recall_14: 0.9151
Epoch 37/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0734 - acc: 0.9705 - precision_14: 0.9112 - auc_14: 0.9932 - recall_14: 0.9079 - val_loss: 0.0830 - val_acc: 0.9681 - val_precision_14: 0.8804 - val_auc_14: 0.9905 - val_recall_14: 0.9015
Epoch 38/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0825 - acc: 0.9670 - precision_14: 0.9023 - auc_14: 0.9914 - recall_14: 0.8952 - val_loss: 0.0816 - val_acc: 0.9673 - val_precision_14: 0.8966 - val_auc_14: 0.9919 - val_recall_14: 0.9036
Epoch 39/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0754 - acc: 0.9699 - precision_14: 0.9096 - auc_14: 0.9927 - recall_14: 0.9059 - val_loss: 0.0842 - val_acc: 0.9683 - val_precision_14: 0.9311 - val_auc_14: 0.9899 - val_recall_14: 0.8672
Epoch 40/100
33/33 [==============================] - 3s 84ms/step - loss: 0.0731 - acc: 0.9712 - precision_14: 0.9129 - auc_14: 0.9930 - recall_14: 0.9107 - val_loss: 0.0776 - val_acc: 0.9703 - val_precision_14: 0.9037 - val_auc_14: 0.9920 - val_recall_14: 0.9055
Epoch 41/100
32/33 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9718 - precision_14: 0.9193 - auc_14: 0.9938 - recall_14: 0.9079Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 86ms/step - loss: 0.0708 - acc: 0.9718 - precision_14: 0.9186 - auc_14: 0.9937 - recall_14: 0.9077 - val_loss: 0.0768 - val_acc: 0.9705 - val_precision_14: 0.8997 - val_auc_14: 0.9918 - val_recall_14: 0.9142
Epoch 00041: early stopping
6/6 [==============================] - 0s 25ms/step - loss: 0.0853 - acc: 0.9665 - precision_14: 0.8934 - auc_14: 0.9911 - recall_14: 0.9013
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb6120fb820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8066389008447514
test_sensitivity 0.9162885814073962
test_specifitivity 0.9757087134349528
test_accuracy 0.9660630967881945
test_precision 0.8796599700799339
test_jaccard_score 0.8066389008447514
test_dicecoef 0.8976007530259159
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-110611.h5
[1.9365526  1.63543839 1.76043477 1.86269447 1.95086529] [0.9660631  0.8066389  0.87965997 0.91628858 0.97570871]

-------------------------
Averaged metrics for Baseline + Gaussian Blur - bacteria: [0.96753857 0.81402576 0.88003158 0.92632769 0.97552467]
-------------------------


-------------------------
RUN: Baseline + Augumentations + Histogram Equalization - bacteria, PARAMS: {'histogram_equalization': True, 'augumentation': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-25 11:06:11.889475: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:06:11.889527: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied histogram equalization on all input images
Applied normalization
Data augumentation on
Prep done
Training samples: 258, channel mean: 0.1526593703632206,
Validation samples: 63, channel mean: 0.14989169631611324
Model built.
Model: "functional_31"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_16 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_285 (Conv2D)             (None, 256, 256, 32) 320         input_16[0][0]
__________________________________________________________________________________________________
conv2d_286 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_285[0][0]
__________________________________________________________________________________________________
max_pooling2d_60 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_286[0][0]
__________________________________________________________________________________________________
conv2d_287 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_60[0][0]
__________________________________________________________________________________________________
conv2d_288 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_287[0][0]
__________________________________________________________________________________________________
max_pooling2d_61 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_288[0][0]
__________________________________________________________________________________________________
conv2d_289 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_61[0][0]
__________________________________________________________________________________________________
conv2d_290 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_289[0][0]
__________________________________________________________________________________________________
max_pooling2d_62 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_290[0][0]
__________________________________________________________________________________________________
conv2d_291 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_62[0][0]
__________________________________________________________________________________________________
conv2d_292 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_291[0][0]
__________________________________________________________________________________________________
max_pooling2d_63 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_292[0][0]
__________________________________________________________________________________________________
conv2d_293 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_63[0][0]
__________________________________________________________________________________________________
conv2d_294 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_293[0][0]
__________________________________________________________________________________________________
up_sampling2d_60 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_294[0][0]
__________________________________________________________________________________________________
concatenate_60 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_60[0][0]
                                                                 conv2d_292[0][0]
__________________________________________________________________________________________________
conv2d_295 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_60[0][0]
__________________________________________________________________________________________________
conv2d_296 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_295[0][0]
__________________________________________________________________________________________________
up_sampling2d_61 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_296[0][0]
__________________________________________________________________________________________________
concatenate_61 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_61[0][0]
                                                                 conv2d_290[0][0]
__________________________________________________________________________________________________
conv2d_297 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_61[0][0]
__________________________________________________________________________________________________
conv2d_298 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_297[0][0]
__________________________________________________________________________________________________
up_sampling2d_62 (UpSampling2D) (None, 128, 128, 128 0           conv2d_298[0][0]
__________________________________________________________________________________________________
concatenate_62 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_62[0][0]
                                                                 conv2d_288[0][0]
__________________________________________________________________________________________________
conv2d_299 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_62[0][0]
__________________________________________________________________________________________________
conv2d_300 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_299[0][0]
__________________________________________________________________________________________________
up_sampling2d_63 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_300[0][0]
__________________________________________________________________________________________________
concatenate_63 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_63[0][0]
                                                                 conv2d_286[0][0]
__________________________________________________________________________________________________
conv2d_301 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_63[0][0]
__________________________________________________________________________________________________
conv2d_302 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_301[0][0]
__________________________________________________________________________________________________
conv2d_303 (Conv2D)             (None, 256, 256, 1)  33          conv2d_302[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6927 - acc: 0.1926 - precision_15: 0.1565 - auc_15: 0.8186 - recall_15: 0.95012021-09-25 11:06:16.772131: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:06:16.772185: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 11:06:17.039120: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 11:06:17.055161: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-110611/train/plugins/profile/2021_09_25_11_06_17
2021-09-25 11:06:17.058845: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-110611/train/plugins/profile/2021_09_25_11_06_17/xeon-e-v100.trace.json.gz
2021-09-25 11:06:17.077859: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-110611/train/plugins/profile/2021_09_25_11_06_17
2021-09-25 11:06:17.085216: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-110611/train/plugins/profile/2021_09_25_11_06_17/xeon-e-v100.memory_profile.json.gz
2021-09-25 11:06:17.106615: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-110611/train/plugins/profile/2021_09_25_11_06_17Dumped tool data for xplane.pb to logs/fit/20210925-110611/train/plugins/profile/2021_09_25_11_06_17/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-110611/train/plugins/profile/2021_09_25_11_06_17/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-110611/train/plugins/profile/2021_09_25_11_06_17/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-110611/train/plugins/profile/2021_09_25_11_06_17/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-110611/train/plugins/profile/2021_09_25_11_06_17/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6907 - acc: 0.4558 - precision_15: 0.2513 - auc_15: 0.8316 - recall_15: 0.9369WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0659s vs `on_train_batch_end` time: 0.2701s). Check your callbacks.
33/33 [==============================] - 4s 107ms/step - loss: 0.6405 - acc: 0.8432 - precision_15: 0.5424 - auc_15: 0.6845 - recall_15: 0.5228 - val_loss: 0.5414 - val_acc: 0.7724 - val_precision_15: 0.1553 - val_auc_15: 0.4690 - val_recall_15: 0.1006
Epoch 2/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2297 - acc: 0.8993 - precision_15: 0.8031 - auc_15: 0.9186 - recall_15: 0.5805 - val_loss: 0.2119 - val_acc: 0.9137 - val_precision_15: 0.7456 - val_auc_15: 0.9365 - val_recall_15: 0.6525
Epoch 3/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2103 - acc: 0.9026 - precision_15: 0.8136 - auc_15: 0.9393 - recall_15: 0.5987 - val_loss: 0.2333 - val_acc: 0.9108 - val_precision_15: 0.7408 - val_auc_15: 0.9338 - val_recall_15: 0.6503
Epoch 4/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2047 - acc: 0.9049 - precision_15: 0.8258 - auc_15: 0.9418 - recall_15: 0.5983 - val_loss: 0.2463 - val_acc: 0.8986 - val_precision_15: 0.7209 - val_auc_15: 0.9254 - val_recall_15: 0.6517
Epoch 5/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1985 - acc: 0.9067 - precision_15: 0.8180 - auc_15: 0.9464 - recall_15: 0.6289 - val_loss: 0.1919 - val_acc: 0.9185 - val_precision_15: 0.7477 - val_auc_15: 0.9540 - val_recall_15: 0.7139
Epoch 6/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1881 - acc: 0.9107 - precision_15: 0.8368 - auc_15: 0.9498 - recall_15: 0.6156 - val_loss: 0.2151 - val_acc: 0.9148 - val_precision_15: 0.7526 - val_auc_15: 0.9349 - val_recall_15: 0.6862
Epoch 7/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1895 - acc: 0.9086 - precision_15: 0.8244 - auc_15: 0.9511 - recall_15: 0.6293 - val_loss: 0.1910 - val_acc: 0.9222 - val_precision_15: 0.7957 - val_auc_15: 0.9483 - val_recall_15: 0.6482
Epoch 8/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1943 - acc: 0.9090 - precision_15: 0.8318 - auc_15: 0.9472 - recall_15: 0.6221 - val_loss: 0.2209 - val_acc: 0.9123 - val_precision_15: 0.8045 - val_auc_15: 0.9326 - val_recall_15: 0.6091
Epoch 9/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1891 - acc: 0.9114 - precision_15: 0.8410 - auc_15: 0.9502 - recall_15: 0.6310 - val_loss: 0.2104 - val_acc: 0.9157 - val_precision_15: 0.7452 - val_auc_15: 0.9434 - val_recall_15: 0.7059
Epoch 10/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1952 - acc: 0.9080 - precision_15: 0.8034 - auc_15: 0.9473 - recall_15: 0.6584 - val_loss: 0.2240 - val_acc: 0.9131 - val_precision_15: 0.8564 - val_auc_15: 0.9362 - val_recall_15: 0.5302
Epoch 11/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1901 - acc: 0.9103 - precision_15: 0.8330 - auc_15: 0.9485 - recall_15: 0.6349 - val_loss: 0.2009 - val_acc: 0.9156 - val_precision_15: 0.7753 - val_auc_15: 0.9491 - val_recall_15: 0.6641
Epoch 12/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1896 - acc: 0.9132 - precision_15: 0.8307 - auc_15: 0.9457 - recall_15: 0.6590 - val_loss: 0.1830 - val_acc: 0.9294 - val_precision_15: 0.8504 - val_auc_15: 0.9497 - val_recall_15: 0.6205
Epoch 13/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1896 - acc: 0.9109 - precision_15: 0.8271 - auc_15: 0.9499 - recall_15: 0.6515 - val_loss: 0.1867 - val_acc: 0.9236 - val_precision_15: 0.7445 - val_auc_15: 0.9539 - val_recall_15: 0.7584
Epoch 14/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1711 - acc: 0.9184 - precision_15: 0.8396 - auc_15: 0.9585 - recall_15: 0.6960 - val_loss: 0.1768 - val_acc: 0.9269 - val_precision_15: 0.8558 - val_auc_15: 0.9613 - val_recall_15: 0.6528
Epoch 15/100
33/33 [==============================] - 3s 88ms/step - loss: 0.1623 - acc: 0.9218 - precision_15: 0.8581 - auc_15: 0.9623 - recall_15: 0.6972 - val_loss: 0.1652 - val_acc: 0.9367 - val_precision_15: 0.8226 - val_auc_15: 0.9644 - val_recall_15: 0.7812
Epoch 16/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1419 - acc: 0.9302 - precision_15: 0.8582 - auc_15: 0.9709 - recall_15: 0.7560 - val_loss: 0.1330 - val_acc: 0.9498 - val_precision_15: 0.8603 - val_auc_15: 0.9755 - val_recall_15: 0.8079
Epoch 17/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1402 - acc: 0.9314 - precision_15: 0.8671 - auc_15: 0.9706 - recall_15: 0.7527 - val_loss: 0.1349 - val_acc: 0.9473 - val_precision_15: 0.8790 - val_auc_15: 0.9767 - val_recall_15: 0.7715
Epoch 18/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1415 - acc: 0.9320 - precision_15: 0.8595 - auc_15: 0.9689 - recall_15: 0.7684 - val_loss: 0.1413 - val_acc: 0.9477 - val_precision_15: 0.8438 - val_auc_15: 0.9731 - val_recall_15: 0.8339
Epoch 19/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1257 - acc: 0.9370 - precision_15: 0.8798 - auc_15: 0.9765 - recall_15: 0.7750 - val_loss: 0.5071 - val_acc: 0.8864 - val_precision_15: 0.9249 - val_auc_15: 0.8551 - val_recall_15: 0.2249
Epoch 20/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1923 - acc: 0.9196 - precision_15: 0.8473 - auc_15: 0.9438 - recall_15: 0.6901 - val_loss: 0.1166 - val_acc: 0.9567 - val_precision_15: 0.8792 - val_auc_15: 0.9801 - val_recall_15: 0.8353
Epoch 21/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1398 - acc: 0.9335 - precision_15: 0.8689 - auc_15: 0.9694 - recall_15: 0.7688 - val_loss: 0.1077 - val_acc: 0.9594 - val_precision_15: 0.9132 - val_auc_15: 0.9844 - val_recall_15: 0.7972
Epoch 22/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1292 - acc: 0.9387 - precision_15: 0.8892 - auc_15: 0.9736 - recall_15: 0.7782 - val_loss: 0.1025 - val_acc: 0.9614 - val_precision_15: 0.8951 - val_auc_15: 0.9846 - val_recall_15: 0.8406
Epoch 23/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1137 - acc: 0.9419 - precision_15: 0.8885 - auc_15: 0.9810 - recall_15: 0.8005 - val_loss: 0.1016 - val_acc: 0.9615 - val_precision_15: 0.9052 - val_auc_15: 0.9858 - val_recall_15: 0.8548
Epoch 24/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0999 - acc: 0.9457 - precision_15: 0.9024 - auc_15: 0.9859 - recall_15: 0.8172 - val_loss: 0.0883 - val_acc: 0.9651 - val_precision_15: 0.8783 - val_auc_15: 0.9897 - val_recall_15: 0.8831
Epoch 25/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0985 - acc: 0.9462 - precision_15: 0.9089 - auc_15: 0.9863 - recall_15: 0.8139 - val_loss: 0.0960 - val_acc: 0.9621 - val_precision_15: 0.8621 - val_auc_15: 0.9888 - val_recall_15: 0.9036
Epoch 26/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0926 - acc: 0.9483 - precision_15: 0.9071 - auc_15: 0.9879 - recall_15: 0.8312 - val_loss: 0.0877 - val_acc: 0.9665 - val_precision_15: 0.8896 - val_auc_15: 0.9892 - val_recall_15: 0.8965
Epoch 27/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1110 - acc: 0.9453 - precision_15: 0.9027 - auc_15: 0.9817 - recall_15: 0.8104 - val_loss: 0.0773 - val_acc: 0.9697 - val_precision_15: 0.9004 - val_auc_15: 0.9924 - val_recall_15: 0.9012
Epoch 28/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1052 - acc: 0.9471 - precision_15: 0.9054 - auc_15: 0.9831 - recall_15: 0.8191 - val_loss: 0.0855 - val_acc: 0.9669 - val_precision_15: 0.9099 - val_auc_15: 0.9900 - val_recall_15: 0.8593
Epoch 29/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1099 - acc: 0.9456 - precision_15: 0.9014 - auc_15: 0.9801 - recall_15: 0.8111 - val_loss: 0.0933 - val_acc: 0.9641 - val_precision_15: 0.8985 - val_auc_15: 0.9888 - val_recall_15: 0.8879
Epoch 30/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0957 - acc: 0.9497 - precision_15: 0.9182 - auc_15: 0.9861 - recall_15: 0.8241 - val_loss: 0.0899 - val_acc: 0.9639 - val_precision_15: 0.8629 - val_auc_15: 0.9902 - val_recall_15: 0.9138
Epoch 31/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0855 - acc: 0.9511 - precision_15: 0.9147 - auc_15: 0.9896 - recall_15: 0.8439 - val_loss: 0.0868 - val_acc: 0.9657 - val_precision_15: 0.9169 - val_auc_15: 0.9902 - val_recall_15: 0.8689
Epoch 32/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1011 - acc: 0.9477 - precision_15: 0.9105 - auc_15: 0.9839 - recall_15: 0.8195 - val_loss: 0.0980 - val_acc: 0.9623 - val_precision_15: 0.9289 - val_auc_15: 0.9880 - val_recall_15: 0.8147
Epoch 33/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0926 - acc: 0.9501 - precision_15: 0.9203 - auc_15: 0.9865 - recall_15: 0.8287 - val_loss: 0.0849 - val_acc: 0.9672 - val_precision_15: 0.9011 - val_auc_15: 0.9914 - val_recall_15: 0.8892
Epoch 34/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1030 - acc: 0.9471 - precision_15: 0.9097 - auc_15: 0.9845 - recall_15: 0.8167 - val_loss: 0.0862 - val_acc: 0.9665 - val_precision_15: 0.8830 - val_auc_15: 0.9909 - val_recall_15: 0.9079
Epoch 35/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0834 - acc: 0.9526 - precision_15: 0.9229 - auc_15: 0.9893 - recall_15: 0.8422 - val_loss: 0.0919 - val_acc: 0.9631 - val_precision_15: 0.8689 - val_auc_15: 0.9918 - val_recall_15: 0.9313
Epoch 36/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0894 - acc: 0.9525 - precision_15: 0.9243 - auc_15: 0.9881 - recall_15: 0.8402 - val_loss: 0.0758 - val_acc: 0.9708 - val_precision_15: 0.9185 - val_auc_15: 0.9920 - val_recall_15: 0.8948
Epoch 37/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0843 - acc: 0.9534 - precision_15: 0.9265 - auc_15: 0.9888 - recall_15: 0.8410 - val_loss: 0.0888 - val_acc: 0.9660 - val_precision_15: 0.8548 - val_auc_15: 0.9918 - val_recall_15: 0.9210
Epoch 38/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1014 - acc: 0.9474 - precision_15: 0.9059 - auc_15: 0.9848 - recall_15: 0.8192 - val_loss: 0.0843 - val_acc: 0.9666 - val_precision_15: 0.8915 - val_auc_15: 0.9908 - val_recall_15: 0.9051
Epoch 39/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0864 - acc: 0.9530 - precision_15: 0.9230 - auc_15: 0.9875 - recall_15: 0.8434 - val_loss: 0.0906 - val_acc: 0.9657 - val_precision_15: 0.9329 - val_auc_15: 0.9892 - val_recall_15: 0.8473
Epoch 40/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0818 - acc: 0.9537 - precision_15: 0.9259 - auc_15: 0.9898 - recall_15: 0.8468 - val_loss: 0.0796 - val_acc: 0.9684 - val_precision_15: 0.9036 - val_auc_15: 0.9912 - val_recall_15: 0.8917
Epoch 41/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0831 - acc: 0.9529 - precision_15: 0.9239 - auc_15: 0.9896 - recall_15: 0.8389 - val_loss: 0.0768 - val_acc: 0.9696 - val_precision_15: 0.9010 - val_auc_15: 0.9921 - val_recall_15: 0.9063
Epoch 42/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0857 - acc: 0.9528 - precision_15: 0.9261 - auc_15: 0.9884 - recall_15: 0.8384 - val_loss: 0.0756 - val_acc: 0.9702 - val_precision_15: 0.8857 - val_auc_15: 0.9935 - val_recall_15: 0.9287
Epoch 43/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0873 - acc: 0.9530 - precision_15: 0.9236 - auc_15: 0.9871 - recall_15: 0.8427 - val_loss: 0.0819 - val_acc: 0.9678 - val_precision_15: 0.9153 - val_auc_15: 0.9919 - val_recall_15: 0.8823
Epoch 44/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0907 - acc: 0.9531 - precision_15: 0.9283 - auc_15: 0.9859 - recall_15: 0.8375 - val_loss: 0.0712 - val_acc: 0.9717 - val_precision_15: 0.9054 - val_auc_15: 0.9930 - val_recall_15: 0.9069
Epoch 45/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0826 - acc: 0.9548 - precision_15: 0.9338 - auc_15: 0.9886 - recall_15: 0.8431 - val_loss: 0.0839 - val_acc: 0.9676 - val_precision_15: 0.8710 - val_auc_15: 0.9925 - val_recall_15: 0.9262
Epoch 46/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0774 - acc: 0.9550 - precision_15: 0.9306 - auc_15: 0.9906 - recall_15: 0.8458 - val_loss: 0.0724 - val_acc: 0.9710 - val_precision_15: 0.8996 - val_auc_15: 0.9931 - val_recall_15: 0.9052
Epoch 47/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0959 - acc: 0.9494 - precision_15: 0.9134 - auc_15: 0.9862 - recall_15: 0.8350 - val_loss: 0.0904 - val_acc: 0.9648 - val_precision_15: 0.8919 - val_auc_15: 0.9892 - val_recall_15: 0.8934
Epoch 48/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1054 - acc: 0.9481 - precision_15: 0.9092 - auc_15: 0.9800 - recall_15: 0.8275 - val_loss: 0.0766 - val_acc: 0.9707 - val_precision_15: 0.8977 - val_auc_15: 0.9932 - val_recall_15: 0.9165
Epoch 49/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0786 - acc: 0.9543 - precision_15: 0.9296 - auc_15: 0.9908 - recall_15: 0.8415 - val_loss: 0.0770 - val_acc: 0.9695 - val_precision_15: 0.8873 - val_auc_15: 0.9923 - val_recall_15: 0.9231
Epoch 50/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0927 - acc: 0.9520 - precision_15: 0.9185 - auc_15: 0.9863 - recall_15: 0.8418 - val_loss: 0.0764 - val_acc: 0.9699 - val_precision_15: 0.9170 - val_auc_15: 0.9922 - val_recall_15: 0.8876
Epoch 51/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0884 - acc: 0.9529 - precision_15: 0.9284 - auc_15: 0.9868 - recall_15: 0.8352 - val_loss: 0.0890 - val_acc: 0.9642 - val_precision_15: 0.8997 - val_auc_15: 0.9903 - val_recall_15: 0.8748
Epoch 52/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0843 - acc: 0.9530 - precision_15: 0.9276 - auc_15: 0.9890 - recall_15: 0.8401 - val_loss: 0.0859 - val_acc: 0.9654 - val_precision_15: 0.8680 - val_auc_15: 0.9931 - val_recall_15: 0.9379
Epoch 53/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0844 - acc: 0.9527 - precision_15: 0.9224 - auc_15: 0.9894 - recall_15: 0.8408 - val_loss: 0.0857 - val_acc: 0.9659 - val_precision_15: 0.8306 - val_auc_15: 0.9927 - val_recall_15: 0.9474
Epoch 54/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0854 - acc: 0.9541 - precision_15: 0.9249 - auc_15: 0.9883 - recall_15: 0.8489 - val_loss: 0.0728 - val_acc: 0.9717 - val_precision_15: 0.9156 - val_auc_15: 0.9935 - val_recall_15: 0.9168
Epoch 55/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0788 - acc: 0.9556 - precision_15: 0.9365 - auc_15: 0.9895 - recall_15: 0.8470 - val_loss: 0.0804 - val_acc: 0.9694 - val_precision_15: 0.8806 - val_auc_15: 0.9924 - val_recall_15: 0.9156
Epoch 56/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0746 - acc: 0.9560 - precision_15: 0.9382 - auc_15: 0.9914 - recall_15: 0.8481 - val_loss: 0.0703 - val_acc: 0.9724 - val_precision_15: 0.9213 - val_auc_15: 0.9934 - val_recall_15: 0.9058
Epoch 57/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0804 - acc: 0.9562 - precision_15: 0.9374 - auc_15: 0.9887 - recall_15: 0.8488 - val_loss: 0.0695 - val_acc: 0.9727 - val_precision_15: 0.9073 - val_auc_15: 0.9938 - val_recall_15: 0.9208
Epoch 58/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0768 - acc: 0.9557 - precision_15: 0.9343 - auc_15: 0.9910 - recall_15: 0.8528 - val_loss: 0.0747 - val_acc: 0.9701 - val_precision_15: 0.9014 - val_auc_15: 0.9930 - val_recall_15: 0.9228
Epoch 59/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0802 - acc: 0.9548 - precision_15: 0.9335 - auc_15: 0.9900 - recall_15: 0.8464 - val_loss: 0.0676 - val_acc: 0.9732 - val_precision_15: 0.9034 - val_auc_15: 0.9935 - val_recall_15: 0.9120
Epoch 60/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0806 - acc: 0.9555 - precision_15: 0.9356 - auc_15: 0.9890 - recall_15: 0.8491 - val_loss: 0.0704 - val_acc: 0.9721 - val_precision_15: 0.9064 - val_auc_15: 0.9932 - val_recall_15: 0.9200
Epoch 61/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0802 - acc: 0.9550 - precision_15: 0.9308 - auc_15: 0.9896 - recall_15: 0.8507 - val_loss: 0.0787 - val_acc: 0.9682 - val_precision_15: 0.8899 - val_auc_15: 0.9922 - val_recall_15: 0.9198
Epoch 62/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0939 - acc: 0.9513 - precision_15: 0.9233 - auc_15: 0.9849 - recall_15: 0.8344 - val_loss: 0.0770 - val_acc: 0.9705 - val_precision_15: 0.9064 - val_auc_15: 0.9923 - val_recall_15: 0.9108
Epoch 63/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0738 - acc: 0.9560 - precision_15: 0.9319 - auc_15: 0.9917 - recall_15: 0.8591 - val_loss: 0.0736 - val_acc: 0.9710 - val_precision_15: 0.8993 - val_auc_15: 0.9928 - val_recall_15: 0.9239
Epoch 64/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0862 - acc: 0.9534 - precision_15: 0.9282 - auc_15: 0.9882 - recall_15: 0.8384 - val_loss: 0.0779 - val_acc: 0.9689 - val_precision_15: 0.8671 - val_auc_15: 0.9938 - val_recall_15: 0.9402
Epoch 65/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1015 - acc: 0.9527 - precision_15: 0.9219 - auc_15: 0.9818 - recall_15: 0.8437 - val_loss: 0.0731 - val_acc: 0.9711 - val_precision_15: 0.9001 - val_auc_15: 0.9925 - val_recall_15: 0.9184
Epoch 66/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0850 - acc: 0.9553 - precision_15: 0.9312 - auc_15: 0.9879 - recall_15: 0.8528 - val_loss: 0.0724 - val_acc: 0.9716 - val_precision_15: 0.9136 - val_auc_15: 0.9921 - val_recall_15: 0.8983
Epoch 67/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0855 - acc: 0.9542 - precision_15: 0.9289 - auc_15: 0.9880 - recall_15: 0.8437 - val_loss: 0.0762 - val_acc: 0.9692 - val_precision_15: 0.8792 - val_auc_15: 0.9923 - val_recall_15: 0.9226
Epoch 68/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0816 - acc: 0.9537 - precision_15: 0.9253 - auc_15: 0.9893 - recall_15: 0.8463 - val_loss: 0.0716 - val_acc: 0.9721 - val_precision_15: 0.9121 - val_auc_15: 0.9932 - val_recall_15: 0.9198
Epoch 69/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0992 - acc: 0.9515 - precision_15: 0.9271 - auc_15: 0.9824 - recall_15: 0.8305 - val_loss: 0.0724 - val_acc: 0.9722 - val_precision_15: 0.9228 - val_auc_15: 0.9917 - val_recall_15: 0.9037
Epoch 70/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0863 - acc: 0.9532 - precision_15: 0.9165 - auc_15: 0.9881 - recall_15: 0.8516 - val_loss: 0.0718 - val_acc: 0.9719 - val_precision_15: 0.9181 - val_auc_15: 0.9921 - val_recall_15: 0.8916
Epoch 71/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0867 - acc: 0.9547 - precision_15: 0.9323 - auc_15: 0.9864 - recall_15: 0.8495 - val_loss: 0.0701 - val_acc: 0.9719 - val_precision_15: 0.8906 - val_auc_15: 0.9935 - val_recall_15: 0.9311
Epoch 72/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0723 - acc: 0.9576 - precision_15: 0.9386 - auc_15: 0.9915 - recall_15: 0.8550 - val_loss: 0.0659 - val_acc: 0.9743 - val_precision_15: 0.9240 - val_auc_15: 0.9937 - val_recall_15: 0.9173
Epoch 73/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0792 - acc: 0.9572 - precision_15: 0.9385 - auc_15: 0.9891 - recall_15: 0.8605 - val_loss: 0.0685 - val_acc: 0.9729 - val_precision_15: 0.9187 - val_auc_15: 0.9931 - val_recall_15: 0.9085
Epoch 74/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0676 - acc: 0.9584 - precision_15: 0.9417 - auc_15: 0.9929 - recall_15: 0.8625 - val_loss: 0.0681 - val_acc: 0.9728 - val_precision_15: 0.9091 - val_auc_15: 0.9940 - val_recall_15: 0.9245
Epoch 75/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0760 - acc: 0.9568 - precision_15: 0.9405 - auc_15: 0.9905 - recall_15: 0.8518 - val_loss: 0.0656 - val_acc: 0.9741 - val_precision_15: 0.9033 - val_auc_15: 0.9943 - val_recall_15: 0.9282
Epoch 76/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0781 - acc: 0.9572 - precision_15: 0.9387 - auc_15: 0.9893 - recall_15: 0.8559 - val_loss: 0.0655 - val_acc: 0.9741 - val_precision_15: 0.9094 - val_auc_15: 0.9934 - val_recall_15: 0.9159
Epoch 77/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0731 - acc: 0.9572 - precision_15: 0.9377 - auc_15: 0.9914 - recall_15: 0.8561 - val_loss: 0.0708 - val_acc: 0.9722 - val_precision_15: 0.8823 - val_auc_15: 0.9946 - val_recall_15: 0.9413
Epoch 78/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0702 - acc: 0.9571 - precision_15: 0.9348 - auc_15: 0.9925 - recall_15: 0.8580 - val_loss: 0.0696 - val_acc: 0.9718 - val_precision_15: 0.9126 - val_auc_15: 0.9943 - val_recall_15: 0.9177
Epoch 79/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0839 - acc: 0.9555 - precision_15: 0.9352 - auc_15: 0.9883 - recall_15: 0.8487 - val_loss: 0.0683 - val_acc: 0.9732 - val_precision_15: 0.9108 - val_auc_15: 0.9938 - val_recall_15: 0.9236
Epoch 80/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0761 - acc: 0.9567 - precision_15: 0.9407 - auc_15: 0.9903 - recall_15: 0.8501 - val_loss: 0.0656 - val_acc: 0.9736 - val_precision_15: 0.9056 - val_auc_15: 0.9946 - val_recall_15: 0.9268
Epoch 81/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0850 - acc: 0.9546 - precision_15: 0.9295 - auc_15: 0.9873 - recall_15: 0.8507 - val_loss: 0.0699 - val_acc: 0.9723 - val_precision_15: 0.9040 - val_auc_15: 0.9931 - val_recall_15: 0.9218
Epoch 82/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0845 - acc: 0.9549 - precision_15: 0.9301 - auc_15: 0.9880 - recall_15: 0.8505 - val_loss: 0.0737 - val_acc: 0.9704 - val_precision_15: 0.8841 - val_auc_15: 0.9945 - val_recall_15: 0.9391
Epoch 83/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0791 - acc: 0.9563 - precision_15: 0.9328 - auc_15: 0.9889 - recall_15: 0.8568 - val_loss: 0.0712 - val_acc: 0.9727 - val_precision_15: 0.9336 - val_auc_15: 0.9928 - val_recall_15: 0.8990
Epoch 84/100
33/33 [==============================] - 3s 88ms/step - loss: 0.0769 - acc: 0.9569 - precision_15: 0.9407 - auc_15: 0.9900 - recall_15: 0.8528 - val_loss: 0.0643 - val_acc: 0.9746 - val_precision_15: 0.8969 - val_auc_15: 0.9931 - val_recall_15: 0.9193
Epoch 85/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0668 - acc: 0.9593 - precision_15: 0.9444 - auc_15: 0.9926 - recall_15: 0.8674 - val_loss: 0.0718 - val_acc: 0.9718 - val_precision_15: 0.9334 - val_auc_15: 0.9925 - val_recall_15: 0.8897
Epoch 86/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0724 - acc: 0.9567 - precision_15: 0.9393 - auc_15: 0.9916 - recall_15: 0.8511 - val_loss: 0.0682 - val_acc: 0.9729 - val_precision_15: 0.9209 - val_auc_15: 0.9933 - val_recall_15: 0.9050
Epoch 87/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0821 - acc: 0.9562 - precision_15: 0.9377 - auc_15: 0.9880 - recall_15: 0.8518 - val_loss: 0.0674 - val_acc: 0.9737 - val_precision_15: 0.9255 - val_auc_15: 0.9931 - val_recall_15: 0.9104
Epoch 88/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0785 - acc: 0.9573 - precision_15: 0.9401 - auc_15: 0.9886 - recall_15: 0.8533 - val_loss: 0.0676 - val_acc: 0.9732 - val_precision_15: 0.9226 - val_auc_15: 0.9938 - val_recall_15: 0.9173
Epoch 89/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0660 - acc: 0.9588 - precision_15: 0.9449 - auc_15: 0.9932 - recall_15: 0.8581 - val_loss: 0.0678 - val_acc: 0.9729 - val_precision_15: 0.9150 - val_auc_15: 0.9937 - val_recall_15: 0.9125
Epoch 90/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1002 - acc: 0.9526 - precision_15: 0.9268 - auc_15: 0.9814 - recall_15: 0.8428 - val_loss: 0.0725 - val_acc: 0.9722 - val_precision_15: 0.9119 - val_auc_15: 0.9930 - val_recall_15: 0.9050
Epoch 91/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0832 - acc: 0.9553 - precision_15: 0.9323 - auc_15: 0.9878 - recall_15: 0.8492 - val_loss: 0.0728 - val_acc: 0.9709 - val_precision_15: 0.9002 - val_auc_15: 0.9941 - val_recall_15: 0.9285
Epoch 92/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0727 - acc: 0.9570 - precision_15: 0.9383 - auc_15: 0.9917 - recall_15: 0.8562 - val_loss: 0.0717 - val_acc: 0.9715 - val_precision_15: 0.9143 - val_auc_15: 0.9927 - val_recall_15: 0.9005
Epoch 93/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0655 - acc: 0.9593 - precision_15: 0.9455 - auc_15: 0.9933 - recall_15: 0.8659 - val_loss: 0.0695 - val_acc: 0.9729 - val_precision_15: 0.9340 - val_auc_15: 0.9931 - val_recall_15: 0.8998
Epoch 94/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0734 - acc: 0.9576 - precision_15: 0.9432 - auc_15: 0.9906 - recall_15: 0.8566 - val_loss: 0.0693 - val_acc: 0.9724 - val_precision_15: 0.9150 - val_auc_15: 0.9937 - val_recall_15: 0.9176
Epoch 95/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0754 - acc: 0.9574 - precision_15: 0.9432 - auc_15: 0.9900 - recall_15: 0.8533 - val_loss: 0.0654 - val_acc: 0.9742 - val_precision_15: 0.8960 - val_auc_15: 0.9944 - val_recall_15: 0.9333
Epoch 96/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0673 - acc: 0.9594 - precision_15: 0.9476 - auc_15: 0.9920 - recall_15: 0.8595 - val_loss: 0.0679 - val_acc: 0.9725 - val_precision_15: 0.8980 - val_auc_15: 0.9939 - val_recall_15: 0.9278
Epoch 97/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0671 - acc: 0.9593 - precision_15: 0.9460 - auc_15: 0.9924 - recall_15: 0.8630 - val_loss: 0.0637 - val_acc: 0.9749 - val_precision_15: 0.9263 - val_auc_15: 0.9939 - val_recall_15: 0.9132
Epoch 98/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0707 - acc: 0.9589 - precision_15: 0.9434 - auc_15: 0.9912 - recall_15: 0.8630 - val_loss: 0.0656 - val_acc: 0.9748 - val_precision_15: 0.9329 - val_auc_15: 0.9929 - val_recall_15: 0.9006
Epoch 99/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0751 - acc: 0.9581 - precision_15: 0.9415 - auc_15: 0.9897 - recall_15: 0.8576 - val_loss: 0.0659 - val_acc: 0.9738 - val_precision_15: 0.9053 - val_auc_15: 0.9941 - val_recall_15: 0.9272
Epoch 100/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0792 - acc: 0.9583 - precision_15: 0.9433 - auc_15: 0.9877 - recall_15: 0.8595 - val_loss: 0.0634 - val_acc: 0.9751 - val_precision_15: 0.9138 - val_auc_15: 0.9936 - val_recall_15: 0.9292
6/6 [==============================] - 0s 25ms/step - loss: 0.0688 - acc: 0.9711 - precision_15: 0.8945 - auc_15: 0.9947 - recall_15: 0.9318
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbe0049daf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8353187099359352
test_sensitivity 0.9380713056265771
test_specifitivity 0.9769850290965968
test_accuracy 0.9706681993272569
test_precision 0.8876223719554016
test_jaccard_score 0.8353187099359352
test_dicecoef 0.9121498174506051
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-111122.h5
[0. 0. 0. 0. 0.] [0.9706682  0.83531871 0.88762237 0.93807131 0.97698503]

-------------------------
Rep: 1
-------------------------

2021-09-25 11:11:23.358563: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:11:23.358619: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied histogram equalization on all input images
Applied normalization
Data augumentation on
Prep done
Training samples: 258, channel mean: 0.1526593703632206,
Validation samples: 63, channel mean: 0.14989169631611324
Model built.
Model: "functional_33"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_17 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_304 (Conv2D)             (None, 256, 256, 32) 320         input_17[0][0]
__________________________________________________________________________________________________
conv2d_305 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_304[0][0]
__________________________________________________________________________________________________
max_pooling2d_64 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_305[0][0]
__________________________________________________________________________________________________
conv2d_306 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_64[0][0]
__________________________________________________________________________________________________
conv2d_307 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_306[0][0]
__________________________________________________________________________________________________
max_pooling2d_65 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_307[0][0]
__________________________________________________________________________________________________
conv2d_308 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_65[0][0]
__________________________________________________________________________________________________
conv2d_309 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_308[0][0]
__________________________________________________________________________________________________
max_pooling2d_66 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_309[0][0]
__________________________________________________________________________________________________
conv2d_310 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_66[0][0]
__________________________________________________________________________________________________
conv2d_311 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_310[0][0]
__________________________________________________________________________________________________
max_pooling2d_67 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_311[0][0]
__________________________________________________________________________________________________
conv2d_312 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_67[0][0]
__________________________________________________________________________________________________
conv2d_313 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_312[0][0]
__________________________________________________________________________________________________
up_sampling2d_64 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_313[0][0]
__________________________________________________________________________________________________
concatenate_64 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_64[0][0]
                                                                 conv2d_311[0][0]
__________________________________________________________________________________________________
conv2d_314 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_64[0][0]
__________________________________________________________________________________________________
conv2d_315 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_314[0][0]
__________________________________________________________________________________________________
up_sampling2d_65 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_315[0][0]
__________________________________________________________________________________________________
concatenate_65 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_65[0][0]
                                                                 conv2d_309[0][0]
__________________________________________________________________________________________________
conv2d_316 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_65[0][0]
__________________________________________________________________________________________________
conv2d_317 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_316[0][0]
__________________________________________________________________________________________________
up_sampling2d_66 (UpSampling2D) (None, 128, 128, 128 0           conv2d_317[0][0]
__________________________________________________________________________________________________
concatenate_66 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_66[0][0]
                                                                 conv2d_307[0][0]
__________________________________________________________________________________________________
conv2d_318 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_66[0][0]
__________________________________________________________________________________________________
conv2d_319 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_318[0][0]
__________________________________________________________________________________________________
up_sampling2d_67 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_319[0][0]
__________________________________________________________________________________________________
concatenate_67 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_67[0][0]
                                                                 conv2d_305[0][0]
__________________________________________________________________________________________________
conv2d_320 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_67[0][0]
__________________________________________________________________________________________________
conv2d_321 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_320[0][0]
__________________________________________________________________________________________________
conv2d_322 (Conv2D)             (None, 256, 256, 1)  33          conv2d_321[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6931 - acc: 0.7843 - precision_16: 0.0758 - auc_16: 0.1837 - recall_16: 0.03682021-09-25 11:11:28.308486: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:11:28.308563: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 11:11:28.582488: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 11:11:28.595292: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-111123/train/plugins/profile/2021_09_25_11_11_28
2021-09-25 11:11:28.602247: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-111123/train/plugins/profile/2021_09_25_11_11_28/xeon-e-v100.trace.json.gz
2021-09-25 11:11:28.622698: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-111123/train/plugins/profile/2021_09_25_11_11_28
2021-09-25 11:11:28.630184: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-111123/train/plugins/profile/2021_09_25_11_11_28/xeon-e-v100.memory_profile.json.gz
2021-09-25 11:11:28.649319: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-111123/train/plugins/profile/2021_09_25_11_11_28Dumped tool data for xplane.pb to logs/fit/20210925-111123/train/plugins/profile/2021_09_25_11_11_28/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-111123/train/plugins/profile/2021_09_25_11_11_28/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-111123/train/plugins/profile/2021_09_25_11_11_28/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-111123/train/plugins/profile/2021_09_25_11_11_28/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-111123/train/plugins/profile/2021_09_25_11_11_28/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6889 - acc: 0.7861 - precision_16: 0.0776 - auc_16: 0.2494 - recall_16: 0.0159WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0648s vs `on_train_batch_end` time: 0.2776s). Check your callbacks.
33/33 [==============================] - 4s 108ms/step - loss: 0.4275 - acc: 0.8799 - precision_16: 0.7209 - auc_16: 0.8051 - recall_16: 0.4938 - val_loss: 0.2426 - val_acc: 0.9025 - val_precision_16: 0.6929 - val_auc_16: 0.9233 - val_recall_16: 0.6831
Epoch 2/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2228 - acc: 0.8994 - precision_16: 0.7824 - auc_16: 0.9278 - recall_16: 0.6133 - val_loss: 0.2043 - val_acc: 0.9204 - val_precision_16: 0.8200 - val_auc_16: 0.9430 - val_recall_16: 0.6075
Epoch 3/100
33/33 [==============================] - 3s 87ms/step - loss: 0.2106 - acc: 0.9065 - precision_16: 0.8009 - auc_16: 0.9373 - recall_16: 0.6527 - val_loss: 0.2364 - val_acc: 0.9162 - val_precision_16: 0.7775 - val_auc_16: 0.9424 - val_recall_16: 0.6407
Epoch 4/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2154 - acc: 0.9028 - precision_16: 0.7984 - auc_16: 0.9331 - recall_16: 0.6178 - val_loss: 0.2693 - val_acc: 0.8806 - val_precision_16: 0.6186 - val_auc_16: 0.9240 - val_recall_16: 0.7640
Epoch 5/100
33/33 [==============================] - 3s 87ms/step - loss: 0.2128 - acc: 0.9012 - precision_16: 0.7640 - auc_16: 0.9388 - recall_16: 0.6772 - val_loss: 0.1998 - val_acc: 0.9165 - val_precision_16: 0.8161 - val_auc_16: 0.9473 - val_recall_16: 0.5934
Epoch 6/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1934 - acc: 0.9079 - precision_16: 0.8258 - auc_16: 0.9480 - recall_16: 0.6136 - val_loss: 0.2147 - val_acc: 0.9157 - val_precision_16: 0.7580 - val_auc_16: 0.9349 - val_recall_16: 0.6844
Epoch 7/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1896 - acc: 0.9096 - precision_16: 0.8287 - auc_16: 0.9504 - recall_16: 0.6358 - val_loss: 0.1811 - val_acc: 0.9265 - val_precision_16: 0.7944 - val_auc_16: 0.9546 - val_recall_16: 0.6881
Epoch 8/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2152 - acc: 0.9028 - precision_16: 0.8029 - auc_16: 0.9320 - recall_16: 0.6143 - val_loss: 0.2617 - val_acc: 0.9055 - val_precision_16: 0.8382 - val_auc_16: 0.9155 - val_recall_16: 0.5198
Epoch 9/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1990 - acc: 0.9059 - precision_16: 0.8010 - auc_16: 0.9460 - recall_16: 0.6488 - val_loss: 0.2329 - val_acc: 0.9067 - val_precision_16: 0.7004 - val_auc_16: 0.9356 - val_recall_16: 0.7113
Epoch 10/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1937 - acc: 0.9096 - precision_16: 0.8162 - auc_16: 0.9472 - recall_16: 0.6545 - val_loss: 0.2254 - val_acc: 0.9127 - val_precision_16: 0.8916 - val_auc_16: 0.9369 - val_recall_16: 0.4994
Epoch 11/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1956 - acc: 0.9086 - precision_16: 0.8196 - auc_16: 0.9449 - recall_16: 0.6437 - val_loss: 0.2138 - val_acc: 0.9104 - val_precision_16: 0.7770 - val_auc_16: 0.9414 - val_recall_16: 0.6158
Epoch 12/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1892 - acc: 0.9122 - precision_16: 0.8333 - auc_16: 0.9476 - recall_16: 0.6440 - val_loss: 0.1858 - val_acc: 0.9309 - val_precision_16: 0.8697 - val_auc_16: 0.9448 - val_recall_16: 0.6136
Epoch 13/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1890 - acc: 0.9117 - precision_16: 0.8359 - auc_16: 0.9491 - recall_16: 0.6434 - val_loss: 0.2054 - val_acc: 0.9190 - val_precision_16: 0.7672 - val_auc_16: 0.9428 - val_recall_16: 0.6714
Epoch 14/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1749 - acc: 0.9181 - precision_16: 0.8410 - auc_16: 0.9565 - recall_16: 0.6961 - val_loss: 0.1959 - val_acc: 0.9204 - val_precision_16: 0.9100 - val_auc_16: 0.9575 - val_recall_16: 0.5573
Epoch 15/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1711 - acc: 0.9210 - precision_16: 0.8449 - auc_16: 0.9545 - recall_16: 0.7096 - val_loss: 0.1826 - val_acc: 0.9305 - val_precision_16: 0.7906 - val_auc_16: 0.9545 - val_recall_16: 0.7819
Epoch 16/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1668 - acc: 0.9236 - precision_16: 0.8386 - auc_16: 0.9564 - recall_16: 0.7319 - val_loss: 0.1411 - val_acc: 0.9467 - val_precision_16: 0.8503 - val_auc_16: 0.9723 - val_recall_16: 0.7970
Epoch 17/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1384 - acc: 0.9331 - precision_16: 0.8687 - auc_16: 0.9703 - recall_16: 0.7634 - val_loss: 0.1351 - val_acc: 0.9464 - val_precision_16: 0.8311 - val_auc_16: 0.9757 - val_recall_16: 0.8280
Epoch 18/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1246 - acc: 0.9366 - precision_16: 0.8735 - auc_16: 0.9771 - recall_16: 0.7884 - val_loss: 0.1232 - val_acc: 0.9516 - val_precision_16: 0.8211 - val_auc_16: 0.9841 - val_recall_16: 0.8990
Epoch 19/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1252 - acc: 0.9365 - precision_16: 0.8755 - auc_16: 0.9775 - recall_16: 0.7832 - val_loss: 0.1066 - val_acc: 0.9584 - val_precision_16: 0.8871 - val_auc_16: 0.9830 - val_recall_16: 0.8129
Epoch 20/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1207 - acc: 0.9393 - precision_16: 0.8864 - auc_16: 0.9782 - recall_16: 0.7881 - val_loss: 0.0937 - val_acc: 0.9640 - val_precision_16: 0.9004 - val_auc_16: 0.9889 - val_recall_16: 0.8629
Epoch 21/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1164 - acc: 0.9410 - precision_16: 0.8885 - auc_16: 0.9797 - recall_16: 0.7977 - val_loss: 0.0948 - val_acc: 0.9629 - val_precision_16: 0.9291 - val_auc_16: 0.9893 - val_recall_16: 0.8071
Epoch 22/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1035 - acc: 0.9441 - precision_16: 0.9080 - auc_16: 0.9852 - recall_16: 0.7947 - val_loss: 0.1007 - val_acc: 0.9607 - val_precision_16: 0.8527 - val_auc_16: 0.9864 - val_recall_16: 0.8909
Epoch 23/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1033 - acc: 0.9446 - precision_16: 0.8917 - auc_16: 0.9845 - recall_16: 0.8220 - val_loss: 0.1009 - val_acc: 0.9614 - val_precision_16: 0.9054 - val_auc_16: 0.9863 - val_recall_16: 0.8540
Epoch 24/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1004 - acc: 0.9469 - precision_16: 0.9091 - auc_16: 0.9847 - recall_16: 0.8186 - val_loss: 0.0889 - val_acc: 0.9636 - val_precision_16: 0.8397 - val_auc_16: 0.9914 - val_recall_16: 0.9274
Epoch 25/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1073 - acc: 0.9451 - precision_16: 0.9014 - auc_16: 0.9836 - recall_16: 0.8136 - val_loss: 0.0856 - val_acc: 0.9665 - val_precision_16: 0.9046 - val_auc_16: 0.9900 - val_recall_16: 0.8798
Epoch 26/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0870 - acc: 0.9505 - precision_16: 0.9149 - auc_16: 0.9895 - recall_16: 0.8384 - val_loss: 0.0868 - val_acc: 0.9650 - val_precision_16: 0.8740 - val_auc_16: 0.9900 - val_recall_16: 0.9059
Epoch 27/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1041 - acc: 0.9457 - precision_16: 0.9039 - auc_16: 0.9829 - recall_16: 0.8117 - val_loss: 0.0825 - val_acc: 0.9680 - val_precision_16: 0.8713 - val_auc_16: 0.9927 - val_recall_16: 0.9272
Epoch 28/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0898 - acc: 0.9500 - precision_16: 0.9141 - auc_16: 0.9878 - recall_16: 0.8305 - val_loss: 0.0830 - val_acc: 0.9679 - val_precision_16: 0.9106 - val_auc_16: 0.9897 - val_recall_16: 0.8658
Epoch 29/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0918 - acc: 0.9488 - precision_16: 0.9084 - auc_16: 0.9874 - recall_16: 0.8282 - val_loss: 0.0913 - val_acc: 0.9647 - val_precision_16: 0.9088 - val_auc_16: 0.9890 - val_recall_16: 0.8795
Epoch 30/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1010 - acc: 0.9493 - precision_16: 0.9187 - auc_16: 0.9826 - recall_16: 0.8189 - val_loss: 0.0854 - val_acc: 0.9675 - val_precision_16: 0.8913 - val_auc_16: 0.9905 - val_recall_16: 0.9015
Epoch 31/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0942 - acc: 0.9496 - precision_16: 0.9093 - auc_16: 0.9863 - recall_16: 0.8399 - val_loss: 0.0822 - val_acc: 0.9676 - val_precision_16: 0.9222 - val_auc_16: 0.9919 - val_recall_16: 0.8754
Epoch 32/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0918 - acc: 0.9493 - precision_16: 0.9116 - auc_16: 0.9874 - recall_16: 0.8298 - val_loss: 0.0877 - val_acc: 0.9665 - val_precision_16: 0.9318 - val_auc_16: 0.9896 - val_recall_16: 0.8416
Epoch 33/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0842 - acc: 0.9522 - precision_16: 0.9259 - auc_16: 0.9893 - recall_16: 0.8364 - val_loss: 0.0847 - val_acc: 0.9679 - val_precision_16: 0.9005 - val_auc_16: 0.9921 - val_recall_16: 0.8946
Epoch 34/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1028 - acc: 0.9481 - precision_16: 0.9166 - auc_16: 0.9838 - recall_16: 0.8150 - val_loss: 0.0808 - val_acc: 0.9689 - val_precision_16: 0.9036 - val_auc_16: 0.9918 - val_recall_16: 0.8982
Epoch 35/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0961 - acc: 0.9511 - precision_16: 0.9193 - auc_16: 0.9841 - recall_16: 0.8347 - val_loss: 0.1006 - val_acc: 0.9601 - val_precision_16: 0.8471 - val_auc_16: 0.9921 - val_recall_16: 0.9439
Epoch 36/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0994 - acc: 0.9502 - precision_16: 0.9169 - auc_16: 0.9852 - recall_16: 0.8320 - val_loss: 0.0789 - val_acc: 0.9690 - val_precision_16: 0.9167 - val_auc_16: 0.9915 - val_recall_16: 0.8845
Epoch 37/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1077 - acc: 0.9477 - precision_16: 0.9074 - auc_16: 0.9801 - recall_16: 0.8166 - val_loss: 0.0959 - val_acc: 0.9621 - val_precision_16: 0.8294 - val_auc_16: 0.9913 - val_recall_16: 0.9283
Epoch 38/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0975 - acc: 0.9487 - precision_16: 0.9052 - auc_16: 0.9858 - recall_16: 0.8296 - val_loss: 0.0809 - val_acc: 0.9684 - val_precision_16: 0.9072 - val_auc_16: 0.9916 - val_recall_16: 0.8977
Epoch 39/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0855 - acc: 0.9539 - precision_16: 0.9281 - auc_16: 0.9874 - recall_16: 0.8418 - val_loss: 0.0848 - val_acc: 0.9683 - val_precision_16: 0.9256 - val_auc_16: 0.9900 - val_recall_16: 0.8732
Epoch 40/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0806 - acc: 0.9547 - precision_16: 0.9280 - auc_16: 0.9899 - recall_16: 0.8522 - val_loss: 0.0738 - val_acc: 0.9706 - val_precision_16: 0.9108 - val_auc_16: 0.9925 - val_recall_16: 0.8989
Epoch 41/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0768 - acc: 0.9547 - precision_16: 0.9301 - auc_16: 0.9913 - recall_16: 0.8456 - val_loss: 0.0731 - val_acc: 0.9711 - val_precision_16: 0.9066 - val_auc_16: 0.9926 - val_recall_16: 0.9101
Epoch 42/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0821 - acc: 0.9536 - precision_16: 0.9274 - auc_16: 0.9898 - recall_16: 0.8428 - val_loss: 0.0716 - val_acc: 0.9723 - val_precision_16: 0.9026 - val_auc_16: 0.9937 - val_recall_16: 0.9219
Epoch 43/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0892 - acc: 0.9512 - precision_16: 0.9209 - auc_16: 0.9875 - recall_16: 0.8319 - val_loss: 0.0774 - val_acc: 0.9696 - val_precision_16: 0.9147 - val_auc_16: 0.9925 - val_recall_16: 0.8958
Epoch 44/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0847 - acc: 0.9536 - precision_16: 0.9284 - auc_16: 0.9885 - recall_16: 0.8411 - val_loss: 0.0688 - val_acc: 0.9727 - val_precision_16: 0.9149 - val_auc_16: 0.9936 - val_recall_16: 0.9033
Epoch 45/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0913 - acc: 0.9526 - precision_16: 0.9285 - auc_16: 0.9850 - recall_16: 0.8330 - val_loss: 0.0760 - val_acc: 0.9704 - val_precision_16: 0.8921 - val_auc_16: 0.9932 - val_recall_16: 0.9178
Epoch 46/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0886 - acc: 0.9541 - precision_16: 0.9288 - auc_16: 0.9863 - recall_16: 0.8434 - val_loss: 0.0740 - val_acc: 0.9704 - val_precision_16: 0.8784 - val_auc_16: 0.9939 - val_recall_16: 0.9291
Epoch 47/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0852 - acc: 0.9532 - precision_16: 0.9200 - auc_16: 0.9886 - recall_16: 0.8530 - val_loss: 0.0819 - val_acc: 0.9671 - val_precision_16: 0.8762 - val_auc_16: 0.9926 - val_recall_16: 0.9303
Epoch 48/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0994 - acc: 0.9509 - precision_16: 0.9176 - auc_16: 0.9828 - recall_16: 0.8376 - val_loss: 0.0791 - val_acc: 0.9690 - val_precision_16: 0.8809 - val_auc_16: 0.9935 - val_recall_16: 0.9270
Epoch 49/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0834 - acc: 0.9542 - precision_16: 0.9279 - auc_16: 0.9885 - recall_16: 0.8434 - val_loss: 0.0760 - val_acc: 0.9698 - val_precision_16: 0.8804 - val_auc_16: 0.9932 - val_recall_16: 0.9352
Epoch 50/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0854 - acc: 0.9536 - precision_16: 0.9232 - auc_16: 0.9885 - recall_16: 0.8494 - val_loss: 0.0700 - val_acc: 0.9725 - val_precision_16: 0.9080 - val_auc_16: 0.9935 - val_recall_16: 0.9165
Epoch 51/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0790 - acc: 0.9558 - precision_16: 0.9365 - auc_16: 0.9894 - recall_16: 0.8475 - val_loss: 0.0742 - val_acc: 0.9709 - val_precision_16: 0.9115 - val_auc_16: 0.9930 - val_recall_16: 0.9070
Epoch 52/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0752 - acc: 0.9564 - precision_16: 0.9370 - auc_16: 0.9910 - recall_16: 0.8544 - val_loss: 0.0755 - val_acc: 0.9695 - val_precision_16: 0.8945 - val_auc_16: 0.9940 - val_recall_16: 0.9289
Epoch 53/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0757 - acc: 0.9565 - precision_16: 0.9356 - auc_16: 0.9908 - recall_16: 0.8546 - val_loss: 0.0866 - val_acc: 0.9650 - val_precision_16: 0.8232 - val_auc_16: 0.9932 - val_recall_16: 0.9521
Epoch 54/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0811 - acc: 0.9548 - precision_16: 0.9265 - auc_16: 0.9894 - recall_16: 0.8545 - val_loss: 0.0706 - val_acc: 0.9719 - val_precision_16: 0.9056 - val_auc_16: 0.9939 - val_recall_16: 0.9306
Epoch 55/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0704 - acc: 0.9576 - precision_16: 0.9412 - auc_16: 0.9920 - recall_16: 0.8560 - val_loss: 0.0772 - val_acc: 0.9704 - val_precision_16: 0.8863 - val_auc_16: 0.9924 - val_recall_16: 0.9160
Epoch 56/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0714 - acc: 0.9570 - precision_16: 0.9405 - auc_16: 0.9919 - recall_16: 0.8533 - val_loss: 0.0671 - val_acc: 0.9740 - val_precision_16: 0.9251 - val_auc_16: 0.9940 - val_recall_16: 0.9120
Epoch 57/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0943 - acc: 0.9547 - precision_16: 0.9301 - auc_16: 0.9827 - recall_16: 0.8455 - val_loss: 0.0714 - val_acc: 0.9720 - val_precision_16: 0.8975 - val_auc_16: 0.9939 - val_recall_16: 0.9284
Epoch 58/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0700 - acc: 0.9575 - precision_16: 0.9372 - auc_16: 0.9927 - recall_16: 0.8630 - val_loss: 0.0707 - val_acc: 0.9724 - val_precision_16: 0.9191 - val_auc_16: 0.9934 - val_recall_16: 0.9158
Epoch 59/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0842 - acc: 0.9547 - precision_16: 0.9299 - auc_16: 0.9884 - recall_16: 0.8496 - val_loss: 0.0802 - val_acc: 0.9682 - val_precision_16: 0.8610 - val_auc_16: 0.9924 - val_recall_16: 0.9306
Epoch 60/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0738 - acc: 0.9569 - precision_16: 0.9371 - auc_16: 0.9911 - recall_16: 0.8602 - val_loss: 0.0679 - val_acc: 0.9733 - val_precision_16: 0.9103 - val_auc_16: 0.9935 - val_recall_16: 0.9238
Epoch 61/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0745 - acc: 0.9573 - precision_16: 0.9380 - auc_16: 0.9906 - recall_16: 0.8599 - val_loss: 0.0707 - val_acc: 0.9721 - val_precision_16: 0.9158 - val_auc_16: 0.9929 - val_recall_16: 0.9139
Epoch 62/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0797 - acc: 0.9567 - precision_16: 0.9378 - auc_16: 0.9887 - recall_16: 0.8545 - val_loss: 0.0723 - val_acc: 0.9719 - val_precision_16: 0.9138 - val_auc_16: 0.9931 - val_recall_16: 0.9117
Epoch 63/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0694 - acc: 0.9580 - precision_16: 0.9379 - auc_16: 0.9924 - recall_16: 0.8649 - val_loss: 0.0691 - val_acc: 0.9723 - val_precision_16: 0.9013 - val_auc_16: 0.9938 - val_recall_16: 0.9302
Epoch 64/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0851 - acc: 0.9558 - precision_16: 0.9355 - auc_16: 0.9868 - recall_16: 0.8493 - val_loss: 0.0741 - val_acc: 0.9699 - val_precision_16: 0.8723 - val_auc_16: 0.9938 - val_recall_16: 0.9402
Epoch 65/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0875 - acc: 0.9556 - precision_16: 0.9302 - auc_16: 0.9862 - recall_16: 0.8540 - val_loss: 0.0692 - val_acc: 0.9725 - val_precision_16: 0.9111 - val_auc_16: 0.9935 - val_recall_16: 0.9145
Epoch 66/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0674 - acc: 0.9585 - precision_16: 0.9396 - auc_16: 0.9930 - recall_16: 0.8671 - val_loss: 0.0690 - val_acc: 0.9724 - val_precision_16: 0.9075 - val_auc_16: 0.9931 - val_recall_16: 0.9113
Epoch 67/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0800 - acc: 0.9553 - precision_16: 0.9296 - auc_16: 0.9900 - recall_16: 0.8520 - val_loss: 0.0722 - val_acc: 0.9711 - val_precision_16: 0.9030 - val_auc_16: 0.9923 - val_recall_16: 0.9054
Epoch 68/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0688 - acc: 0.9580 - precision_16: 0.9414 - auc_16: 0.9924 - recall_16: 0.8579 - val_loss: 0.0678 - val_acc: 0.9729 - val_precision_16: 0.9100 - val_auc_16: 0.9941 - val_recall_16: 0.9280
Epoch 69/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0920 - acc: 0.9549 - precision_16: 0.9324 - auc_16: 0.9842 - recall_16: 0.8513 - val_loss: 0.0708 - val_acc: 0.9732 - val_precision_16: 0.9324 - val_auc_16: 0.9919 - val_recall_16: 0.8998
Epoch 70/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0783 - acc: 0.9564 - precision_16: 0.9295 - auc_16: 0.9900 - recall_16: 0.8607 - val_loss: 0.0664 - val_acc: 0.9736 - val_precision_16: 0.9147 - val_auc_16: 0.9937 - val_recall_16: 0.9085
Epoch 71/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0724 - acc: 0.9575 - precision_16: 0.9400 - auc_16: 0.9917 - recall_16: 0.8623 - val_loss: 0.0662 - val_acc: 0.9737 - val_precision_16: 0.9072 - val_auc_16: 0.9939 - val_recall_16: 0.9224
Epoch 72/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0650 - acc: 0.9598 - precision_16: 0.9454 - auc_16: 0.9930 - recall_16: 0.8641 - val_loss: 0.0629 - val_acc: 0.9753 - val_precision_16: 0.9253 - val_auc_16: 0.9944 - val_recall_16: 0.9224
Epoch 73/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0904 - acc: 0.9563 - precision_16: 0.9367 - auc_16: 0.9849 - recall_16: 0.8569 - val_loss: 0.0684 - val_acc: 0.9731 - val_precision_16: 0.9087 - val_auc_16: 0.9934 - val_recall_16: 0.9217
Epoch 74/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0637 - acc: 0.9599 - precision_16: 0.9464 - auc_16: 0.9938 - recall_16: 0.8696 - val_loss: 0.0665 - val_acc: 0.9736 - val_precision_16: 0.9179 - val_auc_16: 0.9938 - val_recall_16: 0.9194
Epoch 75/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0728 - acc: 0.9585 - precision_16: 0.9458 - auc_16: 0.9907 - recall_16: 0.8580 - val_loss: 0.0626 - val_acc: 0.9753 - val_precision_16: 0.9113 - val_auc_16: 0.9946 - val_recall_16: 0.9273
Epoch 76/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0657 - acc: 0.9594 - precision_16: 0.9465 - auc_16: 0.9928 - recall_16: 0.8651 - val_loss: 0.0644 - val_acc: 0.9742 - val_precision_16: 0.8994 - val_auc_16: 0.9943 - val_recall_16: 0.9290
Epoch 77/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0848 - acc: 0.9541 - precision_16: 0.9274 - auc_16: 0.9878 - recall_16: 0.8448 - val_loss: 0.0753 - val_acc: 0.9708 - val_precision_16: 0.8713 - val_auc_16: 0.9946 - val_recall_16: 0.9463
Epoch 78/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0703 - acc: 0.9573 - precision_16: 0.9360 - auc_16: 0.9924 - recall_16: 0.8572 - val_loss: 0.0703 - val_acc: 0.9712 - val_precision_16: 0.9048 - val_auc_16: 0.9943 - val_recall_16: 0.9235
Epoch 79/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0813 - acc: 0.9565 - precision_16: 0.9349 - auc_16: 0.9885 - recall_16: 0.8575 - val_loss: 0.0659 - val_acc: 0.9741 - val_precision_16: 0.9143 - val_auc_16: 0.9940 - val_recall_16: 0.9251
Epoch 80/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0773 - acc: 0.9581 - precision_16: 0.9450 - auc_16: 0.9886 - recall_16: 0.8557 - val_loss: 0.0650 - val_acc: 0.9734 - val_precision_16: 0.9022 - val_auc_16: 0.9947 - val_recall_16: 0.9294
Epoch 81/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0657 - acc: 0.9590 - precision_16: 0.9441 - auc_16: 0.9931 - recall_16: 0.8674 - val_loss: 0.0649 - val_acc: 0.9741 - val_precision_16: 0.9078 - val_auc_16: 0.9941 - val_recall_16: 0.9296
Epoch 82/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0797 - acc: 0.9575 - precision_16: 0.9391 - auc_16: 0.9886 - recall_16: 0.8598 - val_loss: 0.0707 - val_acc: 0.9706 - val_precision_16: 0.8822 - val_auc_16: 0.9949 - val_recall_16: 0.9438
Epoch 83/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0753 - acc: 0.9575 - precision_16: 0.9363 - auc_16: 0.9897 - recall_16: 0.8616 - val_loss: 0.0678 - val_acc: 0.9737 - val_precision_16: 0.9267 - val_auc_16: 0.9935 - val_recall_16: 0.9139
Epoch 84/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0639 - acc: 0.9598 - precision_16: 0.9464 - auc_16: 0.9935 - recall_16: 0.8677 - val_loss: 0.0619 - val_acc: 0.9756 - val_precision_16: 0.9045 - val_auc_16: 0.9934 - val_recall_16: 0.9172
Epoch 85/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0609 - acc: 0.9608 - precision_16: 0.9497 - auc_16: 0.9942 - recall_16: 0.8730 - val_loss: 0.0713 - val_acc: 0.9718 - val_precision_16: 0.9291 - val_auc_16: 0.9927 - val_recall_16: 0.8943
Epoch 86/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0647 - acc: 0.9598 - precision_16: 0.9485 - auc_16: 0.9929 - recall_16: 0.8623 - val_loss: 0.0645 - val_acc: 0.9743 - val_precision_16: 0.9160 - val_auc_16: 0.9939 - val_recall_16: 0.9201
Epoch 87/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0672 - acc: 0.9599 - precision_16: 0.9464 - auc_16: 0.9923 - recall_16: 0.8685 - val_loss: 0.0673 - val_acc: 0.9743 - val_precision_16: 0.9385 - val_auc_16: 0.9930 - val_recall_16: 0.8995
Epoch 88/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0621 - acc: 0.9602 - precision_16: 0.9484 - auc_16: 0.9942 - recall_16: 0.8683 - val_loss: 0.0725 - val_acc: 0.9711 - val_precision_16: 0.9005 - val_auc_16: 0.9938 - val_recall_16: 0.9305
Epoch 89/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0790 - acc: 0.9558 - precision_16: 0.9343 - auc_16: 0.9892 - recall_16: 0.8490 - val_loss: 0.0694 - val_acc: 0.9723 - val_precision_16: 0.9008 - val_auc_16: 0.9937 - val_recall_16: 0.9261
Epoch 90/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0674 - acc: 0.9585 - precision_16: 0.9440 - auc_16: 0.9930 - recall_16: 0.8681 - val_loss: 0.0684 - val_acc: 0.9735 - val_precision_16: 0.9240 - val_auc_16: 0.9934 - val_recall_16: 0.9000
Epoch 91/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0658 - acc: 0.9591 - precision_16: 0.9453 - auc_16: 0.9931 - recall_16: 0.8616 - val_loss: 0.0789 - val_acc: 0.9681 - val_precision_16: 0.8910 - val_auc_16: 0.9934 - val_recall_16: 0.9218
Epoch 92/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0963 - acc: 0.9523 - precision_16: 0.9189 - auc_16: 0.9838 - recall_16: 0.8459 - val_loss: 0.0702 - val_acc: 0.9719 - val_precision_16: 0.9102 - val_auc_16: 0.9933 - val_recall_16: 0.9083
Epoch 93/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0724 - acc: 0.9587 - precision_16: 0.9428 - auc_16: 0.9909 - recall_16: 0.8643 - val_loss: 0.0660 - val_acc: 0.9740 - val_precision_16: 0.9247 - val_auc_16: 0.9938 - val_recall_16: 0.9178
Epoch 94/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0680 - acc: 0.9587 - precision_16: 0.9456 - auc_16: 0.9924 - recall_16: 0.8618 - val_loss: 0.0672 - val_acc: 0.9730 - val_precision_16: 0.9154 - val_auc_16: 0.9941 - val_recall_16: 0.9209
Epoch 95/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0623 - acc: 0.9601 - precision_16: 0.9489 - auc_16: 0.9940 - recall_16: 0.8682 - val_loss: 0.0645 - val_acc: 0.9747 - val_precision_16: 0.8985 - val_auc_16: 0.9948 - val_recall_16: 0.9340
Epoch 96/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0706 - acc: 0.9593 - precision_16: 0.9480 - auc_16: 0.9906 - recall_16: 0.8592 - val_loss: 0.0674 - val_acc: 0.9726 - val_precision_16: 0.8941 - val_auc_16: 0.9942 - val_recall_16: 0.9334
Epoch 97/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0616 - acc: 0.9605 - precision_16: 0.9487 - auc_16: 0.9939 - recall_16: 0.8690 - val_loss: 0.0636 - val_acc: 0.9748 - val_precision_16: 0.9169 - val_auc_16: 0.9942 - val_recall_16: 0.9236
Epoch 98/100
32/33 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9597 - precision_16: 0.9471 - auc_16: 0.9913 - recall_16: 0.8631Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 87ms/step - loss: 0.0686 - acc: 0.9596 - precision_16: 0.9466 - auc_16: 0.9913 - recall_16: 0.8638 - val_loss: 0.0665 - val_acc: 0.9739 - val_precision_16: 0.9182 - val_auc_16: 0.9929 - val_recall_16: 0.9113
Epoch 00098: early stopping
6/6 [==============================] - 0s 25ms/step - loss: 0.0646 - acc: 0.9742 - precision_16: 0.9175 - auc_16: 0.9948 - recall_16: 0.9240
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbe007224c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8456627096225466
test_sensitivity 0.9314579468925988
test_specifitivity 0.9824303187510323
test_accuracy 0.9741560194227431
test_precision 0.911297129275585
test_jaccard_score 0.8456627096225466
test_dicecoef 0.921267252519764
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-111627.h5
[0.9706682  0.83531871 0.88762237 0.93807131 0.97698503] [0.97415602 0.84566271 0.91129713 0.93145795 0.98243032]

-------------------------
Rep: 2
-------------------------

2021-09-25 11:16:28.520850: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:16:28.520905: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied histogram equalization on all input images
Applied normalization
Data augumentation on
Prep done
Training samples: 258, channel mean: 0.1526593703632206,
Validation samples: 63, channel mean: 0.14989169631611324
Model built.
Model: "functional_35"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_18 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_323 (Conv2D)             (None, 256, 256, 32) 320         input_18[0][0]
__________________________________________________________________________________________________
conv2d_324 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_323[0][0]
__________________________________________________________________________________________________
max_pooling2d_68 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_324[0][0]
__________________________________________________________________________________________________
conv2d_325 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_68[0][0]
__________________________________________________________________________________________________
conv2d_326 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_325[0][0]
__________________________________________________________________________________________________
max_pooling2d_69 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_326[0][0]
__________________________________________________________________________________________________
conv2d_327 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_69[0][0]
__________________________________________________________________________________________________
conv2d_328 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_327[0][0]
__________________________________________________________________________________________________
max_pooling2d_70 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_328[0][0]
__________________________________________________________________________________________________
conv2d_329 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_70[0][0]
__________________________________________________________________________________________________
conv2d_330 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_329[0][0]
__________________________________________________________________________________________________
max_pooling2d_71 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_330[0][0]
__________________________________________________________________________________________________
conv2d_331 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_71[0][0]
__________________________________________________________________________________________________
conv2d_332 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_331[0][0]
__________________________________________________________________________________________________
up_sampling2d_68 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_332[0][0]
__________________________________________________________________________________________________
concatenate_68 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_68[0][0]
                                                                 conv2d_330[0][0]
__________________________________________________________________________________________________
conv2d_333 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_68[0][0]
__________________________________________________________________________________________________
conv2d_334 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_333[0][0]
__________________________________________________________________________________________________
up_sampling2d_69 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_334[0][0]
__________________________________________________________________________________________________
concatenate_69 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_69[0][0]
                                                                 conv2d_328[0][0]
__________________________________________________________________________________________________
conv2d_335 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_69[0][0]
__________________________________________________________________________________________________
conv2d_336 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_335[0][0]
__________________________________________________________________________________________________
up_sampling2d_70 (UpSampling2D) (None, 128, 128, 128 0           conv2d_336[0][0]
__________________________________________________________________________________________________
concatenate_70 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_70[0][0]
                                                                 conv2d_326[0][0]
__________________________________________________________________________________________________
conv2d_337 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_70[0][0]
__________________________________________________________________________________________________
conv2d_338 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_337[0][0]
__________________________________________________________________________________________________
up_sampling2d_71 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_338[0][0]
__________________________________________________________________________________________________
concatenate_71 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_71[0][0]
                                                                 conv2d_324[0][0]
__________________________________________________________________________________________________
conv2d_339 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_71[0][0]
__________________________________________________________________________________________________
conv2d_340 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_339[0][0]
__________________________________________________________________________________________________
conv2d_341 (Conv2D)             (None, 256, 256, 1)  33          conv2d_340[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6953 - acc: 0.8164 - precision_17: 0.0127 - auc_17: 0.0849 - recall_17: 0.00272021-09-25 11:16:33.508831: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:16:33.508902: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 11:16:33.785992: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 11:16:33.795292: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-111628/train/plugins/profile/2021_09_25_11_16_33
2021-09-25 11:16:33.799301: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-111628/train/plugins/profile/2021_09_25_11_16_33/xeon-e-v100.trace.json.gz
2021-09-25 11:16:33.818532: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-111628/train/plugins/profile/2021_09_25_11_16_33
2021-09-25 11:16:33.824908: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-111628/train/plugins/profile/2021_09_25_11_16_33/xeon-e-v100.memory_profile.json.gz
2021-09-25 11:16:33.843968: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-111628/train/plugins/profile/2021_09_25_11_16_33Dumped tool data for xplane.pb to logs/fit/20210925-111628/train/plugins/profile/2021_09_25_11_16_33/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-111628/train/plugins/profile/2021_09_25_11_16_33/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-111628/train/plugins/profile/2021_09_25_11_16_33/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-111628/train/plugins/profile/2021_09_25_11_16_33/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-111628/train/plugins/profile/2021_09_25_11_16_33/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6920 - acc: 0.8081 - precision_17: 0.3690 - auc_17: 0.2686 - recall_17: 0.0591WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0688s vs `on_train_batch_end` time: 0.2678s). Check your callbacks.
33/33 [==============================] - 4s 107ms/step - loss: 0.4589 - acc: 0.8751 - precision_17: 0.7495 - auc_17: 0.7464 - recall_17: 0.3849 - val_loss: 0.2575 - val_acc: 0.8914 - val_precision_17: 0.6370 - val_auc_17: 0.9168 - val_recall_17: 0.7211
Epoch 2/100
33/33 [==============================] - 3s 87ms/step - loss: 0.2215 - acc: 0.8992 - precision_17: 0.7741 - auc_17: 0.9291 - recall_17: 0.6261 - val_loss: 0.2030 - val_acc: 0.9147 - val_precision_17: 0.7059 - val_auc_17: 0.9491 - val_recall_17: 0.7478
Epoch 3/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1911 - acc: 0.9099 - precision_17: 0.8083 - auc_17: 0.9491 - recall_17: 0.6739 - val_loss: 0.2085 - val_acc: 0.9184 - val_precision_17: 0.8262 - val_auc_17: 0.9470 - val_recall_17: 0.5972
Epoch 4/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1908 - acc: 0.9087 - precision_17: 0.8052 - auc_17: 0.9500 - recall_17: 0.6601 - val_loss: 0.2250 - val_acc: 0.9053 - val_precision_17: 0.7029 - val_auc_17: 0.9410 - val_recall_17: 0.7610
Epoch 5/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1821 - acc: 0.9133 - precision_17: 0.8037 - auc_17: 0.9534 - recall_17: 0.7068 - val_loss: 0.1628 - val_acc: 0.9317 - val_precision_17: 0.7932 - val_auc_17: 0.9656 - val_recall_17: 0.7548
Epoch 6/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1674 - acc: 0.9185 - precision_17: 0.8240 - auc_17: 0.9612 - recall_17: 0.7026 - val_loss: 0.1629 - val_acc: 0.9371 - val_precision_17: 0.8321 - val_auc_17: 0.9642 - val_recall_17: 0.7539
Epoch 7/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1570 - acc: 0.9234 - precision_17: 0.8327 - auc_17: 0.9658 - recall_17: 0.7369 - val_loss: 0.1397 - val_acc: 0.9461 - val_precision_17: 0.8637 - val_auc_17: 0.9746 - val_recall_17: 0.7608
Epoch 8/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1583 - acc: 0.9263 - precision_17: 0.8460 - auc_17: 0.9633 - recall_17: 0.7379 - val_loss: 0.1607 - val_acc: 0.9398 - val_precision_17: 0.8623 - val_auc_17: 0.9683 - val_recall_17: 0.7503
Epoch 9/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1434 - acc: 0.9309 - precision_17: 0.8540 - auc_17: 0.9698 - recall_17: 0.7632 - val_loss: 0.1551 - val_acc: 0.9414 - val_precision_17: 0.7770 - val_auc_17: 0.9784 - val_recall_17: 0.8804
Epoch 10/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1431 - acc: 0.9317 - precision_17: 0.8467 - auc_17: 0.9697 - recall_17: 0.7784 - val_loss: 0.1104 - val_acc: 0.9568 - val_precision_17: 0.8713 - val_auc_17: 0.9846 - val_recall_17: 0.8476
Epoch 11/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1225 - acc: 0.9381 - precision_17: 0.8762 - auc_17: 0.9772 - recall_17: 0.7883 - val_loss: 0.1190 - val_acc: 0.9536 - val_precision_17: 0.8207 - val_auc_17: 0.9854 - val_recall_17: 0.9072
Epoch 12/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1357 - acc: 0.9382 - precision_17: 0.8677 - auc_17: 0.9704 - recall_17: 0.7990 - val_loss: 0.1053 - val_acc: 0.9633 - val_precision_17: 0.8929 - val_auc_17: 0.9871 - val_recall_17: 0.8478
Epoch 13/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1236 - acc: 0.9396 - precision_17: 0.8758 - auc_17: 0.9765 - recall_17: 0.7977 - val_loss: 0.1085 - val_acc: 0.9573 - val_precision_17: 0.8221 - val_auc_17: 0.9875 - val_recall_17: 0.9178
Epoch 14/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1118 - acc: 0.9427 - precision_17: 0.8862 - auc_17: 0.9818 - recall_17: 0.8110 - val_loss: 0.0949 - val_acc: 0.9626 - val_precision_17: 0.8871 - val_auc_17: 0.9895 - val_recall_17: 0.8776
Epoch 15/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0953 - acc: 0.9483 - precision_17: 0.9108 - auc_17: 0.9868 - recall_17: 0.8219 - val_loss: 0.1052 - val_acc: 0.9590 - val_precision_17: 0.9017 - val_auc_17: 0.9864 - val_recall_17: 0.8409
Epoch 16/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1071 - acc: 0.9486 - precision_17: 0.9116 - auc_17: 0.9809 - recall_17: 0.8222 - val_loss: 0.0893 - val_acc: 0.9662 - val_precision_17: 0.8951 - val_auc_17: 0.9895 - val_recall_17: 0.8862
Epoch 17/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0937 - acc: 0.9509 - precision_17: 0.9195 - auc_17: 0.9858 - recall_17: 0.8274 - val_loss: 0.0854 - val_acc: 0.9670 - val_precision_17: 0.8881 - val_auc_17: 0.9911 - val_recall_17: 0.9043
Epoch 18/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1061 - acc: 0.9473 - precision_17: 0.9052 - auc_17: 0.9807 - recall_17: 0.8199 - val_loss: 0.0984 - val_acc: 0.9618 - val_precision_17: 0.8505 - val_auc_17: 0.9902 - val_recall_17: 0.9295
Epoch 19/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0897 - acc: 0.9504 - precision_17: 0.9128 - auc_17: 0.9880 - recall_17: 0.8350 - val_loss: 0.0829 - val_acc: 0.9671 - val_precision_17: 0.8891 - val_auc_17: 0.9906 - val_recall_17: 0.8804
Epoch 20/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0946 - acc: 0.9498 - precision_17: 0.9167 - auc_17: 0.9857 - recall_17: 0.8237 - val_loss: 0.0793 - val_acc: 0.9698 - val_precision_17: 0.9110 - val_auc_17: 0.9919 - val_recall_17: 0.8923
Epoch 21/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0863 - acc: 0.9517 - precision_17: 0.9182 - auc_17: 0.9890 - recall_17: 0.8405 - val_loss: 0.0688 - val_acc: 0.9736 - val_precision_17: 0.9311 - val_auc_17: 0.9932 - val_recall_17: 0.8843
Epoch 22/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0857 - acc: 0.9534 - precision_17: 0.9273 - auc_17: 0.9883 - recall_17: 0.8383 - val_loss: 0.0783 - val_acc: 0.9694 - val_precision_17: 0.9181 - val_auc_17: 0.9915 - val_recall_17: 0.8733
Epoch 23/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0918 - acc: 0.9523 - precision_17: 0.9184 - auc_17: 0.9860 - recall_17: 0.8424 - val_loss: 0.1042 - val_acc: 0.9571 - val_precision_17: 0.8260 - val_auc_17: 0.9892 - val_recall_17: 0.9353
Epoch 24/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1108 - acc: 0.9486 - precision_17: 0.9138 - auc_17: 0.9787 - recall_17: 0.8245 - val_loss: 0.0778 - val_acc: 0.9694 - val_precision_17: 0.9051 - val_auc_17: 0.9920 - val_recall_17: 0.8832
Epoch 25/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0835 - acc: 0.9530 - precision_17: 0.9261 - auc_17: 0.9894 - recall_17: 0.8422 - val_loss: 0.0787 - val_acc: 0.9698 - val_precision_17: 0.9163 - val_auc_17: 0.9911 - val_recall_17: 0.8890
Epoch 26/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0775 - acc: 0.9552 - precision_17: 0.9305 - auc_17: 0.9910 - recall_17: 0.8529 - val_loss: 0.0800 - val_acc: 0.9686 - val_precision_17: 0.8941 - val_auc_17: 0.9906 - val_recall_17: 0.9056
Epoch 27/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0850 - acc: 0.9522 - precision_17: 0.9204 - auc_17: 0.9886 - recall_17: 0.8396 - val_loss: 0.0654 - val_acc: 0.9739 - val_precision_17: 0.9070 - val_auc_17: 0.9946 - val_recall_17: 0.9239
Epoch 28/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0742 - acc: 0.9560 - precision_17: 0.9347 - auc_17: 0.9916 - recall_17: 0.8498 - val_loss: 0.0722 - val_acc: 0.9728 - val_precision_17: 0.9200 - val_auc_17: 0.9913 - val_recall_17: 0.8921
Epoch 29/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0802 - acc: 0.9552 - precision_17: 0.9299 - auc_17: 0.9899 - recall_17: 0.8509 - val_loss: 0.0851 - val_acc: 0.9670 - val_precision_17: 0.9164 - val_auc_17: 0.9904 - val_recall_17: 0.8856
Epoch 30/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0770 - acc: 0.9557 - precision_17: 0.9370 - auc_17: 0.9908 - recall_17: 0.8455 - val_loss: 0.0732 - val_acc: 0.9716 - val_precision_17: 0.9017 - val_auc_17: 0.9927 - val_recall_17: 0.9176
Epoch 31/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0845 - acc: 0.9553 - precision_17: 0.9319 - auc_17: 0.9892 - recall_17: 0.8518 - val_loss: 0.0732 - val_acc: 0.9713 - val_precision_17: 0.9312 - val_auc_17: 0.9931 - val_recall_17: 0.8903
Epoch 32/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0767 - acc: 0.9568 - precision_17: 0.9370 - auc_17: 0.9904 - recall_17: 0.8543 - val_loss: 0.0793 - val_acc: 0.9681 - val_precision_17: 0.9187 - val_auc_17: 0.9912 - val_recall_17: 0.8670
Epoch 33/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0699 - acc: 0.9576 - precision_17: 0.9422 - auc_17: 0.9926 - recall_17: 0.8561 - val_loss: 0.0729 - val_acc: 0.9717 - val_precision_17: 0.9057 - val_auc_17: 0.9934 - val_recall_17: 0.9157
Epoch 34/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0956 - acc: 0.9526 - precision_17: 0.9220 - auc_17: 0.9848 - recall_17: 0.8409 - val_loss: 0.0925 - val_acc: 0.9646 - val_precision_17: 0.8506 - val_auc_17: 0.9916 - val_recall_17: 0.9403
Epoch 35/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0797 - acc: 0.9555 - precision_17: 0.9299 - auc_17: 0.9890 - recall_17: 0.8547 - val_loss: 0.0800 - val_acc: 0.9678 - val_precision_17: 0.8875 - val_auc_17: 0.9935 - val_recall_17: 0.9358
Epoch 36/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0708 - acc: 0.9574 - precision_17: 0.9392 - auc_17: 0.9924 - recall_17: 0.8570 - val_loss: 0.0660 - val_acc: 0.9743 - val_precision_17: 0.9281 - val_auc_17: 0.9933 - val_recall_17: 0.9078
Epoch 37/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0695 - acc: 0.9587 - precision_17: 0.9443 - auc_17: 0.9924 - recall_17: 0.8587 - val_loss: 0.0688 - val_acc: 0.9734 - val_precision_17: 0.9038 - val_auc_17: 0.9931 - val_recall_17: 0.9131
Epoch 38/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0787 - acc: 0.9570 - precision_17: 0.9340 - auc_17: 0.9897 - recall_17: 0.8573 - val_loss: 0.0722 - val_acc: 0.9710 - val_precision_17: 0.9261 - val_auc_17: 0.9934 - val_recall_17: 0.8932
Epoch 39/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0775 - acc: 0.9574 - precision_17: 0.9385 - auc_17: 0.9887 - recall_17: 0.8557 - val_loss: 0.0729 - val_acc: 0.9725 - val_precision_17: 0.9324 - val_auc_17: 0.9924 - val_recall_17: 0.8935
Epoch 40/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0700 - acc: 0.9590 - precision_17: 0.9453 - auc_17: 0.9922 - recall_17: 0.8631 - val_loss: 0.0679 - val_acc: 0.9727 - val_precision_17: 0.9006 - val_auc_17: 0.9937 - val_recall_17: 0.9264
Epoch 41/100
32/33 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9563 - precision_17: 0.9332 - auc_17: 0.9866 - recall_17: 0.8496Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 86ms/step - loss: 0.0862 - acc: 0.9562 - precision_17: 0.9326 - auc_17: 0.9866 - recall_17: 0.8492 - val_loss: 0.0766 - val_acc: 0.9713 - val_precision_17: 0.9188 - val_auc_17: 0.9909 - val_recall_17: 0.8972
Epoch 00041: early stopping
6/6 [==============================] - 0s 26ms/step - loss: 0.0781 - acc: 0.9681 - precision_17: 0.8936 - auc_17: 0.9928 - recall_17: 0.9123
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb6157ea670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8238784301357999
test_sensitivity 0.9310777727644926
test_specifitivity 0.9764737742026367
test_accuracy 0.9691046820746527
test_precision 0.8846506513817549
test_jaccard_score 0.8238784301357999
test_dicecoef 0.9072706548067333
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-111846.h5
[1.94482422 1.68098142 1.7989195  1.86952925 1.95941535] [0.96910468 0.82387843 0.88465065 0.93107777 0.97647377]

-------------------------
Averaged metrics for Baseline + Augumentations + Histogram Equalization - bacteria: [0.97130963 0.83495328 0.89452338 0.93353568 0.97862971]
-------------------------


-------------------------
RUN: Baseline + Augumentations + Per Channel Normalization - bacteria, PARAMS: {'augumentation': True, 'per_channel_normalization': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-25 11:18:47.355039: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:18:47.355095: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Normalized per channel
Applied normalization
Data augumentation on
Prep done
Training samples: 258, channel mean: -1.389123527662048e-17,
Validation samples: 63, channel mean: -0.0011996397803231908
Model built.
Model: "functional_37"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_19 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_342 (Conv2D)             (None, 256, 256, 32) 320         input_19[0][0]
__________________________________________________________________________________________________
conv2d_343 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_342[0][0]
__________________________________________________________________________________________________
max_pooling2d_72 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_343[0][0]
__________________________________________________________________________________________________
conv2d_344 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_72[0][0]
__________________________________________________________________________________________________
conv2d_345 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_344[0][0]
__________________________________________________________________________________________________
max_pooling2d_73 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_345[0][0]
__________________________________________________________________________________________________
conv2d_346 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_73[0][0]
__________________________________________________________________________________________________
conv2d_347 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_346[0][0]
__________________________________________________________________________________________________
max_pooling2d_74 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_347[0][0]
__________________________________________________________________________________________________
conv2d_348 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_74[0][0]
__________________________________________________________________________________________________
conv2d_349 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_348[0][0]
__________________________________________________________________________________________________
max_pooling2d_75 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_349[0][0]
__________________________________________________________________________________________________
conv2d_350 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_75[0][0]
__________________________________________________________________________________________________
conv2d_351 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_350[0][0]
__________________________________________________________________________________________________
up_sampling2d_72 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_351[0][0]
__________________________________________________________________________________________________
concatenate_72 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_72[0][0]
                                                                 conv2d_349[0][0]
__________________________________________________________________________________________________
conv2d_352 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_72[0][0]
__________________________________________________________________________________________________
conv2d_353 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_352[0][0]
__________________________________________________________________________________________________
up_sampling2d_73 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_353[0][0]
__________________________________________________________________________________________________
concatenate_73 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_73[0][0]
                                                                 conv2d_347[0][0]
__________________________________________________________________________________________________
conv2d_354 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_73[0][0]
__________________________________________________________________________________________________
conv2d_355 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_354[0][0]
__________________________________________________________________________________________________
up_sampling2d_74 (UpSampling2D) (None, 128, 128, 128 0           conv2d_355[0][0]
__________________________________________________________________________________________________
concatenate_74 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_74[0][0]
                                                                 conv2d_345[0][0]
__________________________________________________________________________________________________
conv2d_356 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_74[0][0]
__________________________________________________________________________________________________
conv2d_357 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_356[0][0]
__________________________________________________________________________________________________
up_sampling2d_75 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_357[0][0]
__________________________________________________________________________________________________
concatenate_75 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_75[0][0]
                                                                 conv2d_343[0][0]
__________________________________________________________________________________________________
conv2d_358 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_75[0][0]
__________________________________________________________________________________________________
conv2d_359 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_358[0][0]
__________________________________________________________________________________________________
conv2d_360 (Conv2D)             (None, 256, 256, 1)  33          conv2d_359[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6924 - acc: 0.7994 - precision_18: 0.1707 - auc_18: 0.2676 - recall_18: 0.07662021-09-25 11:18:49.752208: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:18:49.752268: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 11:18:50.024394: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 11:18:50.033482: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-111847/train/plugins/profile/2021_09_25_11_18_50
2021-09-25 11:18:50.037283: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-111847/train/plugins/profile/2021_09_25_11_18_50/xeon-e-v100.trace.json.gz
2021-09-25 11:18:50.056610: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-111847/train/plugins/profile/2021_09_25_11_18_50
2021-09-25 11:18:50.064767: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-111847/train/plugins/profile/2021_09_25_11_18_50/xeon-e-v100.memory_profile.json.gz
2021-09-25 11:18:50.085951: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-111847/train/plugins/profile/2021_09_25_11_18_50Dumped tool data for xplane.pb to logs/fit/20210925-111847/train/plugins/profile/2021_09_25_11_18_50/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-111847/train/plugins/profile/2021_09_25_11_18_50/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-111847/train/plugins/profile/2021_09_25_11_18_50/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-111847/train/plugins/profile/2021_09_25_11_18_50/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-111847/train/plugins/profile/2021_09_25_11_18_50/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6888 - acc: 0.8237 - precision_18: 0.5669 - auc_18: 0.5012 - recall_18: 0.2107WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0662s vs `on_train_batch_end` time: 0.2689s). Check your callbacks.
33/33 [==============================] - 4s 106ms/step - loss: 0.4821 - acc: 0.8683 - precision_18: 0.8190 - auc_18: 0.7297 - recall_18: 0.2446 - val_loss: 0.3065 - val_acc: 0.8899 - val_precision_18: 0.6355 - val_auc_18: 0.8929 - val_recall_18: 0.7050
Epoch 2/100
33/33 [==============================] - 3s 88ms/step - loss: 0.2571 - acc: 0.8956 - precision_18: 0.7630 - auc_18: 0.8894 - recall_18: 0.6091 - val_loss: 0.2572 - val_acc: 0.9111 - val_precision_18: 0.7514 - val_auc_18: 0.9089 - val_recall_18: 0.6169
Epoch 3/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2302 - acc: 0.9022 - precision_18: 0.7788 - auc_18: 0.9117 - recall_18: 0.6542 - val_loss: 0.2440 - val_acc: 0.9078 - val_precision_18: 0.8497 - val_auc_18: 0.9139 - val_recall_18: 0.4899
Epoch 4/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2342 - acc: 0.9010 - precision_18: 0.7959 - auc_18: 0.9014 - recall_18: 0.6077 - val_loss: 0.2740 - val_acc: 0.8951 - val_precision_18: 0.7229 - val_auc_18: 0.9008 - val_recall_18: 0.6150
Epoch 5/100
33/33 [==============================] - 3s 87ms/step - loss: 0.2145 - acc: 0.9066 - precision_18: 0.8207 - auc_18: 0.9197 - recall_18: 0.6228 - val_loss: 0.2105 - val_acc: 0.9165 - val_precision_18: 0.7492 - val_auc_18: 0.9399 - val_recall_18: 0.6906
Epoch 6/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2029 - acc: 0.9117 - precision_18: 0.8418 - auc_18: 0.9260 - recall_18: 0.6190 - val_loss: 0.2126 - val_acc: 0.9200 - val_precision_18: 0.7635 - val_auc_18: 0.9280 - val_recall_18: 0.7147
Epoch 7/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2198 - acc: 0.9056 - precision_18: 0.8036 - auc_18: 0.9139 - recall_18: 0.6370 - val_loss: 0.2163 - val_acc: 0.9144 - val_precision_18: 0.7195 - val_auc_18: 0.9328 - val_recall_18: 0.7040
Epoch 8/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2070 - acc: 0.9075 - precision_18: 0.8212 - auc_18: 0.9297 - recall_18: 0.6263 - val_loss: 0.3231 - val_acc: 0.9102 - val_precision_18: 0.7775 - val_auc_18: 0.8977 - val_recall_18: 0.6278
Epoch 9/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2170 - acc: 0.9088 - precision_18: 0.8253 - auc_18: 0.9203 - recall_18: 0.6347 - val_loss: 0.2544 - val_acc: 0.9031 - val_precision_18: 0.6680 - val_auc_18: 0.9312 - val_recall_18: 0.7641
Epoch 10/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2143 - acc: 0.9092 - precision_18: 0.8240 - auc_18: 0.9215 - recall_18: 0.6368 - val_loss: 0.2178 - val_acc: 0.9167 - val_precision_18: 0.8089 - val_auc_18: 0.9295 - val_recall_18: 0.6078
Epoch 11/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2189 - acc: 0.9074 - precision_18: 0.8270 - auc_18: 0.9152 - recall_18: 0.6232 - val_loss: 0.2331 - val_acc: 0.9082 - val_precision_18: 0.7548 - val_auc_18: 0.9254 - val_recall_18: 0.6292
Epoch 12/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2021 - acc: 0.9107 - precision_18: 0.8331 - auc_18: 0.9307 - recall_18: 0.6323 - val_loss: 0.2235 - val_acc: 0.9246 - val_precision_18: 0.8245 - val_auc_18: 0.9234 - val_recall_18: 0.6073
Epoch 13/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1998 - acc: 0.9127 - precision_18: 0.8487 - auc_18: 0.9330 - recall_18: 0.6307 - val_loss: 0.2175 - val_acc: 0.9119 - val_precision_18: 0.7256 - val_auc_18: 0.9348 - val_recall_18: 0.6772
Epoch 14/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1926 - acc: 0.9146 - precision_18: 0.8310 - auc_18: 0.9415 - recall_18: 0.6747 - val_loss: 0.2115 - val_acc: 0.9197 - val_precision_18: 0.8462 - val_auc_18: 0.9347 - val_recall_18: 0.6077
Epoch 15/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1936 - acc: 0.9156 - precision_18: 0.8380 - auc_18: 0.9328 - recall_18: 0.6719 - val_loss: 0.2148 - val_acc: 0.9184 - val_precision_18: 0.7745 - val_auc_18: 0.9347 - val_recall_18: 0.7068
Epoch 16/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1731 - acc: 0.9229 - precision_18: 0.8384 - auc_18: 0.9471 - recall_18: 0.7198 - val_loss: 0.1760 - val_acc: 0.9358 - val_precision_18: 0.8479 - val_auc_18: 0.9576 - val_recall_18: 0.7145
Epoch 17/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1628 - acc: 0.9257 - precision_18: 0.8484 - auc_18: 0.9535 - recall_18: 0.7305 - val_loss: 0.1512 - val_acc: 0.9429 - val_precision_18: 0.8205 - val_auc_18: 0.9664 - val_recall_18: 0.8159
Epoch 18/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1497 - acc: 0.9313 - precision_18: 0.8574 - auc_18: 0.9601 - recall_18: 0.7617 - val_loss: 0.1454 - val_acc: 0.9441 - val_precision_18: 0.8313 - val_auc_18: 0.9725 - val_recall_18: 0.8245
Epoch 19/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1398 - acc: 0.9340 - precision_18: 0.8686 - auc_18: 0.9677 - recall_18: 0.7708 - val_loss: 0.1184 - val_acc: 0.9551 - val_precision_18: 0.8871 - val_auc_18: 0.9807 - val_recall_18: 0.7870
Epoch 20/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1466 - acc: 0.9314 - precision_18: 0.8734 - auc_18: 0.9647 - recall_18: 0.7406 - val_loss: 0.1280 - val_acc: 0.9564 - val_precision_18: 0.8864 - val_auc_18: 0.9729 - val_recall_18: 0.8240
Epoch 21/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1432 - acc: 0.9356 - precision_18: 0.8810 - auc_18: 0.9608 - recall_18: 0.7630 - val_loss: 0.1060 - val_acc: 0.9617 - val_precision_18: 0.9027 - val_auc_18: 0.9827 - val_recall_18: 0.8263
Epoch 22/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1355 - acc: 0.9396 - precision_18: 0.8935 - auc_18: 0.9671 - recall_18: 0.7801 - val_loss: 0.1094 - val_acc: 0.9610 - val_precision_18: 0.8889 - val_auc_18: 0.9833 - val_recall_18: 0.8445
Epoch 23/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1092 - acc: 0.9439 - precision_18: 0.8919 - auc_18: 0.9819 - recall_18: 0.8158 - val_loss: 0.1095 - val_acc: 0.9559 - val_precision_18: 0.8318 - val_auc_18: 0.9866 - val_recall_18: 0.9163
Epoch 24/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1257 - acc: 0.9425 - precision_18: 0.8963 - auc_18: 0.9709 - recall_18: 0.8041 - val_loss: 0.0951 - val_acc: 0.9628 - val_precision_18: 0.8463 - val_auc_18: 0.9891 - val_recall_18: 0.9100
Epoch 25/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1098 - acc: 0.9433 - precision_18: 0.9048 - auc_18: 0.9813 - recall_18: 0.7988 - val_loss: 0.1028 - val_acc: 0.9596 - val_precision_18: 0.8440 - val_auc_18: 0.9887 - val_recall_18: 0.9116
Epoch 26/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1102 - acc: 0.9437 - precision_18: 0.8924 - auc_18: 0.9812 - recall_18: 0.8188 - val_loss: 0.0903 - val_acc: 0.9653 - val_precision_18: 0.8937 - val_auc_18: 0.9887 - val_recall_18: 0.8825
Epoch 27/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0943 - acc: 0.9482 - precision_18: 0.9074 - auc_18: 0.9866 - recall_18: 0.8311 - val_loss: 0.0767 - val_acc: 0.9696 - val_precision_18: 0.8869 - val_auc_18: 0.9924 - val_recall_18: 0.9183
Epoch 28/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1029 - acc: 0.9476 - precision_18: 0.9087 - auc_18: 0.9831 - recall_18: 0.8223 - val_loss: 0.0813 - val_acc: 0.9682 - val_precision_18: 0.8977 - val_auc_18: 0.9911 - val_recall_18: 0.8840
Epoch 29/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1049 - acc: 0.9477 - precision_18: 0.9043 - auc_18: 0.9821 - recall_18: 0.8266 - val_loss: 0.0894 - val_acc: 0.9650 - val_precision_18: 0.8985 - val_auc_18: 0.9900 - val_recall_18: 0.8938
Epoch 30/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0859 - acc: 0.9521 - precision_18: 0.9235 - auc_18: 0.9884 - recall_18: 0.8364 - val_loss: 0.0776 - val_acc: 0.9696 - val_precision_18: 0.8869 - val_auc_18: 0.9926 - val_recall_18: 0.9228
Epoch 31/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0927 - acc: 0.9506 - precision_18: 0.9114 - auc_18: 0.9868 - recall_18: 0.8472 - val_loss: 0.0806 - val_acc: 0.9681 - val_precision_18: 0.9172 - val_auc_18: 0.9916 - val_recall_18: 0.8850
Epoch 32/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0889 - acc: 0.9513 - precision_18: 0.9196 - auc_18: 0.9876 - recall_18: 0.8358 - val_loss: 0.0874 - val_acc: 0.9657 - val_precision_18: 0.8980 - val_auc_18: 0.9892 - val_recall_18: 0.8736
Epoch 33/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0951 - acc: 0.9508 - precision_18: 0.9199 - auc_18: 0.9844 - recall_18: 0.8369 - val_loss: 0.0762 - val_acc: 0.9703 - val_precision_18: 0.9081 - val_auc_18: 0.9926 - val_recall_18: 0.9027
Epoch 34/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0933 - acc: 0.9504 - precision_18: 0.9139 - auc_18: 0.9863 - recall_18: 0.8376 - val_loss: 0.0791 - val_acc: 0.9683 - val_precision_18: 0.8969 - val_auc_18: 0.9917 - val_recall_18: 0.9023
Epoch 35/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0897 - acc: 0.9519 - precision_18: 0.9194 - auc_18: 0.9868 - recall_18: 0.8430 - val_loss: 0.0823 - val_acc: 0.9672 - val_precision_18: 0.8904 - val_auc_18: 0.9926 - val_recall_18: 0.9283
Epoch 36/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1000 - acc: 0.9494 - precision_18: 0.9156 - auc_18: 0.9845 - recall_18: 0.8300 - val_loss: 0.0753 - val_acc: 0.9706 - val_precision_18: 0.9193 - val_auc_18: 0.9921 - val_recall_18: 0.8926
Epoch 37/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0847 - acc: 0.9532 - precision_18: 0.9234 - auc_18: 0.9886 - recall_18: 0.8438 - val_loss: 0.0810 - val_acc: 0.9677 - val_precision_18: 0.8713 - val_auc_18: 0.9912 - val_recall_18: 0.9109
Epoch 38/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0915 - acc: 0.9516 - precision_18: 0.9190 - auc_18: 0.9866 - recall_18: 0.8343 - val_loss: 0.0800 - val_acc: 0.9679 - val_precision_18: 0.9005 - val_auc_18: 0.9922 - val_recall_18: 0.9029
Epoch 39/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0828 - acc: 0.9547 - precision_18: 0.9292 - auc_18: 0.9884 - recall_18: 0.8506 - val_loss: 0.0881 - val_acc: 0.9661 - val_precision_18: 0.9322 - val_auc_18: 0.9904 - val_recall_18: 0.8513
Epoch 40/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1018 - acc: 0.9507 - precision_18: 0.9198 - auc_18: 0.9815 - recall_18: 0.8334 - val_loss: 0.0719 - val_acc: 0.9714 - val_precision_18: 0.9050 - val_auc_18: 0.9930 - val_recall_18: 0.9116
Epoch 41/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0804 - acc: 0.9541 - precision_18: 0.9238 - auc_18: 0.9900 - recall_18: 0.8482 - val_loss: 0.0801 - val_acc: 0.9693 - val_precision_18: 0.9205 - val_auc_18: 0.9903 - val_recall_18: 0.8808
Epoch 42/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0823 - acc: 0.9543 - precision_18: 0.9290 - auc_18: 0.9892 - recall_18: 0.8479 - val_loss: 0.0676 - val_acc: 0.9738 - val_precision_18: 0.9155 - val_auc_18: 0.9938 - val_recall_18: 0.9163
Epoch 43/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0739 - acc: 0.9566 - precision_18: 0.9356 - auc_18: 0.9914 - recall_18: 0.8550 - val_loss: 0.0761 - val_acc: 0.9700 - val_precision_18: 0.9017 - val_auc_18: 0.9926 - val_recall_18: 0.9138
Epoch 44/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0724 - acc: 0.9567 - precision_18: 0.9356 - auc_18: 0.9919 - recall_18: 0.8588 - val_loss: 0.0678 - val_acc: 0.9730 - val_precision_18: 0.9053 - val_auc_18: 0.9942 - val_recall_18: 0.9165
Epoch 45/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0815 - acc: 0.9543 - precision_18: 0.9296 - auc_18: 0.9891 - recall_18: 0.8470 - val_loss: 0.0707 - val_acc: 0.9723 - val_precision_18: 0.9112 - val_auc_18: 0.9930 - val_recall_18: 0.9079
Epoch 46/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0888 - acc: 0.9543 - precision_18: 0.9330 - auc_18: 0.9862 - recall_18: 0.8398 - val_loss: 0.0772 - val_acc: 0.9695 - val_precision_18: 0.8788 - val_auc_18: 0.9927 - val_recall_18: 0.9209
Epoch 47/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0738 - acc: 0.9560 - precision_18: 0.9334 - auc_18: 0.9918 - recall_18: 0.8569 - val_loss: 0.0805 - val_acc: 0.9702 - val_precision_18: 0.9002 - val_auc_18: 0.9916 - val_recall_18: 0.9201
Epoch 48/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0778 - acc: 0.9564 - precision_18: 0.9343 - auc_18: 0.9904 - recall_18: 0.8591 - val_loss: 0.0675 - val_acc: 0.9732 - val_precision_18: 0.9055 - val_auc_18: 0.9944 - val_recall_18: 0.9246
Epoch 49/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0815 - acc: 0.9556 - precision_18: 0.9318 - auc_18: 0.9889 - recall_18: 0.8495 - val_loss: 0.0767 - val_acc: 0.9695 - val_precision_18: 0.8887 - val_auc_18: 0.9923 - val_recall_18: 0.9215
Epoch 50/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0819 - acc: 0.9541 - precision_18: 0.9279 - auc_18: 0.9889 - recall_18: 0.8451 - val_loss: 0.0719 - val_acc: 0.9721 - val_precision_18: 0.8999 - val_auc_18: 0.9932 - val_recall_18: 0.9240
Epoch 51/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0832 - acc: 0.9553 - precision_18: 0.9307 - auc_18: 0.9876 - recall_18: 0.8504 - val_loss: 0.0743 - val_acc: 0.9708 - val_precision_18: 0.9200 - val_auc_18: 0.9933 - val_recall_18: 0.8963
Epoch 52/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0708 - acc: 0.9570 - precision_18: 0.9378 - auc_18: 0.9926 - recall_18: 0.8578 - val_loss: 0.0795 - val_acc: 0.9667 - val_precision_18: 0.8706 - val_auc_18: 0.9941 - val_recall_18: 0.9431
Epoch 53/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0805 - acc: 0.9552 - precision_18: 0.9295 - auc_18: 0.9890 - recall_18: 0.8511 - val_loss: 0.0736 - val_acc: 0.9702 - val_precision_18: 0.8512 - val_auc_18: 0.9936 - val_recall_18: 0.9520
Epoch 54/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0759 - acc: 0.9569 - precision_18: 0.9321 - auc_18: 0.9904 - recall_18: 0.8610 - val_loss: 0.0696 - val_acc: 0.9736 - val_precision_18: 0.9133 - val_auc_18: 0.9942 - val_recall_18: 0.9317
Epoch 55/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0712 - acc: 0.9581 - precision_18: 0.9416 - auc_18: 0.9913 - recall_18: 0.8578 - val_loss: 0.0695 - val_acc: 0.9726 - val_precision_18: 0.9145 - val_auc_18: 0.9929 - val_recall_18: 0.8977
Epoch 56/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0700 - acc: 0.9577 - precision_18: 0.9419 - auc_18: 0.9923 - recall_18: 0.8583 - val_loss: 0.0670 - val_acc: 0.9735 - val_precision_18: 0.9143 - val_auc_18: 0.9946 - val_recall_18: 0.9217
Epoch 57/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0791 - acc: 0.9568 - precision_18: 0.9347 - auc_18: 0.9887 - recall_18: 0.8577 - val_loss: 0.0688 - val_acc: 0.9724 - val_precision_18: 0.9083 - val_auc_18: 0.9935 - val_recall_18: 0.9169
Epoch 58/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0735 - acc: 0.9572 - precision_18: 0.9396 - auc_18: 0.9914 - recall_18: 0.8566 - val_loss: 0.0706 - val_acc: 0.9717 - val_precision_18: 0.9010 - val_auc_18: 0.9941 - val_recall_18: 0.9338
Epoch 59/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0739 - acc: 0.9572 - precision_18: 0.9385 - auc_18: 0.9910 - recall_18: 0.8575 - val_loss: 0.0662 - val_acc: 0.9731 - val_precision_18: 0.9075 - val_auc_18: 0.9936 - val_recall_18: 0.9064
Epoch 60/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0747 - acc: 0.9573 - precision_18: 0.9401 - auc_18: 0.9903 - recall_18: 0.8586 - val_loss: 0.0703 - val_acc: 0.9728 - val_precision_18: 0.8983 - val_auc_18: 0.9941 - val_recall_18: 0.9356
Epoch 61/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0765 - acc: 0.9567 - precision_18: 0.9375 - auc_18: 0.9902 - recall_18: 0.8548 - val_loss: 0.0675 - val_acc: 0.9736 - val_precision_18: 0.9204 - val_auc_18: 0.9935 - val_recall_18: 0.9181
Epoch 62/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0761 - acc: 0.9573 - precision_18: 0.9385 - auc_18: 0.9899 - recall_18: 0.8585 - val_loss: 0.0700 - val_acc: 0.9728 - val_precision_18: 0.9149 - val_auc_18: 0.9935 - val_recall_18: 0.9168
Epoch 63/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0728 - acc: 0.9582 - precision_18: 0.9386 - auc_18: 0.9907 - recall_18: 0.8649 - val_loss: 0.0681 - val_acc: 0.9731 - val_precision_18: 0.9144 - val_auc_18: 0.9933 - val_recall_18: 0.9193
Epoch 64/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0738 - acc: 0.9580 - precision_18: 0.9416 - auc_18: 0.9903 - recall_18: 0.8575 - val_loss: 0.0697 - val_acc: 0.9719 - val_precision_18: 0.8792 - val_auc_18: 0.9945 - val_recall_18: 0.9457
Epoch 65/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0913 - acc: 0.9555 - precision_18: 0.9302 - auc_18: 0.9853 - recall_18: 0.8512 - val_loss: 0.0681 - val_acc: 0.9741 - val_precision_18: 0.9291 - val_auc_18: 0.9928 - val_recall_18: 0.9046
Epoch 66/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0838 - acc: 0.9557 - precision_18: 0.9330 - auc_18: 0.9873 - recall_18: 0.8549 - val_loss: 0.0662 - val_acc: 0.9741 - val_precision_18: 0.9227 - val_auc_18: 0.9932 - val_recall_18: 0.9057
Epoch 67/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0807 - acc: 0.9570 - precision_18: 0.9366 - auc_18: 0.9882 - recall_18: 0.8552 - val_loss: 0.0691 - val_acc: 0.9727 - val_precision_18: 0.9013 - val_auc_18: 0.9929 - val_recall_18: 0.9193
Epoch 68/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0740 - acc: 0.9576 - precision_18: 0.9369 - auc_18: 0.9906 - recall_18: 0.8614 - val_loss: 0.0673 - val_acc: 0.9742 - val_precision_18: 0.9180 - val_auc_18: 0.9937 - val_recall_18: 0.9268
Epoch 69/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0757 - acc: 0.9576 - precision_18: 0.9415 - auc_18: 0.9902 - recall_18: 0.8583 - val_loss: 0.0702 - val_acc: 0.9727 - val_precision_18: 0.9438 - val_auc_18: 0.9927 - val_recall_18: 0.8843
Epoch 70/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0677 - acc: 0.9585 - precision_18: 0.9386 - auc_18: 0.9931 - recall_18: 0.8647 - val_loss: 0.0631 - val_acc: 0.9749 - val_precision_18: 0.9214 - val_auc_18: 0.9941 - val_recall_18: 0.9098
Epoch 71/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0794 - acc: 0.9569 - precision_18: 0.9389 - auc_18: 0.9887 - recall_18: 0.8576 - val_loss: 0.0662 - val_acc: 0.9741 - val_precision_18: 0.9120 - val_auc_18: 0.9934 - val_recall_18: 0.9196
Epoch 72/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0639 - acc: 0.9601 - precision_18: 0.9466 - auc_18: 0.9933 - recall_18: 0.8628 - val_loss: 0.0643 - val_acc: 0.9752 - val_precision_18: 0.9245 - val_auc_18: 0.9942 - val_recall_18: 0.9228
Epoch 73/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0793 - acc: 0.9582 - precision_18: 0.9421 - auc_18: 0.9879 - recall_18: 0.8614 - val_loss: 0.0666 - val_acc: 0.9737 - val_precision_18: 0.9259 - val_auc_18: 0.9933 - val_recall_18: 0.9055
Epoch 74/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0664 - acc: 0.9598 - precision_18: 0.9467 - auc_18: 0.9927 - recall_18: 0.8670 - val_loss: 0.0645 - val_acc: 0.9748 - val_precision_18: 0.9273 - val_auc_18: 0.9938 - val_recall_18: 0.9162
Epoch 75/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0624 - acc: 0.9601 - precision_18: 0.9494 - auc_18: 0.9939 - recall_18: 0.8669 - val_loss: 0.0672 - val_acc: 0.9729 - val_precision_18: 0.8927 - val_auc_18: 0.9943 - val_recall_18: 0.9337
Epoch 76/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0743 - acc: 0.9588 - precision_18: 0.9450 - auc_18: 0.9898 - recall_18: 0.8601 - val_loss: 0.0692 - val_acc: 0.9741 - val_precision_18: 0.8997 - val_auc_18: 0.9936 - val_recall_18: 0.9284
Epoch 77/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0662 - acc: 0.9602 - precision_18: 0.9504 - auc_18: 0.9922 - recall_18: 0.8643 - val_loss: 0.0767 - val_acc: 0.9720 - val_precision_18: 0.8819 - val_auc_18: 0.9942 - val_recall_18: 0.9406
Epoch 78/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0988 - acc: 0.9521 - precision_18: 0.9261 - auc_18: 0.9824 - recall_18: 0.8256 - val_loss: 0.0727 - val_acc: 0.9721 - val_precision_18: 0.9197 - val_auc_18: 0.9938 - val_recall_18: 0.9114
Epoch 79/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0874 - acc: 0.9537 - precision_18: 0.9222 - auc_18: 0.9860 - recall_18: 0.8536 - val_loss: 0.0727 - val_acc: 0.9722 - val_precision_18: 0.9401 - val_auc_18: 0.9923 - val_recall_18: 0.8831
Epoch 80/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0779 - acc: 0.9567 - precision_18: 0.9414 - auc_18: 0.9897 - recall_18: 0.8467 - val_loss: 0.0645 - val_acc: 0.9744 - val_precision_18: 0.9151 - val_auc_18: 0.9946 - val_recall_18: 0.9209
Epoch 81/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0625 - acc: 0.9604 - precision_18: 0.9488 - auc_18: 0.9940 - recall_18: 0.8720 - val_loss: 0.0648 - val_acc: 0.9747 - val_precision_18: 0.9113 - val_auc_18: 0.9940 - val_recall_18: 0.9295
Epoch 82/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0659 - acc: 0.9603 - precision_18: 0.9519 - auc_18: 0.9924 - recall_18: 0.8629 - val_loss: 0.0686 - val_acc: 0.9724 - val_precision_18: 0.8911 - val_auc_18: 0.9949 - val_recall_18: 0.9439
Epoch 83/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0730 - acc: 0.9589 - precision_18: 0.9436 - auc_18: 0.9903 - recall_18: 0.8640 - val_loss: 0.0641 - val_acc: 0.9752 - val_precision_18: 0.9273 - val_auc_18: 0.9943 - val_recall_18: 0.9230
Epoch 84/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0936 - acc: 0.9556 - precision_18: 0.9365 - auc_18: 0.9822 - recall_18: 0.8471 - val_loss: 0.0631 - val_acc: 0.9757 - val_precision_18: 0.8928 - val_auc_18: 0.9932 - val_recall_18: 0.9337
Epoch 85/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0640 - acc: 0.9605 - precision_18: 0.9475 - auc_18: 0.9934 - recall_18: 0.8699 - val_loss: 0.0702 - val_acc: 0.9723 - val_precision_18: 0.9311 - val_auc_18: 0.9928 - val_recall_18: 0.8957
Epoch 86/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0785 - acc: 0.9585 - precision_18: 0.9443 - auc_18: 0.9878 - recall_18: 0.8574 - val_loss: 0.0636 - val_acc: 0.9751 - val_precision_18: 0.9212 - val_auc_18: 0.9938 - val_recall_18: 0.9195
Epoch 87/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0667 - acc: 0.9603 - precision_18: 0.9474 - auc_18: 0.9923 - recall_18: 0.8707 - val_loss: 0.0658 - val_acc: 0.9740 - val_precision_18: 0.9324 - val_auc_18: 0.9933 - val_recall_18: 0.9045
Epoch 88/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0643 - acc: 0.9607 - precision_18: 0.9507 - auc_18: 0.9931 - recall_18: 0.8675 - val_loss: 0.0640 - val_acc: 0.9746 - val_precision_18: 0.9275 - val_auc_18: 0.9947 - val_recall_18: 0.9208
Epoch 89/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0723 - acc: 0.9585 - precision_18: 0.9437 - auc_18: 0.9904 - recall_18: 0.8580 - val_loss: 0.0705 - val_acc: 0.9727 - val_precision_18: 0.8993 - val_auc_18: 0.9939 - val_recall_18: 0.9304
Epoch 90/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0863 - acc: 0.9554 - precision_18: 0.9362 - auc_18: 0.9864 - recall_18: 0.8524 - val_loss: 0.0662 - val_acc: 0.9741 - val_precision_18: 0.9206 - val_auc_18: 0.9936 - val_recall_18: 0.9082
Epoch 91/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0854 - acc: 0.9568 - precision_18: 0.9358 - auc_18: 0.9846 - recall_18: 0.8565 - val_loss: 0.0757 - val_acc: 0.9699 - val_precision_18: 0.8950 - val_auc_18: 0.9935 - val_recall_18: 0.9287
Epoch 92/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0746 - acc: 0.9574 - precision_18: 0.9400 - auc_18: 0.9911 - recall_18: 0.8568 - val_loss: 0.0708 - val_acc: 0.9721 - val_precision_18: 0.9184 - val_auc_18: 0.9925 - val_recall_18: 0.8997
Epoch 93/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0702 - acc: 0.9592 - precision_18: 0.9438 - auc_18: 0.9915 - recall_18: 0.8661 - val_loss: 0.0659 - val_acc: 0.9745 - val_precision_18: 0.9433 - val_auc_18: 0.9936 - val_recall_18: 0.9001
Epoch 94/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0625 - acc: 0.9606 - precision_18: 0.9530 - auc_18: 0.9938 - recall_18: 0.8666 - val_loss: 0.0645 - val_acc: 0.9747 - val_precision_18: 0.9239 - val_auc_18: 0.9944 - val_recall_18: 0.9221
Epoch 95/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0716 - acc: 0.9596 - precision_18: 0.9510 - auc_18: 0.9904 - recall_18: 0.8586 - val_loss: 0.0613 - val_acc: 0.9764 - val_precision_18: 0.9061 - val_auc_18: 0.9946 - val_recall_18: 0.9366
Epoch 96/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0581 - acc: 0.9622 - precision_18: 0.9553 - auc_18: 0.9943 - recall_18: 0.8727 - val_loss: 0.0633 - val_acc: 0.9750 - val_precision_18: 0.9231 - val_auc_18: 0.9938 - val_recall_18: 0.9148
Epoch 97/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0663 - acc: 0.9612 - precision_18: 0.9533 - auc_18: 0.9917 - recall_18: 0.8674 - val_loss: 0.0616 - val_acc: 0.9761 - val_precision_18: 0.9270 - val_auc_18: 0.9944 - val_recall_18: 0.9207
Epoch 98/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0573 - acc: 0.9621 - precision_18: 0.9555 - auc_18: 0.9948 - recall_18: 0.8714 - val_loss: 0.0642 - val_acc: 0.9750 - val_precision_18: 0.9199 - val_auc_18: 0.9933 - val_recall_18: 0.9167
Epoch 99/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0824 - acc: 0.9577 - precision_18: 0.9396 - auc_18: 0.9875 - recall_18: 0.8560 - val_loss: 0.0639 - val_acc: 0.9749 - val_precision_18: 0.9174 - val_auc_18: 0.9943 - val_recall_18: 0.9201
Epoch 100/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0648 - acc: 0.9609 - precision_18: 0.9526 - auc_18: 0.9923 - recall_18: 0.8684 - val_loss: 0.0651 - val_acc: 0.9748 - val_precision_18: 0.9187 - val_auc_18: 0.9937 - val_recall_18: 0.9208
6/6 [==============================] - 0s 26ms/step - loss: 0.0646 - acc: 0.9742 - precision_18: 0.9022 - auc_18: 0.9954 - recall_18: 0.9437
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbe6c11c040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8466752547165411
test_sensitivity 0.9487454253772497
test_specifitivity 0.9784107137652648
test_accuracy 0.9735951741536458
test_precision 0.8949135510565982
test_jaccard_score 0.8466752547165411
test_dicecoef 0.9210435861792056
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-112354.h5
[0. 0. 0. 0. 0.] [0.97359517 0.84667525 0.89491355 0.94874543 0.97841071]

-------------------------
Rep: 1
-------------------------

2021-09-25 11:23:54.950170: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:23:54.950225: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Normalized per channel
Applied normalization
Data augumentation on
Prep done
Training samples: 258, channel mean: -1.389123527662048e-17,
Validation samples: 63, channel mean: -0.0011996397803231908
Model built.
Model: "functional_39"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_20 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_361 (Conv2D)             (None, 256, 256, 32) 320         input_20[0][0]
__________________________________________________________________________________________________
conv2d_362 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_361[0][0]
__________________________________________________________________________________________________
max_pooling2d_76 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_362[0][0]
__________________________________________________________________________________________________
conv2d_363 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_76[0][0]
__________________________________________________________________________________________________
conv2d_364 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_363[0][0]
__________________________________________________________________________________________________
max_pooling2d_77 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_364[0][0]
__________________________________________________________________________________________________
conv2d_365 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_77[0][0]
__________________________________________________________________________________________________
conv2d_366 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_365[0][0]
__________________________________________________________________________________________________
max_pooling2d_78 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_366[0][0]
__________________________________________________________________________________________________
conv2d_367 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_78[0][0]
__________________________________________________________________________________________________
conv2d_368 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_367[0][0]
__________________________________________________________________________________________________
max_pooling2d_79 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_368[0][0]
__________________________________________________________________________________________________
conv2d_369 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_79[0][0]
__________________________________________________________________________________________________
conv2d_370 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_369[0][0]
__________________________________________________________________________________________________
up_sampling2d_76 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_370[0][0]
__________________________________________________________________________________________________
concatenate_76 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_76[0][0]
                                                                 conv2d_368[0][0]
__________________________________________________________________________________________________
conv2d_371 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_76[0][0]
__________________________________________________________________________________________________
conv2d_372 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_371[0][0]
__________________________________________________________________________________________________
up_sampling2d_77 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_372[0][0]
__________________________________________________________________________________________________
concatenate_77 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_77[0][0]
                                                                 conv2d_366[0][0]
__________________________________________________________________________________________________
conv2d_373 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_77[0][0]
__________________________________________________________________________________________________
conv2d_374 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_373[0][0]
__________________________________________________________________________________________________
up_sampling2d_78 (UpSampling2D) (None, 128, 128, 128 0           conv2d_374[0][0]
__________________________________________________________________________________________________
concatenate_78 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_78[0][0]
                                                                 conv2d_364[0][0]
__________________________________________________________________________________________________
conv2d_375 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_78[0][0]
__________________________________________________________________________________________________
conv2d_376 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_375[0][0]
__________________________________________________________________________________________________
up_sampling2d_79 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_376[0][0]
__________________________________________________________________________________________________
concatenate_79 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_79[0][0]
                                                                 conv2d_362[0][0]
__________________________________________________________________________________________________
conv2d_377 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_79[0][0]
__________________________________________________________________________________________________
conv2d_378 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_377[0][0]
__________________________________________________________________________________________________
conv2d_379 (Conv2D)             (None, 256, 256, 1)  33          conv2d_378[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6927 - acc: 0.8332 - precision_19: 0.2889 - auc_19: 0.4674 - recall_19: 0.05472021-09-25 11:23:57.329760: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:23:57.329840: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 11:23:57.611718: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 11:23:57.620440: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-112354/train/plugins/profile/2021_09_25_11_23_57
2021-09-25 11:23:57.623845: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-112354/train/plugins/profile/2021_09_25_11_23_57/xeon-e-v100.trace.json.gz
2021-09-25 11:23:57.644547: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-112354/train/plugins/profile/2021_09_25_11_23_57
2021-09-25 11:23:57.653026: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-112354/train/plugins/profile/2021_09_25_11_23_57/xeon-e-v100.memory_profile.json.gz
2021-09-25 11:23:57.673884: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-112354/train/plugins/profile/2021_09_25_11_23_57Dumped tool data for xplane.pb to logs/fit/20210925-112354/train/plugins/profile/2021_09_25_11_23_57/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-112354/train/plugins/profile/2021_09_25_11_23_57/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-112354/train/plugins/profile/2021_09_25_11_23_57/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-112354/train/plugins/profile/2021_09_25_11_23_57/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-112354/train/plugins/profile/2021_09_25_11_23_57/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6901 - acc: 0.8403 - precision_19: 0.6860 - auc_19: 0.6092 - recall_19: 0.2821WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0649s vs `on_train_batch_end` time: 0.2808s). Check your callbacks.
33/33 [==============================] - 4s 108ms/step - loss: 0.4226 - acc: 0.8860 - precision_19: 0.7817 - auc_19: 0.7978 - recall_19: 0.4624 - val_loss: 0.2761 - val_acc: 0.8956 - val_precision_19: 0.6576 - val_auc_19: 0.8994 - val_recall_19: 0.7022
Epoch 2/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2283 - acc: 0.9017 - precision_19: 0.7837 - auc_19: 0.9082 - recall_19: 0.6408 - val_loss: 0.2413 - val_acc: 0.9048 - val_precision_19: 0.6802 - val_auc_19: 0.9212 - val_recall_19: 0.7009
Epoch 3/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2194 - acc: 0.9050 - precision_19: 0.8090 - auc_19: 0.9200 - recall_19: 0.6267 - val_loss: 0.2459 - val_acc: 0.9125 - val_precision_19: 0.7679 - val_auc_19: 0.9160 - val_recall_19: 0.6211
Epoch 4/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2160 - acc: 0.9074 - precision_19: 0.8211 - auc_19: 0.9216 - recall_19: 0.6255 - val_loss: 0.2577 - val_acc: 0.8992 - val_precision_19: 0.7203 - val_auc_19: 0.9132 - val_recall_19: 0.6591
Epoch 5/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1980 - acc: 0.9125 - precision_19: 0.8317 - auc_19: 0.9355 - recall_19: 0.6564 - val_loss: 0.2020 - val_acc: 0.9207 - val_precision_19: 0.8096 - val_auc_19: 0.9424 - val_recall_19: 0.6368
Epoch 6/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2041 - acc: 0.9080 - precision_19: 0.8176 - auc_19: 0.9324 - recall_19: 0.6187 - val_loss: 0.2230 - val_acc: 0.9160 - val_precision_19: 0.7529 - val_auc_19: 0.9197 - val_recall_19: 0.6965
Epoch 7/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2002 - acc: 0.9109 - precision_19: 0.8282 - auc_19: 0.9347 - recall_19: 0.6415 - val_loss: 0.1974 - val_acc: 0.9226 - val_precision_19: 0.7497 - val_auc_19: 0.9431 - val_recall_19: 0.7267
Epoch 8/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1925 - acc: 0.9147 - precision_19: 0.8368 - auc_19: 0.9389 - recall_19: 0.6631 - val_loss: 0.2298 - val_acc: 0.9169 - val_precision_19: 0.8294 - val_auc_19: 0.9243 - val_recall_19: 0.6161
Epoch 9/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1913 - acc: 0.9142 - precision_19: 0.8333 - auc_19: 0.9410 - recall_19: 0.6664 - val_loss: 0.2269 - val_acc: 0.9216 - val_precision_19: 0.7356 - val_auc_19: 0.9473 - val_recall_19: 0.7833
Epoch 10/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1839 - acc: 0.9177 - precision_19: 0.8376 - auc_19: 0.9446 - recall_19: 0.6861 - val_loss: 0.1826 - val_acc: 0.9313 - val_precision_19: 0.8158 - val_auc_19: 0.9520 - val_recall_19: 0.7211
Epoch 11/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1665 - acc: 0.9255 - precision_19: 0.8628 - auc_19: 0.9534 - recall_19: 0.7178 - val_loss: 0.1569 - val_acc: 0.9369 - val_precision_19: 0.7974 - val_auc_19: 0.9691 - val_recall_19: 0.8107
Epoch 12/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1564 - acc: 0.9287 - precision_19: 0.8541 - auc_19: 0.9577 - recall_19: 0.7508 - val_loss: 0.1551 - val_acc: 0.9454 - val_precision_19: 0.8554 - val_auc_19: 0.9586 - val_recall_19: 0.7488
Epoch 13/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1556 - acc: 0.9308 - precision_19: 0.8667 - auc_19: 0.9573 - recall_19: 0.7511 - val_loss: 0.1439 - val_acc: 0.9394 - val_precision_19: 0.7447 - val_auc_19: 0.9809 - val_recall_19: 0.9154
Epoch 14/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1365 - acc: 0.9350 - precision_19: 0.8680 - auc_19: 0.9685 - recall_19: 0.7839 - val_loss: 0.1254 - val_acc: 0.9532 - val_precision_19: 0.8888 - val_auc_19: 0.9791 - val_recall_19: 0.8082
Epoch 15/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1263 - acc: 0.9386 - precision_19: 0.8899 - auc_19: 0.9733 - recall_19: 0.7831 - val_loss: 0.1462 - val_acc: 0.9449 - val_precision_19: 0.8139 - val_auc_19: 0.9703 - val_recall_19: 0.8597
Epoch 16/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1318 - acc: 0.9380 - precision_19: 0.8718 - auc_19: 0.9705 - recall_19: 0.7981 - val_loss: 0.1260 - val_acc: 0.9516 - val_precision_19: 0.8875 - val_auc_19: 0.9795 - val_recall_19: 0.7882
Epoch 17/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1189 - acc: 0.9412 - precision_19: 0.8952 - auc_19: 0.9764 - recall_19: 0.7911 - val_loss: 0.1030 - val_acc: 0.9598 - val_precision_19: 0.8650 - val_auc_19: 0.9853 - val_recall_19: 0.8819
Epoch 18/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1166 - acc: 0.9425 - precision_19: 0.8900 - auc_19: 0.9771 - recall_19: 0.8088 - val_loss: 0.1005 - val_acc: 0.9601 - val_precision_19: 0.8563 - val_auc_19: 0.9883 - val_recall_19: 0.9082
Epoch 19/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0986 - acc: 0.9475 - precision_19: 0.9060 - auc_19: 0.9846 - recall_19: 0.8256 - val_loss: 0.1049 - val_acc: 0.9618 - val_precision_19: 0.8883 - val_auc_19: 0.9855 - val_recall_19: 0.8391
Epoch 20/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1006 - acc: 0.9473 - precision_19: 0.9091 - auc_19: 0.9842 - recall_19: 0.8209 - val_loss: 0.1384 - val_acc: 0.9572 - val_precision_19: 0.9385 - val_auc_19: 0.9720 - val_recall_19: 0.7747
Epoch 21/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1305 - acc: 0.9400 - precision_19: 0.8909 - auc_19: 0.9712 - recall_19: 0.7874 - val_loss: 0.0824 - val_acc: 0.9694 - val_precision_19: 0.9015 - val_auc_19: 0.9922 - val_recall_19: 0.8870
Epoch 22/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1046 - acc: 0.9471 - precision_19: 0.9160 - auc_19: 0.9825 - recall_19: 0.8066 - val_loss: 0.0883 - val_acc: 0.9660 - val_precision_19: 0.8828 - val_auc_19: 0.9898 - val_recall_19: 0.8906
Epoch 23/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1038 - acc: 0.9477 - precision_19: 0.9115 - auc_19: 0.9828 - recall_19: 0.8229 - val_loss: 0.0952 - val_acc: 0.9638 - val_precision_19: 0.8864 - val_auc_19: 0.9869 - val_recall_19: 0.8937
Epoch 24/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0964 - acc: 0.9500 - precision_19: 0.9190 - auc_19: 0.9848 - recall_19: 0.8311 - val_loss: 0.0818 - val_acc: 0.9669 - val_precision_19: 0.8596 - val_auc_19: 0.9921 - val_recall_19: 0.9240
Epoch 25/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1101 - acc: 0.9461 - precision_19: 0.9074 - auc_19: 0.9808 - recall_19: 0.8152 - val_loss: 0.0868 - val_acc: 0.9668 - val_precision_19: 0.9066 - val_auc_19: 0.9904 - val_recall_19: 0.8796
Epoch 26/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0891 - acc: 0.9511 - precision_19: 0.9179 - auc_19: 0.9877 - recall_19: 0.8387 - val_loss: 0.0819 - val_acc: 0.9668 - val_precision_19: 0.8837 - val_auc_19: 0.9911 - val_recall_19: 0.9062
Epoch 27/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0945 - acc: 0.9498 - precision_19: 0.9119 - auc_19: 0.9857 - recall_19: 0.8347 - val_loss: 0.0694 - val_acc: 0.9733 - val_precision_19: 0.9184 - val_auc_19: 0.9940 - val_recall_19: 0.9054
Epoch 28/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1003 - acc: 0.9485 - precision_19: 0.9140 - auc_19: 0.9838 - recall_19: 0.8237 - val_loss: 0.0782 - val_acc: 0.9699 - val_precision_19: 0.9021 - val_auc_19: 0.9910 - val_recall_19: 0.8912
Epoch 29/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0855 - acc: 0.9526 - precision_19: 0.9219 - auc_19: 0.9886 - recall_19: 0.8421 - val_loss: 0.0848 - val_acc: 0.9668 - val_precision_19: 0.9150 - val_auc_19: 0.9912 - val_recall_19: 0.8856
Epoch 30/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0966 - acc: 0.9508 - precision_19: 0.9230 - auc_19: 0.9844 - recall_19: 0.8269 - val_loss: 0.0758 - val_acc: 0.9710 - val_precision_19: 0.9104 - val_auc_19: 0.9928 - val_recall_19: 0.9030
Epoch 31/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0879 - acc: 0.9526 - precision_19: 0.9184 - auc_19: 0.9874 - recall_19: 0.8502 - val_loss: 0.0812 - val_acc: 0.9678 - val_precision_19: 0.9161 - val_auc_19: 0.9922 - val_recall_19: 0.8842
Epoch 32/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0971 - acc: 0.9507 - precision_19: 0.9204 - auc_19: 0.9823 - recall_19: 0.8269 - val_loss: 0.0875 - val_acc: 0.9663 - val_precision_19: 0.9320 - val_auc_19: 0.9900 - val_recall_19: 0.8400
Epoch 33/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0840 - acc: 0.9535 - precision_19: 0.9310 - auc_19: 0.9889 - recall_19: 0.8408 - val_loss: 0.0766 - val_acc: 0.9698 - val_precision_19: 0.9077 - val_auc_19: 0.9928 - val_recall_19: 0.8995
Epoch 34/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0846 - acc: 0.9527 - precision_19: 0.9241 - auc_19: 0.9890 - recall_19: 0.8389 - val_loss: 0.0793 - val_acc: 0.9685 - val_precision_19: 0.8905 - val_auc_19: 0.9926 - val_recall_19: 0.9121
Epoch 35/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0850 - acc: 0.9534 - precision_19: 0.9269 - auc_19: 0.9878 - recall_19: 0.8406 - val_loss: 0.0839 - val_acc: 0.9662 - val_precision_19: 0.8769 - val_auc_19: 0.9933 - val_recall_19: 0.9401
Epoch 36/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1032 - acc: 0.9506 - precision_19: 0.9221 - auc_19: 0.9827 - recall_19: 0.8278 - val_loss: 0.0758 - val_acc: 0.9711 - val_precision_19: 0.9202 - val_auc_19: 0.9915 - val_recall_19: 0.8947
Epoch 37/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0881 - acc: 0.9538 - precision_19: 0.9270 - auc_19: 0.9863 - recall_19: 0.8395 - val_loss: 0.0801 - val_acc: 0.9680 - val_precision_19: 0.8660 - val_auc_19: 0.9926 - val_recall_19: 0.9206
Epoch 38/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0764 - acc: 0.9554 - precision_19: 0.9287 - auc_19: 0.9912 - recall_19: 0.8516 - val_loss: 0.0761 - val_acc: 0.9695 - val_precision_19: 0.9028 - val_auc_19: 0.9931 - val_recall_19: 0.9110
Epoch 39/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0813 - acc: 0.9552 - precision_19: 0.9305 - auc_19: 0.9884 - recall_19: 0.8492 - val_loss: 0.0839 - val_acc: 0.9679 - val_precision_19: 0.9422 - val_auc_19: 0.9909 - val_recall_19: 0.8528
Epoch 40/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0786 - acc: 0.9553 - precision_19: 0.9323 - auc_19: 0.9903 - recall_19: 0.8511 - val_loss: 0.0714 - val_acc: 0.9718 - val_precision_19: 0.9099 - val_auc_19: 0.9929 - val_recall_19: 0.9084
Epoch 41/100
32/33 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9551 - precision_19: 0.9286 - auc_19: 0.9853 - recall_19: 0.8455Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 85ms/step - loss: 0.0893 - acc: 0.9550 - precision_19: 0.9282 - auc_19: 0.9853 - recall_19: 0.8451 - val_loss: 0.0725 - val_acc: 0.9712 - val_precision_19: 0.9073 - val_auc_19: 0.9923 - val_recall_19: 0.9104
Epoch 00041: early stopping
6/6 [==============================] - 0s 26ms/step - loss: 0.0752 - acc: 0.9702 - precision_19: 0.9114 - auc_19: 0.9934 - recall_19: 0.9042
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbe00389dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8268796919483246
test_sensitivity 0.919292374793202
test_specifitivity 0.98044439910751
test_accuracy 0.9705176459418403
test_precision 0.9010853786130659
test_jaccard_score 0.8268796919483246
test_dicecoef 0.9100978256261574
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-112611.h5
[0.97359517 0.84667525 0.89491355 0.94874543 0.97841071] [0.97051765 0.82687969 0.90108538 0.91929237 0.9804444 ]

-------------------------
Rep: 2
-------------------------

2021-09-25 11:26:11.604577: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:26:11.604640: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Normalized per channel
Applied normalization
Data augumentation on
Prep done
Training samples: 258, channel mean: -1.389123527662048e-17,
Validation samples: 63, channel mean: -0.0011996397803231908
Model built.
Model: "functional_41"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_21 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_380 (Conv2D)             (None, 256, 256, 32) 320         input_21[0][0]
__________________________________________________________________________________________________
conv2d_381 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_380[0][0]
__________________________________________________________________________________________________
max_pooling2d_80 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_381[0][0]
__________________________________________________________________________________________________
conv2d_382 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_80[0][0]
__________________________________________________________________________________________________
conv2d_383 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_382[0][0]
__________________________________________________________________________________________________
max_pooling2d_81 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_383[0][0]
__________________________________________________________________________________________________
conv2d_384 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_81[0][0]
__________________________________________________________________________________________________
conv2d_385 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_384[0][0]
__________________________________________________________________________________________________
max_pooling2d_82 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_385[0][0]
__________________________________________________________________________________________________
conv2d_386 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_82[0][0]
__________________________________________________________________________________________________
conv2d_387 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_386[0][0]
__________________________________________________________________________________________________
max_pooling2d_83 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_387[0][0]
__________________________________________________________________________________________________
conv2d_388 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_83[0][0]
__________________________________________________________________________________________________
conv2d_389 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_388[0][0]
__________________________________________________________________________________________________
up_sampling2d_80 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_389[0][0]
__________________________________________________________________________________________________
concatenate_80 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_80[0][0]
                                                                 conv2d_387[0][0]
__________________________________________________________________________________________________
conv2d_390 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_80[0][0]
__________________________________________________________________________________________________
conv2d_391 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_390[0][0]
__________________________________________________________________________________________________
up_sampling2d_81 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_391[0][0]
__________________________________________________________________________________________________
concatenate_81 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_81[0][0]
                                                                 conv2d_385[0][0]
__________________________________________________________________________________________________
conv2d_392 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_81[0][0]
__________________________________________________________________________________________________
conv2d_393 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_392[0][0]
__________________________________________________________________________________________________
up_sampling2d_82 (UpSampling2D) (None, 128, 128, 128 0           conv2d_393[0][0]
__________________________________________________________________________________________________
concatenate_82 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_82[0][0]
                                                                 conv2d_383[0][0]
__________________________________________________________________________________________________
conv2d_394 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_82[0][0]
__________________________________________________________________________________________________
conv2d_395 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_394[0][0]
__________________________________________________________________________________________________
up_sampling2d_83 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_395[0][0]
__________________________________________________________________________________________________
concatenate_83 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_83[0][0]
                                                                 conv2d_381[0][0]
__________________________________________________________________________________________________
conv2d_396 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_83[0][0]
__________________________________________________________________________________________________
conv2d_397 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_396[0][0]
__________________________________________________________________________________________________
conv2d_398 (Conv2D)             (None, 256, 256, 1)  33          conv2d_397[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6937 - acc: 0.3228 - precision_20: 0.0408 - auc_20: 0.3399 - recall_20: 0.15182021-09-25 11:26:14.052319: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:26:14.052635: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 11:26:14.336049: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 11:26:14.363635: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-112611/train/plugins/profile/2021_09_25_11_26_14
2021-09-25 11:26:14.367624: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-112611/train/plugins/profile/2021_09_25_11_26_14/xeon-e-v100.trace.json.gz
2021-09-25 11:26:14.386995: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-112611/train/plugins/profile/2021_09_25_11_26_14
2021-09-25 11:26:14.393703: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-112611/train/plugins/profile/2021_09_25_11_26_14/xeon-e-v100.memory_profile.json.gz
2021-09-25 11:26:14.410708: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-112611/train/plugins/profile/2021_09_25_11_26_14Dumped tool data for xplane.pb to logs/fit/20210925-112611/train/plugins/profile/2021_09_25_11_26_14/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-112611/train/plugins/profile/2021_09_25_11_26_14/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-112611/train/plugins/profile/2021_09_25_11_26_14/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-112611/train/plugins/profile/2021_09_25_11_26_14/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-112611/train/plugins/profile/2021_09_25_11_26_14/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6893 - acc: 0.5860 - precision_20: 0.1569 - auc_20: 0.5485 - recall_20: 0.2842WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0685s vs `on_train_batch_end` time: 0.2911s). Check your callbacks.
33/33 [==============================] - 4s 109ms/step - loss: 0.8727 - acc: 0.8472 - precision_20: 0.5847 - auc_20: 0.5898 - recall_20: 0.1871 - val_loss: 0.4657 - val_acc: 0.8503 - val_precision_20: 0.9719 - val_auc_20: 0.8878 - val_recall_20: 0.0505
Epoch 2/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2808 - acc: 0.8836 - precision_20: 0.7773 - auc_20: 0.8742 - recall_20: 0.4499 - val_loss: 0.2456 - val_acc: 0.8998 - val_precision_20: 0.6532 - val_auc_20: 0.9196 - val_recall_20: 0.7210
Epoch 3/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2309 - acc: 0.9024 - precision_20: 0.7977 - auc_20: 0.9128 - recall_20: 0.6215 - val_loss: 0.2889 - val_acc: 0.9020 - val_precision_20: 0.6748 - val_auc_20: 0.9110 - val_recall_20: 0.7061
Epoch 4/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2252 - acc: 0.9031 - precision_20: 0.7957 - auc_20: 0.9145 - recall_20: 0.6336 - val_loss: 0.2691 - val_acc: 0.8935 - val_precision_20: 0.7139 - val_auc_20: 0.9000 - val_recall_20: 0.6169
Epoch 5/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2154 - acc: 0.9040 - precision_20: 0.8041 - auc_20: 0.9261 - recall_20: 0.6271 - val_loss: 0.2106 - val_acc: 0.9152 - val_precision_20: 0.7505 - val_auc_20: 0.9356 - val_recall_20: 0.6763
Epoch 6/100
33/33 [==============================] - 3s 87ms/step - loss: 0.2089 - acc: 0.9067 - precision_20: 0.8285 - auc_20: 0.9332 - recall_20: 0.5960 - val_loss: 0.2197 - val_acc: 0.9169 - val_precision_20: 0.7596 - val_auc_20: 0.9238 - val_recall_20: 0.6933
Epoch 7/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1981 - acc: 0.9090 - precision_20: 0.8286 - auc_20: 0.9418 - recall_20: 0.6273 - val_loss: 0.1939 - val_acc: 0.9235 - val_precision_20: 0.7803 - val_auc_20: 0.9489 - val_recall_20: 0.6822
Epoch 8/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1920 - acc: 0.9127 - precision_20: 0.8421 - auc_20: 0.9434 - recall_20: 0.6408 - val_loss: 0.2361 - val_acc: 0.9126 - val_precision_20: 0.8002 - val_auc_20: 0.9278 - val_recall_20: 0.6170
Epoch 9/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1875 - acc: 0.9153 - precision_20: 0.8547 - auc_20: 0.9463 - recall_20: 0.6441 - val_loss: 0.2227 - val_acc: 0.9117 - val_precision_20: 0.7089 - val_auc_20: 0.9434 - val_recall_20: 0.7443
Epoch 10/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1895 - acc: 0.9153 - precision_20: 0.8438 - auc_20: 0.9453 - recall_20: 0.6604 - val_loss: 0.1865 - val_acc: 0.9270 - val_precision_20: 0.8350 - val_auc_20: 0.9577 - val_recall_20: 0.6611
Epoch 11/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1771 - acc: 0.9196 - precision_20: 0.8601 - auc_20: 0.9523 - recall_20: 0.6750 - val_loss: 0.1844 - val_acc: 0.9236 - val_precision_20: 0.7358 - val_auc_20: 0.9663 - val_recall_20: 0.8139
Epoch 12/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1667 - acc: 0.9221 - precision_20: 0.8540 - auc_20: 0.9587 - recall_20: 0.7036 - val_loss: 0.1548 - val_acc: 0.9410 - val_precision_20: 0.8306 - val_auc_20: 0.9647 - val_recall_20: 0.7431
Epoch 13/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1506 - acc: 0.9293 - precision_20: 0.8661 - auc_20: 0.9651 - recall_20: 0.7447 - val_loss: 0.1319 - val_acc: 0.9490 - val_precision_20: 0.7948 - val_auc_20: 0.9811 - val_recall_20: 0.8962
Epoch 14/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1372 - acc: 0.9345 - precision_20: 0.8687 - auc_20: 0.9707 - recall_20: 0.7804 - val_loss: 0.1417 - val_acc: 0.9455 - val_precision_20: 0.9039 - val_auc_20: 0.9766 - val_recall_20: 0.7373
Epoch 15/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1418 - acc: 0.9346 - precision_20: 0.8798 - auc_20: 0.9651 - recall_20: 0.7626 - val_loss: 0.1317 - val_acc: 0.9527 - val_precision_20: 0.8686 - val_auc_20: 0.9768 - val_recall_20: 0.8373
Epoch 16/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1403 - acc: 0.9378 - precision_20: 0.8856 - auc_20: 0.9659 - recall_20: 0.7765 - val_loss: 0.1106 - val_acc: 0.9609 - val_precision_20: 0.8757 - val_auc_20: 0.9818 - val_recall_20: 0.8719
Epoch 17/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1141 - acc: 0.9432 - precision_20: 0.9005 - auc_20: 0.9786 - recall_20: 0.7956 - val_loss: 0.1140 - val_acc: 0.9555 - val_precision_20: 0.8371 - val_auc_20: 0.9827 - val_recall_20: 0.8903
Epoch 18/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1134 - acc: 0.9433 - precision_20: 0.8955 - auc_20: 0.9787 - recall_20: 0.8060 - val_loss: 0.1053 - val_acc: 0.9579 - val_precision_20: 0.8460 - val_auc_20: 0.9877 - val_recall_20: 0.9075
Epoch 19/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0994 - acc: 0.9468 - precision_20: 0.9039 - auc_20: 0.9855 - recall_20: 0.8230 - val_loss: 0.0862 - val_acc: 0.9672 - val_precision_20: 0.8907 - val_auc_20: 0.9883 - val_recall_20: 0.8784
Epoch 20/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1170 - acc: 0.9450 - precision_20: 0.9012 - auc_20: 0.9759 - recall_20: 0.8137 - val_loss: 0.0847 - val_acc: 0.9683 - val_precision_20: 0.9236 - val_auc_20: 0.9898 - val_recall_20: 0.8673
Epoch 21/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1110 - acc: 0.9457 - precision_20: 0.9078 - auc_20: 0.9794 - recall_20: 0.8073 - val_loss: 0.0754 - val_acc: 0.9707 - val_precision_20: 0.9039 - val_auc_20: 0.9917 - val_recall_20: 0.8943
Epoch 22/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0972 - acc: 0.9496 - precision_20: 0.9155 - auc_20: 0.9852 - recall_20: 0.8278 - val_loss: 0.0814 - val_acc: 0.9684 - val_precision_20: 0.9159 - val_auc_20: 0.9906 - val_recall_20: 0.8682
Epoch 23/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0905 - acc: 0.9511 - precision_20: 0.9144 - auc_20: 0.9872 - recall_20: 0.8421 - val_loss: 0.0867 - val_acc: 0.9663 - val_precision_20: 0.8980 - val_auc_20: 0.9896 - val_recall_20: 0.8965
Epoch 24/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1070 - acc: 0.9496 - precision_20: 0.9156 - auc_20: 0.9799 - recall_20: 0.8302 - val_loss: 0.0775 - val_acc: 0.9694 - val_precision_20: 0.8770 - val_auc_20: 0.9923 - val_recall_20: 0.9187
Epoch 25/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0804 - acc: 0.9535 - precision_20: 0.9246 - auc_20: 0.9905 - recall_20: 0.8495 - val_loss: 0.0819 - val_acc: 0.9673 - val_precision_20: 0.8747 - val_auc_20: 0.9918 - val_recall_20: 0.9245
Epoch 26/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0807 - acc: 0.9548 - precision_20: 0.9254 - auc_20: 0.9899 - recall_20: 0.8575 - val_loss: 0.0828 - val_acc: 0.9677 - val_precision_20: 0.8902 - val_auc_20: 0.9903 - val_recall_20: 0.9044
Epoch 27/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0902 - acc: 0.9515 - precision_20: 0.9167 - auc_20: 0.9869 - recall_20: 0.8405 - val_loss: 0.0666 - val_acc: 0.9736 - val_precision_20: 0.9153 - val_auc_20: 0.9942 - val_recall_20: 0.9116
Epoch 28/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1049 - acc: 0.9506 - precision_20: 0.9182 - auc_20: 0.9808 - recall_20: 0.8300 - val_loss: 0.0827 - val_acc: 0.9691 - val_precision_20: 0.9015 - val_auc_20: 0.9890 - val_recall_20: 0.8860
Epoch 29/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1117 - acc: 0.9490 - precision_20: 0.9115 - auc_20: 0.9768 - recall_20: 0.8279 - val_loss: 0.0879 - val_acc: 0.9659 - val_precision_20: 0.9113 - val_auc_20: 0.9904 - val_recall_20: 0.8844
Epoch 30/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0830 - acc: 0.9538 - precision_20: 0.9296 - auc_20: 0.9889 - recall_20: 0.8408 - val_loss: 0.0749 - val_acc: 0.9708 - val_precision_20: 0.9053 - val_auc_20: 0.9926 - val_recall_20: 0.9075
Epoch 31/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0839 - acc: 0.9541 - precision_20: 0.9231 - auc_20: 0.9887 - recall_20: 0.8555 - val_loss: 0.0748 - val_acc: 0.9704 - val_precision_20: 0.9181 - val_auc_20: 0.9928 - val_recall_20: 0.8991
Epoch 32/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0769 - acc: 0.9556 - precision_20: 0.9321 - auc_20: 0.9908 - recall_20: 0.8519 - val_loss: 0.0728 - val_acc: 0.9723 - val_precision_20: 0.9235 - val_auc_20: 0.9918 - val_recall_20: 0.8919
Epoch 33/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0774 - acc: 0.9564 - precision_20: 0.9350 - auc_20: 0.9902 - recall_20: 0.8583 - val_loss: 0.0814 - val_acc: 0.9686 - val_precision_20: 0.9432 - val_auc_20: 0.9915 - val_recall_20: 0.8515
Epoch 34/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0936 - acc: 0.9505 - precision_20: 0.9143 - auc_20: 0.9859 - recall_20: 0.8359 - val_loss: 0.0763 - val_acc: 0.9703 - val_precision_20: 0.8967 - val_auc_20: 0.9926 - val_recall_20: 0.9173
Epoch 35/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0909 - acc: 0.9537 - precision_20: 0.9255 - auc_20: 0.9845 - recall_20: 0.8466 - val_loss: 0.0852 - val_acc: 0.9663 - val_precision_20: 0.8756 - val_auc_20: 0.9931 - val_recall_20: 0.9431
Epoch 36/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0860 - acc: 0.9548 - precision_20: 0.9309 - auc_20: 0.9884 - recall_20: 0.8459 - val_loss: 0.0735 - val_acc: 0.9721 - val_precision_20: 0.9329 - val_auc_20: 0.9917 - val_recall_20: 0.8872
Epoch 37/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0695 - acc: 0.9583 - precision_20: 0.9393 - auc_20: 0.9922 - recall_20: 0.8629 - val_loss: 0.0794 - val_acc: 0.9695 - val_precision_20: 0.8755 - val_auc_20: 0.9917 - val_recall_20: 0.9193
Epoch 38/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0774 - acc: 0.9563 - precision_20: 0.9334 - auc_20: 0.9905 - recall_20: 0.8524 - val_loss: 0.0787 - val_acc: 0.9683 - val_precision_20: 0.9071 - val_auc_20: 0.9921 - val_recall_20: 0.8969
Epoch 39/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0749 - acc: 0.9573 - precision_20: 0.9372 - auc_20: 0.9909 - recall_20: 0.8578 - val_loss: 0.0777 - val_acc: 0.9704 - val_precision_20: 0.9263 - val_auc_20: 0.9913 - val_recall_20: 0.8861
Epoch 40/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0905 - acc: 0.9552 - precision_20: 0.9322 - auc_20: 0.9853 - recall_20: 0.8493 - val_loss: 0.0748 - val_acc: 0.9708 - val_precision_20: 0.9009 - val_auc_20: 0.9925 - val_recall_20: 0.9129
Epoch 41/100
32/33 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9561 - precision_20: 0.9318 - auc_20: 0.9891 - recall_20: 0.8524Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 87ms/step - loss: 0.0809 - acc: 0.9561 - precision_20: 0.9317 - auc_20: 0.9891 - recall_20: 0.8518 - val_loss: 0.0732 - val_acc: 0.9716 - val_precision_20: 0.9212 - val_auc_20: 0.9920 - val_recall_20: 0.8963
Epoch 00041: early stopping
6/6 [==============================] - 0s 25ms/step - loss: 0.0727 - acc: 0.9712 - precision_20: 0.9075 - auc_20: 0.9937 - recall_20: 0.9156
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb613ca0670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8286421967988054
test_sensitivity 0.9259266222155378
test_specifitivity 0.9800760365156623
test_accuracy 0.9712860107421875
test_precision 0.9000582756499156
test_jaccard_score 0.8286421967988054
test_dicecoef 0.9128092132020201
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-112827.h5
[1.94411282 1.67355495 1.79599893 1.8680378  1.95885511] [0.97128601 0.8286422  0.90005828 0.92592662 0.98007604]

-------------------------
Averaged metrics for Baseline + Augumentations + Per Channel Normalization - bacteria: [0.97179961 0.83406571 0.89868574 0.93132147 0.97964372]
-------------------------


-------------------------
RUN: Baseline + Augumentations + Gaussian Blur - bacteria, PARAMS: {'augumentation': True, 'gaussian_blur': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-25 11:28:28.496125: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:28:28.496174: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied gaussian blur on all input images
Applied normalization
Data augumentation on
Prep done
Training samples: 258, channel mean: 0.20280802028486108,
Validation samples: 63, channel mean: 0.20159956655730865
Model built.
Model: "functional_43"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_22 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_399 (Conv2D)             (None, 256, 256, 32) 320         input_22[0][0]
__________________________________________________________________________________________________
conv2d_400 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_399[0][0]
__________________________________________________________________________________________________
max_pooling2d_84 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_400[0][0]
__________________________________________________________________________________________________
conv2d_401 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_84[0][0]
__________________________________________________________________________________________________
conv2d_402 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_401[0][0]
__________________________________________________________________________________________________
max_pooling2d_85 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_402[0][0]
__________________________________________________________________________________________________
conv2d_403 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_85[0][0]
__________________________________________________________________________________________________
conv2d_404 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_403[0][0]
__________________________________________________________________________________________________
max_pooling2d_86 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_404[0][0]
__________________________________________________________________________________________________
conv2d_405 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_86[0][0]
__________________________________________________________________________________________________
conv2d_406 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_405[0][0]
__________________________________________________________________________________________________
max_pooling2d_87 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_406[0][0]
__________________________________________________________________________________________________
conv2d_407 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_87[0][0]
__________________________________________________________________________________________________
conv2d_408 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_407[0][0]
__________________________________________________________________________________________________
up_sampling2d_84 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_408[0][0]
__________________________________________________________________________________________________
concatenate_84 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_84[0][0]
                                                                 conv2d_406[0][0]
__________________________________________________________________________________________________
conv2d_409 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_84[0][0]
__________________________________________________________________________________________________
conv2d_410 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_409[0][0]
__________________________________________________________________________________________________
up_sampling2d_85 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_410[0][0]
__________________________________________________________________________________________________
concatenate_85 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_85[0][0]
                                                                 conv2d_404[0][0]
__________________________________________________________________________________________________
conv2d_411 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_85[0][0]
__________________________________________________________________________________________________
conv2d_412 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_411[0][0]
__________________________________________________________________________________________________
up_sampling2d_86 (UpSampling2D) (None, 128, 128, 128 0           conv2d_412[0][0]
__________________________________________________________________________________________________
concatenate_86 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_86[0][0]
                                                                 conv2d_402[0][0]
__________________________________________________________________________________________________
conv2d_413 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_86[0][0]
__________________________________________________________________________________________________
conv2d_414 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_413[0][0]
__________________________________________________________________________________________________
up_sampling2d_87 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_414[0][0]
__________________________________________________________________________________________________
concatenate_87 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_87[0][0]
                                                                 conv2d_400[0][0]
__________________________________________________________________________________________________
conv2d_415 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_87[0][0]
__________________________________________________________________________________________________
conv2d_416 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_415[0][0]
__________________________________________________________________________________________________
conv2d_417 (Conv2D)             (None, 256, 256, 1)  33          conv2d_416[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6931 - acc: 0.6118 - precision_21: 0.1614 - auc_21: 0.4787 - recall_21: 0.35382021-09-25 11:28:33.580346: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:28:33.580413: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 11:28:33.859193: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 11:28:33.868748: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-112828/train/plugins/profile/2021_09_25_11_28_33
2021-09-25 11:28:33.872581: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-112828/train/plugins/profile/2021_09_25_11_28_33/xeon-e-v100.trace.json.gz
2021-09-25 11:28:33.892293: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-112828/train/plugins/profile/2021_09_25_11_28_33
2021-09-25 11:28:33.899198: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-112828/train/plugins/profile/2021_09_25_11_28_33/xeon-e-v100.memory_profile.json.gz
2021-09-25 11:28:33.922977: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-112828/train/plugins/profile/2021_09_25_11_28_33Dumped tool data for xplane.pb to logs/fit/20210925-112828/train/plugins/profile/2021_09_25_11_28_33/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-112828/train/plugins/profile/2021_09_25_11_28_33/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-112828/train/plugins/profile/2021_09_25_11_28_33/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-112828/train/plugins/profile/2021_09_25_11_28_33/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-112828/train/plugins/profile/2021_09_25_11_28_33/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6910 - acc: 0.7316 - precision_21: 0.3035 - auc_21: 0.5886 - recall_21: 0.3382WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0676s vs `on_train_batch_end` time: 0.2766s). Check your callbacks.
33/33 [==============================] - 4s 107ms/step - loss: 0.4954 - acc: 0.8548 - precision_21: 0.7027 - auc_21: 0.6732 - recall_21: 0.1620 - val_loss: 0.2965 - val_acc: 0.8782 - val_precision_21: 0.5933 - val_auc_21: 0.8907 - val_recall_21: 0.7197
Epoch 2/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2452 - acc: 0.8927 - precision_21: 0.7390 - auc_21: 0.9057 - recall_21: 0.6224 - val_loss: 0.2419 - val_acc: 0.9100 - val_precision_21: 0.7592 - val_auc_21: 0.9090 - val_recall_21: 0.5934
Epoch 3/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2287 - acc: 0.9001 - precision_21: 0.7808 - auc_21: 0.9187 - recall_21: 0.6201 - val_loss: 0.2581 - val_acc: 0.9114 - val_precision_21: 0.7922 - val_auc_21: 0.9077 - val_recall_21: 0.5778
Epoch 4/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2253 - acc: 0.9030 - precision_21: 0.8108 - auc_21: 0.9192 - recall_21: 0.5989 - val_loss: 0.2990 - val_acc: 0.8756 - val_precision_21: 0.6160 - val_auc_21: 0.8938 - val_recall_21: 0.7001
Epoch 5/100
33/33 [==============================] - 3s 87ms/step - loss: 0.2186 - acc: 0.9036 - precision_21: 0.7971 - auc_21: 0.9265 - recall_21: 0.6263 - val_loss: 0.2207 - val_acc: 0.9143 - val_precision_21: 0.7594 - val_auc_21: 0.9290 - val_recall_21: 0.6521
Epoch 6/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2057 - acc: 0.9073 - precision_21: 0.8235 - auc_21: 0.9334 - recall_21: 0.6051 - val_loss: 0.2377 - val_acc: 0.9113 - val_precision_21: 0.7390 - val_auc_21: 0.9096 - val_recall_21: 0.6775
Epoch 7/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2098 - acc: 0.9060 - precision_21: 0.8187 - auc_21: 0.9319 - recall_21: 0.6066 - val_loss: 0.2269 - val_acc: 0.9116 - val_precision_21: 0.7593 - val_auc_21: 0.9211 - val_recall_21: 0.6019
Epoch 8/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2158 - acc: 0.9026 - precision_21: 0.8102 - auc_21: 0.9296 - recall_21: 0.5924 - val_loss: 0.2616 - val_acc: 0.9019 - val_precision_21: 0.7427 - val_auc_21: 0.8979 - val_recall_21: 0.6083
Epoch 9/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2128 - acc: 0.9057 - precision_21: 0.8232 - auc_21: 0.9294 - recall_21: 0.6039 - val_loss: 0.2567 - val_acc: 0.9055 - val_precision_21: 0.7110 - val_auc_21: 0.9147 - val_recall_21: 0.6733
Epoch 10/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2171 - acc: 0.9060 - precision_21: 0.8182 - auc_21: 0.9274 - recall_21: 0.6131 - val_loss: 0.2349 - val_acc: 0.9140 - val_precision_21: 0.8538 - val_auc_21: 0.9132 - val_recall_21: 0.5397
Epoch 11/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2096 - acc: 0.9058 - precision_21: 0.8203 - auc_21: 0.9324 - recall_21: 0.6112 - val_loss: 0.2418 - val_acc: 0.9065 - val_precision_21: 0.7754 - val_auc_21: 0.9150 - val_recall_21: 0.5835
Epoch 12/100
33/33 [==============================] - 3s 87ms/step - loss: 0.2030 - acc: 0.9084 - precision_21: 0.8324 - auc_21: 0.9374 - recall_21: 0.6168 - val_loss: 0.2079 - val_acc: 0.9256 - val_precision_21: 0.8695 - val_auc_21: 0.9287 - val_recall_21: 0.5709
Epoch 13/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1933 - acc: 0.9131 - precision_21: 0.8477 - auc_21: 0.9427 - recall_21: 0.6354 - val_loss: 0.2288 - val_acc: 0.9120 - val_precision_21: 0.7118 - val_auc_21: 0.9250 - val_recall_21: 0.7088
Epoch 14/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1833 - acc: 0.9168 - precision_21: 0.8334 - auc_21: 0.9476 - recall_21: 0.6881 - val_loss: 0.2036 - val_acc: 0.9205 - val_precision_21: 0.8780 - val_auc_21: 0.9520 - val_recall_21: 0.5839
Epoch 15/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1728 - acc: 0.9214 - precision_21: 0.8647 - auc_21: 0.9529 - recall_21: 0.6818 - val_loss: 0.1925 - val_acc: 0.9275 - val_precision_21: 0.8097 - val_auc_21: 0.9475 - val_recall_21: 0.7280
Epoch 16/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1474 - acc: 0.9298 - precision_21: 0.8664 - auc_21: 0.9661 - recall_21: 0.7351 - val_loss: 0.1451 - val_acc: 0.9440 - val_precision_21: 0.8502 - val_auc_21: 0.9699 - val_recall_21: 0.7758
Epoch 17/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1423 - acc: 0.9318 - precision_21: 0.8733 - auc_21: 0.9693 - recall_21: 0.7419 - val_loss: 0.1379 - val_acc: 0.9437 - val_precision_21: 0.8323 - val_auc_21: 0.9754 - val_recall_21: 0.8046
Epoch 18/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1400 - acc: 0.9343 - precision_21: 0.8751 - auc_21: 0.9682 - recall_21: 0.7618 - val_loss: 0.1308 - val_acc: 0.9479 - val_precision_21: 0.8150 - val_auc_21: 0.9808 - val_recall_21: 0.8805
Epoch 19/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1180 - acc: 0.9389 - precision_21: 0.8881 - auc_21: 0.9804 - recall_21: 0.7831 - val_loss: 0.1127 - val_acc: 0.9555 - val_precision_21: 0.8648 - val_auc_21: 0.9823 - val_recall_21: 0.8169
Epoch 20/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1336 - acc: 0.9362 - precision_21: 0.8835 - auc_21: 0.9722 - recall_21: 0.7714 - val_loss: 0.1042 - val_acc: 0.9602 - val_precision_21: 0.9108 - val_auc_21: 0.9876 - val_recall_21: 0.8235
Epoch 21/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1319 - acc: 0.9369 - precision_21: 0.8817 - auc_21: 0.9724 - recall_21: 0.7750 - val_loss: 0.0979 - val_acc: 0.9629 - val_precision_21: 0.9209 - val_auc_21: 0.9888 - val_recall_21: 0.8152
Epoch 22/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1083 - acc: 0.9436 - precision_21: 0.9046 - auc_21: 0.9831 - recall_21: 0.7986 - val_loss: 0.1046 - val_acc: 0.9592 - val_precision_21: 0.8878 - val_auc_21: 0.9857 - val_recall_21: 0.8322
Epoch 23/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1087 - acc: 0.9436 - precision_21: 0.8979 - auc_21: 0.9827 - recall_21: 0.8111 - val_loss: 0.1159 - val_acc: 0.9528 - val_precision_21: 0.8578 - val_auc_21: 0.9835 - val_recall_21: 0.8536
Epoch 24/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1117 - acc: 0.9431 - precision_21: 0.9004 - auc_21: 0.9816 - recall_21: 0.8018 - val_loss: 0.0921 - val_acc: 0.9632 - val_precision_21: 0.8550 - val_auc_21: 0.9896 - val_recall_21: 0.9003
Epoch 25/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1075 - acc: 0.9445 - precision_21: 0.9065 - auc_21: 0.9831 - recall_21: 0.8074 - val_loss: 0.1032 - val_acc: 0.9582 - val_precision_21: 0.8309 - val_auc_21: 0.9890 - val_recall_21: 0.9218
Epoch 26/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1065 - acc: 0.9450 - precision_21: 0.8959 - auc_21: 0.9833 - recall_21: 0.8237 - val_loss: 0.0907 - val_acc: 0.9654 - val_precision_21: 0.8928 - val_auc_21: 0.9891 - val_recall_21: 0.8841
Epoch 27/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1138 - acc: 0.9439 - precision_21: 0.9000 - auc_21: 0.9798 - recall_21: 0.8061 - val_loss: 0.0841 - val_acc: 0.9673 - val_precision_21: 0.8855 - val_auc_21: 0.9903 - val_recall_21: 0.9028
Epoch 28/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1002 - acc: 0.9480 - precision_21: 0.9118 - auc_21: 0.9849 - recall_21: 0.8213 - val_loss: 0.0880 - val_acc: 0.9662 - val_precision_21: 0.9054 - val_auc_21: 0.9890 - val_recall_21: 0.8591
Epoch 29/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0966 - acc: 0.9482 - precision_21: 0.9077 - auc_21: 0.9863 - recall_21: 0.8277 - val_loss: 0.0994 - val_acc: 0.9617 - val_precision_21: 0.9040 - val_auc_21: 0.9870 - val_recall_21: 0.8656
Epoch 30/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0941 - acc: 0.9504 - precision_21: 0.9205 - auc_21: 0.9862 - recall_21: 0.8280 - val_loss: 0.0996 - val_acc: 0.9606 - val_precision_21: 0.8438 - val_auc_21: 0.9890 - val_recall_21: 0.9165
Epoch 31/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1208 - acc: 0.9385 - precision_21: 0.8779 - auc_21: 0.9785 - recall_21: 0.7950 - val_loss: 0.0965 - val_acc: 0.9629 - val_precision_21: 0.9061 - val_auc_21: 0.9879 - val_recall_21: 0.8625
Epoch 32/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1068 - acc: 0.9443 - precision_21: 0.8950 - auc_21: 0.9827 - recall_21: 0.8150 - val_loss: 0.0920 - val_acc: 0.9641 - val_precision_21: 0.9030 - val_auc_21: 0.9895 - val_recall_21: 0.8555
Epoch 33/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0930 - acc: 0.9494 - precision_21: 0.9162 - auc_21: 0.9867 - recall_21: 0.8280 - val_loss: 0.0886 - val_acc: 0.9655 - val_precision_21: 0.8968 - val_auc_21: 0.9900 - val_recall_21: 0.8826
Epoch 34/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1090 - acc: 0.9459 - precision_21: 0.9051 - auc_21: 0.9818 - recall_21: 0.8138 - val_loss: 0.0932 - val_acc: 0.9633 - val_precision_21: 0.8816 - val_auc_21: 0.9885 - val_recall_21: 0.8861
Epoch 35/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1006 - acc: 0.9491 - precision_21: 0.9129 - auc_21: 0.9824 - recall_21: 0.8276 - val_loss: 0.0904 - val_acc: 0.9645 - val_precision_21: 0.8723 - val_auc_21: 0.9915 - val_recall_21: 0.9354
Epoch 36/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0931 - acc: 0.9507 - precision_21: 0.9219 - auc_21: 0.9869 - recall_21: 0.8303 - val_loss: 0.0797 - val_acc: 0.9683 - val_precision_21: 0.8892 - val_auc_21: 0.9913 - val_recall_21: 0.9136
Epoch 37/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0878 - acc: 0.9533 - precision_21: 0.9247 - auc_21: 0.9870 - recall_21: 0.8401 - val_loss: 0.0843 - val_acc: 0.9668 - val_precision_21: 0.8774 - val_auc_21: 0.9903 - val_recall_21: 0.8951
Epoch 38/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0808 - acc: 0.9552 - precision_21: 0.9308 - auc_21: 0.9897 - recall_21: 0.8483 - val_loss: 0.0870 - val_acc: 0.9658 - val_precision_21: 0.8996 - val_auc_21: 0.9900 - val_recall_21: 0.8889
Epoch 39/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0866 - acc: 0.9538 - precision_21: 0.9269 - auc_21: 0.9876 - recall_21: 0.8427 - val_loss: 0.0914 - val_acc: 0.9656 - val_precision_21: 0.9214 - val_auc_21: 0.9889 - val_recall_21: 0.8593
Epoch 40/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0891 - acc: 0.9514 - precision_21: 0.9184 - auc_21: 0.9877 - recall_21: 0.8382 - val_loss: 0.0919 - val_acc: 0.9645 - val_precision_21: 0.9169 - val_auc_21: 0.9889 - val_recall_21: 0.8483
Epoch 41/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1028 - acc: 0.9487 - precision_21: 0.9075 - auc_21: 0.9820 - recall_21: 0.8269 - val_loss: 0.0838 - val_acc: 0.9680 - val_precision_21: 0.9075 - val_auc_21: 0.9898 - val_recall_21: 0.8870
Epoch 42/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0839 - acc: 0.9536 - precision_21: 0.9269 - auc_21: 0.9890 - recall_21: 0.8427 - val_loss: 0.0761 - val_acc: 0.9704 - val_precision_21: 0.8957 - val_auc_21: 0.9923 - val_recall_21: 0.9163
Epoch 43/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0787 - acc: 0.9546 - precision_21: 0.9273 - auc_21: 0.9908 - recall_21: 0.8507 - val_loss: 0.0866 - val_acc: 0.9662 - val_precision_21: 0.8874 - val_auc_21: 0.9912 - val_recall_21: 0.9055
Epoch 44/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0879 - acc: 0.9529 - precision_21: 0.9227 - auc_21: 0.9873 - recall_21: 0.8445 - val_loss: 0.0837 - val_acc: 0.9667 - val_precision_21: 0.8632 - val_auc_21: 0.9925 - val_recall_21: 0.9257
Epoch 45/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0932 - acc: 0.9517 - precision_21: 0.9206 - auc_21: 0.9843 - recall_21: 0.8347 - val_loss: 0.0872 - val_acc: 0.9648 - val_precision_21: 0.8798 - val_auc_21: 0.9903 - val_recall_21: 0.8923
Epoch 46/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0946 - acc: 0.9523 - precision_21: 0.9242 - auc_21: 0.9838 - recall_21: 0.8357 - val_loss: 0.0789 - val_acc: 0.9691 - val_precision_21: 0.8818 - val_auc_21: 0.9925 - val_recall_21: 0.9141
Epoch 47/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0828 - acc: 0.9549 - precision_21: 0.9280 - auc_21: 0.9883 - recall_21: 0.8535 - val_loss: 0.0812 - val_acc: 0.9688 - val_precision_21: 0.9211 - val_auc_21: 0.9904 - val_recall_21: 0.8849
Epoch 48/100
33/33 [==============================] - 3s 88ms/step - loss: 0.1002 - acc: 0.9527 - precision_21: 0.9252 - auc_21: 0.9830 - recall_21: 0.8415 - val_loss: 0.0750 - val_acc: 0.9705 - val_precision_21: 0.8932 - val_auc_21: 0.9930 - val_recall_21: 0.9215
Epoch 49/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0828 - acc: 0.9535 - precision_21: 0.9232 - auc_21: 0.9896 - recall_21: 0.8434 - val_loss: 0.0828 - val_acc: 0.9671 - val_precision_21: 0.8825 - val_auc_21: 0.9920 - val_recall_21: 0.9122
Epoch 50/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0746 - acc: 0.9562 - precision_21: 0.9306 - auc_21: 0.9916 - recall_21: 0.8593 - val_loss: 0.0794 - val_acc: 0.9690 - val_precision_21: 0.8997 - val_auc_21: 0.9916 - val_recall_21: 0.9020
Epoch 51/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0788 - acc: 0.9562 - precision_21: 0.9347 - auc_21: 0.9896 - recall_21: 0.8522 - val_loss: 0.0833 - val_acc: 0.9674 - val_precision_21: 0.9117 - val_auc_21: 0.9907 - val_recall_21: 0.8821
Epoch 52/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0758 - acc: 0.9564 - precision_21: 0.9343 - auc_21: 0.9910 - recall_21: 0.8564 - val_loss: 0.1033 - val_acc: 0.9585 - val_precision_21: 0.8327 - val_auc_21: 0.9914 - val_recall_21: 0.9441
Epoch 53/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0799 - acc: 0.9547 - precision_21: 0.9283 - auc_21: 0.9903 - recall_21: 0.8485 - val_loss: 0.0846 - val_acc: 0.9661 - val_precision_21: 0.8367 - val_auc_21: 0.9920 - val_recall_21: 0.9392
Epoch 54/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0800 - acc: 0.9560 - precision_21: 0.9317 - auc_21: 0.9893 - recall_21: 0.8533 - val_loss: 0.0764 - val_acc: 0.9703 - val_precision_21: 0.9036 - val_auc_21: 0.9926 - val_recall_21: 0.9223
Epoch 55/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0772 - acc: 0.9559 - precision_21: 0.9351 - auc_21: 0.9903 - recall_21: 0.8474 - val_loss: 0.0823 - val_acc: 0.9682 - val_precision_21: 0.8886 - val_auc_21: 0.9899 - val_recall_21: 0.8956
Epoch 56/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0842 - acc: 0.9540 - precision_21: 0.9313 - auc_21: 0.9886 - recall_21: 0.8417 - val_loss: 0.0772 - val_acc: 0.9700 - val_precision_21: 0.8984 - val_auc_21: 0.9928 - val_recall_21: 0.9169
Epoch 57/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0773 - acc: 0.9563 - precision_21: 0.9312 - auc_21: 0.9900 - recall_21: 0.8582 - val_loss: 0.0757 - val_acc: 0.9702 - val_precision_21: 0.9017 - val_auc_21: 0.9925 - val_recall_21: 0.9100
Epoch 58/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0822 - acc: 0.9561 - precision_21: 0.9347 - auc_21: 0.9877 - recall_21: 0.8539 - val_loss: 0.0774 - val_acc: 0.9695 - val_precision_21: 0.9105 - val_auc_21: 0.9920 - val_recall_21: 0.9076
Epoch 59/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0808 - acc: 0.9559 - precision_21: 0.9342 - auc_21: 0.9891 - recall_21: 0.8522 - val_loss: 0.0730 - val_acc: 0.9708 - val_precision_21: 0.9042 - val_auc_21: 0.9924 - val_recall_21: 0.8930
Epoch 60/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0799 - acc: 0.9565 - precision_21: 0.9375 - auc_21: 0.9888 - recall_21: 0.8529 - val_loss: 0.0711 - val_acc: 0.9724 - val_precision_21: 0.9120 - val_auc_21: 0.9924 - val_recall_21: 0.9153
Epoch 61/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0685 - acc: 0.9583 - precision_21: 0.9404 - auc_21: 0.9929 - recall_21: 0.8619 - val_loss: 0.0757 - val_acc: 0.9703 - val_precision_21: 0.9094 - val_auc_21: 0.9921 - val_recall_21: 0.9095
Epoch 62/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0751 - acc: 0.9557 - precision_21: 0.9352 - auc_21: 0.9915 - recall_21: 0.8515 - val_loss: 0.0767 - val_acc: 0.9700 - val_precision_21: 0.8984 - val_auc_21: 0.9928 - val_recall_21: 0.9174
Epoch 63/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0875 - acc: 0.9540 - precision_21: 0.9242 - auc_21: 0.9863 - recall_21: 0.8517 - val_loss: 0.0750 - val_acc: 0.9710 - val_precision_21: 0.9074 - val_auc_21: 0.9921 - val_recall_21: 0.9141
Epoch 64/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0737 - acc: 0.9566 - precision_21: 0.9377 - auc_21: 0.9914 - recall_21: 0.8519 - val_loss: 0.0877 - val_acc: 0.9657 - val_precision_21: 0.8517 - val_auc_21: 0.9921 - val_recall_21: 0.9383
Epoch 65/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0899 - acc: 0.9548 - precision_21: 0.9281 - auc_21: 0.9857 - recall_21: 0.8492 - val_loss: 0.0760 - val_acc: 0.9709 - val_precision_21: 0.9218 - val_auc_21: 0.9913 - val_recall_21: 0.8906
Epoch 66/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0671 - acc: 0.9590 - precision_21: 0.9401 - auc_21: 0.9932 - recall_21: 0.8693 - val_loss: 0.0742 - val_acc: 0.9711 - val_precision_21: 0.9131 - val_auc_21: 0.9915 - val_recall_21: 0.8950
Epoch 67/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0791 - acc: 0.9582 - precision_21: 0.9415 - auc_21: 0.9888 - recall_21: 0.8561 - val_loss: 0.0790 - val_acc: 0.9686 - val_precision_21: 0.8795 - val_auc_21: 0.9917 - val_recall_21: 0.9175
Epoch 68/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0762 - acc: 0.9571 - precision_21: 0.9362 - auc_21: 0.9900 - recall_21: 0.8566 - val_loss: 0.0706 - val_acc: 0.9729 - val_precision_21: 0.9191 - val_auc_21: 0.9931 - val_recall_21: 0.9169
Epoch 69/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0921 - acc: 0.9551 - precision_21: 0.9325 - auc_21: 0.9844 - recall_21: 0.8497 - val_loss: 0.0782 - val_acc: 0.9700 - val_precision_21: 0.9195 - val_auc_21: 0.9905 - val_recall_21: 0.8930
Epoch 70/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0724 - acc: 0.9570 - precision_21: 0.9302 - auc_21: 0.9920 - recall_21: 0.8634 - val_loss: 0.0695 - val_acc: 0.9724 - val_precision_21: 0.9024 - val_auc_21: 0.9930 - val_recall_21: 0.9143
Epoch 71/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0970 - acc: 0.9530 - precision_21: 0.9253 - auc_21: 0.9836 - recall_21: 0.8419 - val_loss: 0.0746 - val_acc: 0.9708 - val_precision_21: 0.8934 - val_auc_21: 0.9923 - val_recall_21: 0.9193
Epoch 72/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0695 - acc: 0.9582 - precision_21: 0.9382 - auc_21: 0.9923 - recall_21: 0.8599 - val_loss: 0.0761 - val_acc: 0.9701 - val_precision_21: 0.8990 - val_auc_21: 0.9923 - val_recall_21: 0.9198
Epoch 73/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0885 - acc: 0.9552 - precision_21: 0.9321 - auc_21: 0.9860 - recall_21: 0.8506 - val_loss: 0.0757 - val_acc: 0.9701 - val_precision_21: 0.9054 - val_auc_21: 0.9921 - val_recall_21: 0.9045
Epoch 74/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0646 - acc: 0.9598 - precision_21: 0.9465 - auc_21: 0.9937 - recall_21: 0.8668 - val_loss: 0.0682 - val_acc: 0.9735 - val_precision_21: 0.9322 - val_auc_21: 0.9934 - val_recall_21: 0.9019
Epoch 75/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0637 - acc: 0.9599 - precision_21: 0.9495 - auc_21: 0.9937 - recall_21: 0.8631 - val_loss: 0.0708 - val_acc: 0.9725 - val_precision_21: 0.8898 - val_auc_21: 0.9938 - val_recall_21: 0.9343
Epoch 76/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0703 - acc: 0.9580 - precision_21: 0.9415 - auc_21: 0.9921 - recall_21: 0.8586 - val_loss: 0.0774 - val_acc: 0.9690 - val_precision_21: 0.8751 - val_auc_21: 0.9919 - val_recall_21: 0.9215
Epoch 77/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0674 - acc: 0.9589 - precision_21: 0.9438 - auc_21: 0.9929 - recall_21: 0.8618 - val_loss: 0.0722 - val_acc: 0.9717 - val_precision_21: 0.8921 - val_auc_21: 0.9938 - val_recall_21: 0.9246
Epoch 78/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0813 - acc: 0.9550 - precision_21: 0.9252 - auc_21: 0.9891 - recall_21: 0.8523 - val_loss: 0.0802 - val_acc: 0.9684 - val_precision_21: 0.9043 - val_auc_21: 0.9925 - val_recall_21: 0.9056
Epoch 79/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0752 - acc: 0.9571 - precision_21: 0.9372 - auc_21: 0.9909 - recall_21: 0.8596 - val_loss: 0.0738 - val_acc: 0.9716 - val_precision_21: 0.9214 - val_auc_21: 0.9917 - val_recall_21: 0.9000
Epoch 80/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0745 - acc: 0.9588 - precision_21: 0.9463 - auc_21: 0.9902 - recall_21: 0.8573 - val_loss: 0.0718 - val_acc: 0.9712 - val_precision_21: 0.8853 - val_auc_21: 0.9941 - val_recall_21: 0.9362
Epoch 81/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0955 - acc: 0.9553 - precision_21: 0.9316 - auc_21: 0.9820 - recall_21: 0.8502 - val_loss: 0.0732 - val_acc: 0.9717 - val_precision_21: 0.8962 - val_auc_21: 0.9921 - val_recall_21: 0.9274
Epoch 82/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0686 - acc: 0.9591 - precision_21: 0.9454 - auc_21: 0.9922 - recall_21: 0.8618 - val_loss: 0.0715 - val_acc: 0.9716 - val_precision_21: 0.8920 - val_auc_21: 0.9946 - val_recall_21: 0.9375
Epoch 83/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0947 - acc: 0.9549 - precision_21: 0.9310 - auc_21: 0.9820 - recall_21: 0.8507 - val_loss: 0.0723 - val_acc: 0.9729 - val_precision_21: 0.9279 - val_auc_21: 0.9925 - val_recall_21: 0.9069
Epoch 84/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0756 - acc: 0.9562 - precision_21: 0.9368 - auc_21: 0.9912 - recall_21: 0.8514 - val_loss: 0.0835 - val_acc: 0.9671 - val_precision_21: 0.8400 - val_auc_21: 0.9922 - val_recall_21: 0.9366
Epoch 85/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0733 - acc: 0.9581 - precision_21: 0.9399 - auc_21: 0.9911 - recall_21: 0.8618 - val_loss: 0.0805 - val_acc: 0.9694 - val_precision_21: 0.9250 - val_auc_21: 0.9904 - val_recall_21: 0.8827
Epoch 86/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0822 - acc: 0.9571 - precision_21: 0.9390 - auc_21: 0.9873 - recall_21: 0.8518 - val_loss: 0.0715 - val_acc: 0.9717 - val_precision_21: 0.9007 - val_auc_21: 0.9929 - val_recall_21: 0.9209
Epoch 87/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0713 - acc: 0.9588 - precision_21: 0.9440 - auc_21: 0.9916 - recall_21: 0.8623 - val_loss: 0.0704 - val_acc: 0.9724 - val_precision_21: 0.9267 - val_auc_21: 0.9928 - val_recall_21: 0.8999
Epoch 88/100
32/33 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9603 - precision_21: 0.9490 - auc_21: 0.9925 - recall_21: 0.8655Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 86ms/step - loss: 0.0659 - acc: 0.9602 - precision_21: 0.9493 - auc_21: 0.9926 - recall_21: 0.8649 - val_loss: 0.0701 - val_acc: 0.9725 - val_precision_21: 0.9218 - val_auc_21: 0.9934 - val_recall_21: 0.9134
Epoch 00088: early stopping
6/6 [==============================] - 0s 25ms/step - loss: 0.0686 - acc: 0.9728 - precision_21: 0.9145 - auc_21: 0.9944 - recall_21: 0.9182
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb612241b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8364730368755661
test_sensitivity 0.9263569291957019
test_specifitivity 0.9814373589292712
test_accuracy 0.9724962022569444
test_precision 0.9062861463633949
test_jaccard_score 0.8364730368755661
test_dicecoef 0.9162116319476725
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-113304.h5
[0. 0. 0. 0. 0.] [0.9724962  0.83647304 0.90628615 0.92635693 0.98143736]

-------------------------
Rep: 1
-------------------------

2021-09-25 11:33:04.853006: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:33:04.853063: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied gaussian blur on all input images
Applied normalization
Data augumentation on
Prep done
Training samples: 258, channel mean: 0.20280802028486108,
Validation samples: 63, channel mean: 0.20159956655730865
Model built.
Model: "functional_45"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_23 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_418 (Conv2D)             (None, 256, 256, 32) 320         input_23[0][0]
__________________________________________________________________________________________________
conv2d_419 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_418[0][0]
__________________________________________________________________________________________________
max_pooling2d_88 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_419[0][0]
__________________________________________________________________________________________________
conv2d_420 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_88[0][0]
__________________________________________________________________________________________________
conv2d_421 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_420[0][0]
__________________________________________________________________________________________________
max_pooling2d_89 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_421[0][0]
__________________________________________________________________________________________________
conv2d_422 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_89[0][0]
__________________________________________________________________________________________________
conv2d_423 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_422[0][0]
__________________________________________________________________________________________________
max_pooling2d_90 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_423[0][0]
__________________________________________________________________________________________________
conv2d_424 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_90[0][0]
__________________________________________________________________________________________________
conv2d_425 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_424[0][0]
__________________________________________________________________________________________________
max_pooling2d_91 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_425[0][0]
__________________________________________________________________________________________________
conv2d_426 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_91[0][0]
__________________________________________________________________________________________________
conv2d_427 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_426[0][0]
__________________________________________________________________________________________________
up_sampling2d_88 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_427[0][0]
__________________________________________________________________________________________________
concatenate_88 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_88[0][0]
                                                                 conv2d_425[0][0]
__________________________________________________________________________________________________
conv2d_428 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_88[0][0]
__________________________________________________________________________________________________
conv2d_429 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_428[0][0]
__________________________________________________________________________________________________
up_sampling2d_89 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_429[0][0]
__________________________________________________________________________________________________
concatenate_89 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_89[0][0]
                                                                 conv2d_423[0][0]
__________________________________________________________________________________________________
conv2d_430 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_89[0][0]
__________________________________________________________________________________________________
conv2d_431 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_430[0][0]
__________________________________________________________________________________________________
up_sampling2d_90 (UpSampling2D) (None, 128, 128, 128 0           conv2d_431[0][0]
__________________________________________________________________________________________________
concatenate_90 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_90[0][0]
                                                                 conv2d_421[0][0]
__________________________________________________________________________________________________
conv2d_432 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_90[0][0]
__________________________________________________________________________________________________
conv2d_433 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_432[0][0]
__________________________________________________________________________________________________
up_sampling2d_91 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_433[0][0]
__________________________________________________________________________________________________
concatenate_91 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_91[0][0]
                                                                 conv2d_419[0][0]
__________________________________________________________________________________________________
conv2d_434 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_91[0][0]
__________________________________________________________________________________________________
conv2d_435 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_434[0][0]
__________________________________________________________________________________________________
conv2d_436 (Conv2D)             (None, 256, 256, 1)  33          conv2d_435[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6932 - acc: 0.1727 - precision_22: 0.1588 - auc_22: 0.8289 - recall_22: 0.99662021-09-25 11:33:10.000376: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:33:10.000429: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 11:33:10.281335: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 11:33:10.290802: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-113304/train/plugins/profile/2021_09_25_11_33_10
2021-09-25 11:33:10.294560: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-113304/train/plugins/profile/2021_09_25_11_33_10/xeon-e-v100.trace.json.gz
2021-09-25 11:33:10.314510: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-113304/train/plugins/profile/2021_09_25_11_33_10
2021-09-25 11:33:10.321634: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-113304/train/plugins/profile/2021_09_25_11_33_10/xeon-e-v100.memory_profile.json.gz
2021-09-25 11:33:10.344264: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-113304/train/plugins/profile/2021_09_25_11_33_10Dumped tool data for xplane.pb to logs/fit/20210925-113304/train/plugins/profile/2021_09_25_11_33_10/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-113304/train/plugins/profile/2021_09_25_11_33_10/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-113304/train/plugins/profile/2021_09_25_11_33_10/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-113304/train/plugins/profile/2021_09_25_11_33_10/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-113304/train/plugins/profile/2021_09_25_11_33_10/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6922 - acc: 0.4875 - precision_22: 0.1738 - auc_22: 0.6310 - recall_22: 0.4658WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0658s vs `on_train_batch_end` time: 0.2795s). Check your callbacks.
33/33 [==============================] - 4s 107ms/step - loss: 0.5944 - acc: 0.8595 - precision_22: 0.6247 - auc_22: 0.6859 - recall_22: 0.3850 - val_loss: 0.3362 - val_acc: 0.8927 - val_precision_22: 0.7236 - val_auc_22: 0.8441 - val_recall_22: 0.5145
Epoch 2/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2430 - acc: 0.8957 - precision_22: 0.7708 - auc_22: 0.9042 - recall_22: 0.5895 - val_loss: 0.2236 - val_acc: 0.9162 - val_precision_22: 0.8382 - val_auc_22: 0.9276 - val_recall_22: 0.5534
Epoch 3/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2207 - acc: 0.9023 - precision_22: 0.8060 - auc_22: 0.9256 - recall_22: 0.5975 - val_loss: 0.2365 - val_acc: 0.9128 - val_precision_22: 0.7621 - val_auc_22: 0.9256 - val_recall_22: 0.6332
Epoch 4/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2291 - acc: 0.9011 - precision_22: 0.8019 - auc_22: 0.9197 - recall_22: 0.5851 - val_loss: 0.3135 - val_acc: 0.8638 - val_precision_22: 0.5775 - val_auc_22: 0.8920 - val_recall_22: 0.7201
Epoch 5/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2275 - acc: 0.9005 - precision_22: 0.7937 - auc_22: 0.9210 - recall_22: 0.5967 - val_loss: 0.2184 - val_acc: 0.9158 - val_precision_22: 0.7526 - val_auc_22: 0.9315 - val_recall_22: 0.6786
Epoch 6/100
33/33 [==============================] - 3s 87ms/step - loss: 0.2014 - acc: 0.9102 - precision_22: 0.8475 - auc_22: 0.9387 - recall_22: 0.5937 - val_loss: 0.2122 - val_acc: 0.9195 - val_precision_22: 0.7335 - val_auc_22: 0.9411 - val_recall_22: 0.7702
Epoch 7/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1973 - acc: 0.9093 - precision_22: 0.8118 - auc_22: 0.9425 - recall_22: 0.6519 - val_loss: 0.1891 - val_acc: 0.9280 - val_precision_22: 0.8474 - val_auc_22: 0.9510 - val_recall_22: 0.6347
Epoch 8/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1989 - acc: 0.9096 - precision_22: 0.8151 - auc_22: 0.9397 - recall_22: 0.6435 - val_loss: 0.2108 - val_acc: 0.9159 - val_precision_22: 0.7744 - val_auc_22: 0.9399 - val_recall_22: 0.6818
Epoch 9/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1842 - acc: 0.9162 - precision_22: 0.8527 - auc_22: 0.9490 - recall_22: 0.6524 - val_loss: 0.2256 - val_acc: 0.9111 - val_precision_22: 0.6963 - val_auc_22: 0.9418 - val_recall_22: 0.7715
Epoch 10/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1809 - acc: 0.9155 - precision_22: 0.8226 - auc_22: 0.9520 - recall_22: 0.6947 - val_loss: 0.1924 - val_acc: 0.9235 - val_precision_22: 0.8259 - val_auc_22: 0.9511 - val_recall_22: 0.6435
Epoch 11/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1894 - acc: 0.9124 - precision_22: 0.8332 - auc_22: 0.9466 - recall_22: 0.6490 - val_loss: 0.2022 - val_acc: 0.9103 - val_precision_22: 0.7235 - val_auc_22: 0.9528 - val_recall_22: 0.7097
Epoch 12/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1659 - acc: 0.9183 - precision_22: 0.8303 - auc_22: 0.9614 - recall_22: 0.7028 - val_loss: 0.1805 - val_acc: 0.9299 - val_precision_22: 0.7794 - val_auc_22: 0.9479 - val_recall_22: 0.7181
Epoch 13/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1812 - acc: 0.9152 - precision_22: 0.8371 - auc_22: 0.9537 - recall_22: 0.6704 - val_loss: 0.1921 - val_acc: 0.9185 - val_precision_22: 0.7180 - val_auc_22: 0.9536 - val_recall_22: 0.7642
Epoch 14/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1597 - acc: 0.9220 - precision_22: 0.8408 - auc_22: 0.9651 - recall_22: 0.7279 - val_loss: 0.1517 - val_acc: 0.9383 - val_precision_22: 0.8592 - val_auc_22: 0.9718 - val_recall_22: 0.7338
Epoch 15/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1635 - acc: 0.9228 - precision_22: 0.8501 - auc_22: 0.9608 - recall_22: 0.7101 - val_loss: 0.2408 - val_acc: 0.9119 - val_precision_22: 0.8486 - val_auc_22: 0.9311 - val_recall_22: 0.5613
Epoch 16/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1696 - acc: 0.9235 - precision_22: 0.8601 - auc_22: 0.9544 - recall_22: 0.6990 - val_loss: 0.1564 - val_acc: 0.9347 - val_precision_22: 0.8533 - val_auc_22: 0.9691 - val_recall_22: 0.6996
Epoch 17/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1425 - acc: 0.9320 - precision_22: 0.8792 - auc_22: 0.9696 - recall_22: 0.7390 - val_loss: 0.1513 - val_acc: 0.9393 - val_precision_22: 0.7781 - val_auc_22: 0.9756 - val_recall_22: 0.8593
Epoch 18/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1413 - acc: 0.9332 - precision_22: 0.8731 - auc_22: 0.9687 - recall_22: 0.7545 - val_loss: 0.1320 - val_acc: 0.9486 - val_precision_22: 0.8259 - val_auc_22: 0.9797 - val_recall_22: 0.8677
Epoch 19/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1263 - acc: 0.9397 - precision_22: 0.8925 - auc_22: 0.9753 - recall_22: 0.7823 - val_loss: 0.1065 - val_acc: 0.9592 - val_precision_22: 0.8669 - val_auc_22: 0.9832 - val_recall_22: 0.8449
Epoch 20/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1227 - acc: 0.9394 - precision_22: 0.8933 - auc_22: 0.9768 - recall_22: 0.7782 - val_loss: 0.0972 - val_acc: 0.9620 - val_precision_22: 0.8850 - val_auc_22: 0.9874 - val_recall_22: 0.8678
Epoch 21/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1190 - acc: 0.9425 - precision_22: 0.9002 - auc_22: 0.9780 - recall_22: 0.7917 - val_loss: 0.0969 - val_acc: 0.9623 - val_precision_22: 0.9038 - val_auc_22: 0.9874 - val_recall_22: 0.8297
Epoch 22/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1090 - acc: 0.9451 - precision_22: 0.9082 - auc_22: 0.9816 - recall_22: 0.8009 - val_loss: 0.1058 - val_acc: 0.9581 - val_precision_22: 0.8559 - val_auc_22: 0.9852 - val_recall_22: 0.8655
Epoch 23/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1105 - acc: 0.9457 - precision_22: 0.9002 - auc_22: 0.9807 - recall_22: 0.8149 - val_loss: 0.1040 - val_acc: 0.9602 - val_precision_22: 0.8844 - val_auc_22: 0.9850 - val_recall_22: 0.8711
Epoch 24/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0942 - acc: 0.9504 - precision_22: 0.9180 - auc_22: 0.9865 - recall_22: 0.8321 - val_loss: 0.0974 - val_acc: 0.9608 - val_precision_22: 0.8317 - val_auc_22: 0.9893 - val_recall_22: 0.9163
Epoch 25/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0993 - acc: 0.9484 - precision_22: 0.9128 - auc_22: 0.9852 - recall_22: 0.8224 - val_loss: 0.0981 - val_acc: 0.9618 - val_precision_22: 0.8712 - val_auc_22: 0.9875 - val_recall_22: 0.8883
Epoch 26/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0911 - acc: 0.9510 - precision_22: 0.9154 - auc_22: 0.9875 - recall_22: 0.8413 - val_loss: 0.0920 - val_acc: 0.9652 - val_precision_22: 0.8930 - val_auc_22: 0.9879 - val_recall_22: 0.8826
Epoch 27/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1060 - acc: 0.9457 - precision_22: 0.9005 - auc_22: 0.9828 - recall_22: 0.8200 - val_loss: 0.0960 - val_acc: 0.9627 - val_precision_22: 0.9043 - val_auc_22: 0.9875 - val_recall_22: 0.8453
Epoch 28/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1079 - acc: 0.9474 - precision_22: 0.9062 - auc_22: 0.9813 - recall_22: 0.8206 - val_loss: 0.0865 - val_acc: 0.9671 - val_precision_22: 0.9045 - val_auc_22: 0.9890 - val_recall_22: 0.8670
Epoch 29/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1092 - acc: 0.9458 - precision_22: 0.8968 - auc_22: 0.9805 - recall_22: 0.8187 - val_loss: 0.1017 - val_acc: 0.9617 - val_precision_22: 0.9138 - val_auc_22: 0.9864 - val_recall_22: 0.8538
Epoch 30/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0954 - acc: 0.9520 - precision_22: 0.9223 - auc_22: 0.9840 - recall_22: 0.8378 - val_loss: 0.0896 - val_acc: 0.9670 - val_precision_22: 0.9036 - val_auc_22: 0.9893 - val_recall_22: 0.8823
Epoch 31/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0931 - acc: 0.9504 - precision_22: 0.9151 - auc_22: 0.9865 - recall_22: 0.8364 - val_loss: 0.0879 - val_acc: 0.9660 - val_precision_22: 0.9105 - val_auc_22: 0.9897 - val_recall_22: 0.8785
Epoch 32/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0858 - acc: 0.9527 - precision_22: 0.9207 - auc_22: 0.9886 - recall_22: 0.8433 - val_loss: 0.0865 - val_acc: 0.9667 - val_precision_22: 0.9093 - val_auc_22: 0.9891 - val_recall_22: 0.8674
Epoch 33/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0991 - acc: 0.9512 - precision_22: 0.9195 - auc_22: 0.9829 - recall_22: 0.8352 - val_loss: 0.0890 - val_acc: 0.9665 - val_precision_22: 0.9141 - val_auc_22: 0.9893 - val_recall_22: 0.8690
Epoch 34/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0930 - acc: 0.9504 - precision_22: 0.9137 - auc_22: 0.9870 - recall_22: 0.8348 - val_loss: 0.0874 - val_acc: 0.9665 - val_precision_22: 0.9029 - val_auc_22: 0.9903 - val_recall_22: 0.8820
Epoch 35/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0935 - acc: 0.9519 - precision_22: 0.9226 - auc_22: 0.9841 - recall_22: 0.8347 - val_loss: 0.0842 - val_acc: 0.9672 - val_precision_22: 0.9028 - val_auc_22: 0.9911 - val_recall_22: 0.9120
Epoch 36/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0887 - acc: 0.9546 - precision_22: 0.9328 - auc_22: 0.9870 - recall_22: 0.8435 - val_loss: 0.0764 - val_acc: 0.9702 - val_precision_22: 0.9091 - val_auc_22: 0.9915 - val_recall_22: 0.9015
Epoch 37/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0751 - acc: 0.9569 - precision_22: 0.9356 - auc_22: 0.9910 - recall_22: 0.8545 - val_loss: 0.0867 - val_acc: 0.9668 - val_precision_22: 0.8665 - val_auc_22: 0.9903 - val_recall_22: 0.9100
Epoch 38/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0735 - acc: 0.9569 - precision_22: 0.9350 - auc_22: 0.9916 - recall_22: 0.8559 - val_loss: 0.0834 - val_acc: 0.9671 - val_precision_22: 0.9040 - val_auc_22: 0.9911 - val_recall_22: 0.8926
Epoch 39/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0804 - acc: 0.9556 - precision_22: 0.9321 - auc_22: 0.9891 - recall_22: 0.8493 - val_loss: 0.0841 - val_acc: 0.9681 - val_precision_22: 0.9279 - val_auc_22: 0.9903 - val_recall_22: 0.8688
Epoch 40/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0997 - acc: 0.9517 - precision_22: 0.9247 - auc_22: 0.9821 - recall_22: 0.8315 - val_loss: 0.0827 - val_acc: 0.9679 - val_precision_22: 0.9014 - val_auc_22: 0.9901 - val_recall_22: 0.8912
Epoch 41/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0764 - acc: 0.9556 - precision_22: 0.9310 - auc_22: 0.9911 - recall_22: 0.8513 - val_loss: 0.0801 - val_acc: 0.9688 - val_precision_22: 0.9145 - val_auc_22: 0.9910 - val_recall_22: 0.8847
Epoch 42/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0805 - acc: 0.9552 - precision_22: 0.9338 - auc_22: 0.9896 - recall_22: 0.8473 - val_loss: 0.0754 - val_acc: 0.9697 - val_precision_22: 0.8863 - val_auc_22: 0.9928 - val_recall_22: 0.9239
Epoch 43/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0747 - acc: 0.9564 - precision_22: 0.9373 - auc_22: 0.9915 - recall_22: 0.8496 - val_loss: 0.0835 - val_acc: 0.9677 - val_precision_22: 0.8941 - val_auc_22: 0.9913 - val_recall_22: 0.9076
Epoch 44/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0799 - acc: 0.9551 - precision_22: 0.9307 - auc_22: 0.9900 - recall_22: 0.8516 - val_loss: 0.0792 - val_acc: 0.9679 - val_precision_22: 0.8766 - val_auc_22: 0.9923 - val_recall_22: 0.9158
Epoch 45/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1049 - acc: 0.9519 - precision_22: 0.9212 - auc_22: 0.9786 - recall_22: 0.8343 - val_loss: 0.0995 - val_acc: 0.9596 - val_precision_22: 0.8406 - val_auc_22: 0.9890 - val_recall_22: 0.9093
Epoch 46/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0914 - acc: 0.9535 - precision_22: 0.9275 - auc_22: 0.9851 - recall_22: 0.8387 - val_loss: 0.0892 - val_acc: 0.9650 - val_precision_22: 0.8463 - val_auc_22: 0.9918 - val_recall_22: 0.9335
Epoch 47/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0853 - acc: 0.9550 - precision_22: 0.9300 - auc_22: 0.9871 - recall_22: 0.8519 - val_loss: 0.0834 - val_acc: 0.9683 - val_precision_22: 0.9026 - val_auc_22: 0.9901 - val_recall_22: 0.9039
Epoch 48/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1125 - acc: 0.9497 - precision_22: 0.9183 - auc_22: 0.9789 - recall_22: 0.8276 - val_loss: 0.0830 - val_acc: 0.9687 - val_precision_22: 0.9204 - val_auc_22: 0.9893 - val_recall_22: 0.8754
Epoch 49/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0885 - acc: 0.9538 - precision_22: 0.9270 - auc_22: 0.9868 - recall_22: 0.8441 - val_loss: 0.0813 - val_acc: 0.9681 - val_precision_22: 0.8875 - val_auc_22: 0.9914 - val_recall_22: 0.9130
Epoch 50/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0865 - acc: 0.9544 - precision_22: 0.9267 - auc_22: 0.9872 - recall_22: 0.8484 - val_loss: 0.0813 - val_acc: 0.9684 - val_precision_22: 0.8966 - val_auc_22: 0.9911 - val_recall_22: 0.9012
Epoch 51/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1035 - acc: 0.9475 - precision_22: 0.9090 - auc_22: 0.9829 - recall_22: 0.8194 - val_loss: 0.0952 - val_acc: 0.9618 - val_precision_22: 0.9176 - val_auc_22: 0.9898 - val_recall_22: 0.8369
Epoch 52/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0867 - acc: 0.9526 - precision_22: 0.9239 - auc_22: 0.9881 - recall_22: 0.8417 - val_loss: 0.0888 - val_acc: 0.9647 - val_precision_22: 0.8709 - val_auc_22: 0.9921 - val_recall_22: 0.9289
Epoch 53/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0741 - acc: 0.9566 - precision_22: 0.9354 - auc_22: 0.9912 - recall_22: 0.8526 - val_loss: 0.0814 - val_acc: 0.9684 - val_precision_22: 0.8611 - val_auc_22: 0.9907 - val_recall_22: 0.9208
Epoch 54/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0727 - acc: 0.9569 - precision_22: 0.9348 - auc_22: 0.9916 - recall_22: 0.8571 - val_loss: 0.0771 - val_acc: 0.9695 - val_precision_22: 0.9009 - val_auc_22: 0.9930 - val_recall_22: 0.9203
Epoch 55/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0724 - acc: 0.9575 - precision_22: 0.9393 - auc_22: 0.9917 - recall_22: 0.8562 - val_loss: 0.0862 - val_acc: 0.9659 - val_precision_22: 0.8616 - val_auc_22: 0.9904 - val_recall_22: 0.9149
Epoch 56/100
32/33 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9563 - precision_22: 0.9354 - auc_22: 0.9905 - recall_22: 0.8531Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 86ms/step - loss: 0.0777 - acc: 0.9561 - precision_22: 0.9356 - auc_22: 0.9905 - recall_22: 0.8514 - val_loss: 0.0789 - val_acc: 0.9687 - val_precision_22: 0.8951 - val_auc_22: 0.9920 - val_recall_22: 0.9121
Epoch 00056: early stopping
6/6 [==============================] - 0s 25ms/step - loss: 0.0803 - acc: 0.9679 - precision_22: 0.8807 - auc_22: 0.9929 - recall_22: 0.9282
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbdc86f9790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8137942945491358
test_sensitivity 0.9376054878762052
test_specifitivity 0.9720793299201098
test_accuracy 0.9664832221137153
test_precision 0.8668006867078769
test_jaccard_score 0.8137942945491358
test_dicecoef 0.9008138989986564
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-113606.h5
[0.9724962  0.83647304 0.90628615 0.92635693 0.98143736] [0.96648322 0.81379429 0.86680069 0.93760549 0.97207933]

-------------------------
Rep: 2
-------------------------

2021-09-25 11:36:06.973655: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:36:06.973742: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied gaussian blur on all input images
Applied normalization
Data augumentation on
Prep done
Training samples: 258, channel mean: 0.20280802028486108,
Validation samples: 63, channel mean: 0.20159956655730865
Model built.
Model: "functional_47"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_24 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_437 (Conv2D)             (None, 256, 256, 32) 320         input_24[0][0]
__________________________________________________________________________________________________
conv2d_438 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_437[0][0]
__________________________________________________________________________________________________
max_pooling2d_92 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_438[0][0]
__________________________________________________________________________________________________
conv2d_439 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_92[0][0]
__________________________________________________________________________________________________
conv2d_440 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_439[0][0]
__________________________________________________________________________________________________
max_pooling2d_93 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_440[0][0]
__________________________________________________________________________________________________
conv2d_441 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_93[0][0]
__________________________________________________________________________________________________
conv2d_442 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_441[0][0]
__________________________________________________________________________________________________
max_pooling2d_94 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_442[0][0]
__________________________________________________________________________________________________
conv2d_443 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_94[0][0]
__________________________________________________________________________________________________
conv2d_444 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_443[0][0]
__________________________________________________________________________________________________
max_pooling2d_95 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_444[0][0]
__________________________________________________________________________________________________
conv2d_445 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_95[0][0]
__________________________________________________________________________________________________
conv2d_446 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_445[0][0]
__________________________________________________________________________________________________
up_sampling2d_92 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_446[0][0]
__________________________________________________________________________________________________
concatenate_92 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_92[0][0]
                                                                 conv2d_444[0][0]
__________________________________________________________________________________________________
conv2d_447 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_92[0][0]
__________________________________________________________________________________________________
conv2d_448 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_447[0][0]
__________________________________________________________________________________________________
up_sampling2d_93 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_448[0][0]
__________________________________________________________________________________________________
concatenate_93 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_93[0][0]
                                                                 conv2d_442[0][0]
__________________________________________________________________________________________________
conv2d_449 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_93[0][0]
__________________________________________________________________________________________________
conv2d_450 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_449[0][0]
__________________________________________________________________________________________________
up_sampling2d_94 (UpSampling2D) (None, 128, 128, 128 0           conv2d_450[0][0]
__________________________________________________________________________________________________
concatenate_94 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_94[0][0]
                                                                 conv2d_440[0][0]
__________________________________________________________________________________________________
conv2d_451 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_94[0][0]
__________________________________________________________________________________________________
conv2d_452 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_451[0][0]
__________________________________________________________________________________________________
up_sampling2d_95 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_452[0][0]
__________________________________________________________________________________________________
concatenate_95 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_95[0][0]
                                                                 conv2d_438[0][0]
__________________________________________________________________________________________________
conv2d_453 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_95[0][0]
__________________________________________________________________________________________________
conv2d_454 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_453[0][0]
__________________________________________________________________________________________________
conv2d_455 (Conv2D)             (None, 256, 256, 1)  33          conv2d_454[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6929 - acc: 0.7678 - precision_23: 0.2169 - auc_23: 0.3106 - recall_23: 0.18152021-09-25 11:36:12.124052: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:36:12.124109: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 11:36:12.407603: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 11:36:12.417143: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-113606/train/plugins/profile/2021_09_25_11_36_12
2021-09-25 11:36:12.420649: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-113606/train/plugins/profile/2021_09_25_11_36_12/xeon-e-v100.trace.json.gz
2021-09-25 11:36:12.440577: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-113606/train/plugins/profile/2021_09_25_11_36_12
2021-09-25 11:36:12.446919: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-113606/train/plugins/profile/2021_09_25_11_36_12/xeon-e-v100.memory_profile.json.gz
2021-09-25 11:36:12.465364: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-113606/train/plugins/profile/2021_09_25_11_36_12Dumped tool data for xplane.pb to logs/fit/20210925-113606/train/plugins/profile/2021_09_25_11_36_12/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-113606/train/plugins/profile/2021_09_25_11_36_12/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-113606/train/plugins/profile/2021_09_25_11_36_12/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-113606/train/plugins/profile/2021_09_25_11_36_12/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-113606/train/plugins/profile/2021_09_25_11_36_12/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6898 - acc: 0.7814 - precision_23: 0.2646 - auc_23: 0.4156 - recall_23: 0.0994WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0649s vs `on_train_batch_end` time: 0.2781s). Check your callbacks.
33/33 [==============================] - 4s 108ms/step - loss: 0.6693 - acc: 0.8643 - precision_23: 0.8260 - auc_23: 0.6580 - recall_23: 0.2022 - val_loss: 0.3027 - val_acc: 0.8748 - val_precision_23: 0.5841 - val_auc_23: 0.8885 - val_recall_23: 0.7108
Epoch 2/100
33/33 [==============================] - 3s 87ms/step - loss: 0.2467 - acc: 0.8922 - precision_23: 0.7414 - auc_23: 0.8999 - recall_23: 0.6068 - val_loss: 0.2696 - val_acc: 0.9067 - val_precision_23: 0.7345 - val_auc_23: 0.8972 - val_recall_23: 0.6008
Epoch 3/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2372 - acc: 0.8979 - precision_23: 0.7808 - auc_23: 0.9141 - recall_23: 0.5943 - val_loss: 0.2654 - val_acc: 0.9045 - val_precision_23: 0.7039 - val_auc_23: 0.9068 - val_recall_23: 0.6588
Epoch 4/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2237 - acc: 0.9015 - precision_23: 0.7957 - auc_23: 0.9243 - recall_23: 0.6017 - val_loss: 0.2949 - val_acc: 0.8796 - val_precision_23: 0.6343 - val_auc_23: 0.8930 - val_recall_23: 0.6787
Epoch 5/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2209 - acc: 0.9004 - precision_23: 0.7924 - auc_23: 0.9281 - recall_23: 0.6070 - val_loss: 0.2674 - val_acc: 0.9080 - val_precision_23: 0.8658 - val_auc_23: 0.9161 - val_recall_23: 0.4789
Epoch 6/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2309 - acc: 0.8996 - precision_23: 0.7857 - auc_23: 0.9142 - recall_23: 0.5839 - val_loss: 0.2424 - val_acc: 0.9119 - val_precision_23: 0.7816 - val_auc_23: 0.9038 - val_recall_23: 0.6138
Epoch 7/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2068 - acc: 0.9055 - precision_23: 0.8327 - auc_23: 0.9351 - recall_23: 0.5837 - val_loss: 0.2224 - val_acc: 0.9164 - val_precision_23: 0.7700 - val_auc_23: 0.9222 - val_recall_23: 0.6322
Epoch 8/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2164 - acc: 0.9032 - precision_23: 0.8143 - auc_23: 0.9289 - recall_23: 0.5882 - val_loss: 0.2627 - val_acc: 0.9017 - val_precision_23: 0.7388 - val_auc_23: 0.8963 - val_recall_23: 0.6124
Epoch 9/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2097 - acc: 0.9068 - precision_23: 0.8344 - auc_23: 0.9314 - recall_23: 0.5967 - val_loss: 0.2623 - val_acc: 0.8996 - val_precision_23: 0.6748 - val_auc_23: 0.9113 - val_recall_23: 0.6987
Epoch 10/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2179 - acc: 0.9049 - precision_23: 0.8219 - auc_23: 0.9277 - recall_23: 0.5973 - val_loss: 0.2463 - val_acc: 0.9086 - val_precision_23: 0.8935 - val_auc_23: 0.9081 - val_recall_23: 0.4685
Epoch 11/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2082 - acc: 0.9067 - precision_23: 0.8323 - auc_23: 0.9332 - recall_23: 0.6010 - val_loss: 0.2430 - val_acc: 0.9070 - val_precision_23: 0.7953 - val_auc_23: 0.9118 - val_recall_23: 0.5626
Epoch 12/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2096 - acc: 0.9089 - precision_23: 0.8352 - auc_23: 0.9298 - recall_23: 0.6139 - val_loss: 0.2230 - val_acc: 0.9155 - val_precision_23: 0.6984 - val_auc_23: 0.9278 - val_recall_23: 0.7309
Epoch 13/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2311 - acc: 0.9008 - precision_23: 0.7922 - auc_23: 0.9196 - recall_23: 0.6056 - val_loss: 0.2636 - val_acc: 0.9013 - val_precision_23: 0.6994 - val_auc_23: 0.8993 - val_recall_23: 0.6159
Epoch 14/100
33/33 [==============================] - 3s 85ms/step - loss: 0.2146 - acc: 0.9039 - precision_23: 0.8015 - auc_23: 0.9309 - recall_23: 0.6271 - val_loss: 0.2448 - val_acc: 0.9072 - val_precision_23: 0.8124 - val_auc_23: 0.9092 - val_recall_23: 0.5451
Epoch 15/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2136 - acc: 0.9043 - precision_23: 0.8318 - auc_23: 0.9286 - recall_23: 0.5864 - val_loss: 0.2531 - val_acc: 0.9059 - val_precision_23: 0.7733 - val_auc_23: 0.9027 - val_recall_23: 0.6003
Epoch 16/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2091 - acc: 0.9084 - precision_23: 0.8348 - auc_23: 0.9302 - recall_23: 0.6108 - val_loss: 0.2315 - val_acc: 0.9140 - val_precision_23: 0.7440 - val_auc_23: 0.9163 - val_recall_23: 0.6804
Epoch 17/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1969 - acc: 0.9106 - precision_23: 0.8480 - auc_23: 0.9415 - recall_23: 0.6119 - val_loss: 0.2302 - val_acc: 0.9188 - val_precision_23: 0.7461 - val_auc_23: 0.9250 - val_recall_23: 0.7336
Epoch 18/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1963 - acc: 0.9111 - precision_23: 0.8280 - auc_23: 0.9422 - recall_23: 0.6474 - val_loss: 0.2138 - val_acc: 0.9145 - val_precision_23: 0.7388 - val_auc_23: 0.9414 - val_recall_23: 0.7367
Epoch 19/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1748 - acc: 0.9199 - precision_23: 0.8465 - auc_23: 0.9531 - recall_23: 0.6890 - val_loss: 0.1611 - val_acc: 0.9354 - val_precision_23: 0.8912 - val_auc_23: 0.9667 - val_recall_23: 0.6257
Epoch 20/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1669 - acc: 0.9217 - precision_23: 0.8521 - auc_23: 0.9591 - recall_23: 0.6981 - val_loss: 0.1586 - val_acc: 0.9403 - val_precision_23: 0.8899 - val_auc_23: 0.9670 - val_recall_23: 0.7013
Epoch 21/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1599 - acc: 0.9248 - precision_23: 0.8544 - auc_23: 0.9610 - recall_23: 0.7214 - val_loss: 0.1396 - val_acc: 0.9473 - val_precision_23: 0.8540 - val_auc_23: 0.9702 - val_recall_23: 0.7697
Epoch 22/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1394 - acc: 0.9335 - precision_23: 0.8851 - auc_23: 0.9711 - recall_23: 0.7447 - val_loss: 0.1355 - val_acc: 0.9497 - val_precision_23: 0.8584 - val_auc_23: 0.9776 - val_recall_23: 0.7946
Epoch 23/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1379 - acc: 0.9330 - precision_23: 0.8681 - auc_23: 0.9725 - recall_23: 0.7631 - val_loss: 0.1242 - val_acc: 0.9497 - val_precision_23: 0.8509 - val_auc_23: 0.9812 - val_recall_23: 0.8408
Epoch 24/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1295 - acc: 0.9367 - precision_23: 0.8831 - auc_23: 0.9757 - recall_23: 0.7766 - val_loss: 0.1063 - val_acc: 0.9578 - val_precision_23: 0.8324 - val_auc_23: 0.9866 - val_recall_23: 0.8900
Epoch 25/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1282 - acc: 0.9359 - precision_23: 0.8829 - auc_23: 0.9770 - recall_23: 0.7744 - val_loss: 0.1362 - val_acc: 0.9433 - val_precision_23: 0.7771 - val_auc_23: 0.9820 - val_recall_23: 0.8973
Epoch 26/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1378 - acc: 0.9335 - precision_23: 0.8626 - auc_23: 0.9736 - recall_23: 0.7822 - val_loss: 0.1059 - val_acc: 0.9600 - val_precision_23: 0.8758 - val_auc_23: 0.9847 - val_recall_23: 0.8659
Epoch 27/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1262 - acc: 0.9393 - precision_23: 0.8888 - auc_23: 0.9760 - recall_23: 0.7862 - val_loss: 0.0899 - val_acc: 0.9652 - val_precision_23: 0.8855 - val_auc_23: 0.9895 - val_recall_23: 0.8865
Epoch 28/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1000 - acc: 0.9462 - precision_23: 0.9043 - auc_23: 0.9856 - recall_23: 0.8167 - val_loss: 0.0935 - val_acc: 0.9630 - val_precision_23: 0.8546 - val_auc_23: 0.9892 - val_recall_23: 0.9013
Epoch 29/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1126 - acc: 0.9418 - precision_23: 0.8913 - auc_23: 0.9823 - recall_23: 0.7990 - val_loss: 0.1118 - val_acc: 0.9564 - val_precision_23: 0.8580 - val_auc_23: 0.9851 - val_recall_23: 0.8889
Epoch 30/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1025 - acc: 0.9459 - precision_23: 0.9026 - auc_23: 0.9841 - recall_23: 0.8195 - val_loss: 0.0961 - val_acc: 0.9632 - val_precision_23: 0.8723 - val_auc_23: 0.9880 - val_recall_23: 0.8949
Epoch 31/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0930 - acc: 0.9485 - precision_23: 0.9077 - auc_23: 0.9881 - recall_23: 0.8343 - val_loss: 0.0967 - val_acc: 0.9617 - val_precision_23: 0.8914 - val_auc_23: 0.9880 - val_recall_23: 0.8721
Epoch 32/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1117 - acc: 0.9426 - precision_23: 0.8881 - auc_23: 0.9812 - recall_23: 0.8113 - val_loss: 0.1244 - val_acc: 0.9519 - val_precision_23: 0.8074 - val_auc_23: 0.9838 - val_recall_23: 0.8981
Epoch 33/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1125 - acc: 0.9442 - precision_23: 0.8992 - auc_23: 0.9802 - recall_23: 0.8146 - val_loss: 0.0957 - val_acc: 0.9624 - val_precision_23: 0.8931 - val_auc_23: 0.9891 - val_recall_23: 0.8645
Epoch 34/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1105 - acc: 0.9442 - precision_23: 0.9032 - auc_23: 0.9821 - recall_23: 0.8048 - val_loss: 0.1032 - val_acc: 0.9587 - val_precision_23: 0.8533 - val_auc_23: 0.9876 - val_recall_23: 0.8911
Epoch 35/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1033 - acc: 0.9454 - precision_23: 0.8995 - auc_23: 0.9839 - recall_23: 0.8197 - val_loss: 0.1054 - val_acc: 0.9576 - val_precision_23: 0.8426 - val_auc_23: 0.9899 - val_recall_23: 0.9344
Epoch 36/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1020 - acc: 0.9480 - precision_23: 0.9133 - auc_23: 0.9853 - recall_23: 0.8183 - val_loss: 0.0839 - val_acc: 0.9675 - val_precision_23: 0.9031 - val_auc_23: 0.9902 - val_recall_23: 0.8898
Epoch 37/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0952 - acc: 0.9496 - precision_23: 0.9135 - auc_23: 0.9858 - recall_23: 0.8266 - val_loss: 0.0991 - val_acc: 0.9604 - val_precision_23: 0.8285 - val_auc_23: 0.9891 - val_recall_23: 0.9153
Epoch 38/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0918 - acc: 0.9504 - precision_23: 0.9160 - auc_23: 0.9876 - recall_23: 0.8310 - val_loss: 0.0932 - val_acc: 0.9626 - val_precision_23: 0.8833 - val_auc_23: 0.9891 - val_recall_23: 0.8878
Epoch 39/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0925 - acc: 0.9508 - precision_23: 0.9158 - auc_23: 0.9862 - recall_23: 0.8360 - val_loss: 0.0935 - val_acc: 0.9647 - val_precision_23: 0.9098 - val_auc_23: 0.9878 - val_recall_23: 0.8667
Epoch 40/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0900 - acc: 0.9515 - precision_23: 0.9191 - auc_23: 0.9878 - recall_23: 0.8396 - val_loss: 0.0924 - val_acc: 0.9649 - val_precision_23: 0.9067 - val_auc_23: 0.9875 - val_recall_23: 0.8628
Epoch 41/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0937 - acc: 0.9505 - precision_23: 0.9137 - auc_23: 0.9863 - recall_23: 0.8333 - val_loss: 0.0845 - val_acc: 0.9668 - val_precision_23: 0.8886 - val_auc_23: 0.9907 - val_recall_23: 0.9021
Epoch 42/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0828 - acc: 0.9525 - precision_23: 0.9194 - auc_23: 0.9903 - recall_23: 0.8459 - val_loss: 0.0800 - val_acc: 0.9686 - val_precision_23: 0.8842 - val_auc_23: 0.9921 - val_recall_23: 0.9188
Epoch 43/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0947 - acc: 0.9500 - precision_23: 0.9171 - auc_23: 0.9860 - recall_23: 0.8274 - val_loss: 0.0915 - val_acc: 0.9655 - val_precision_23: 0.8942 - val_auc_23: 0.9903 - val_recall_23: 0.8918
Epoch 44/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0921 - acc: 0.9515 - precision_23: 0.9186 - auc_23: 0.9865 - recall_23: 0.8389 - val_loss: 0.0842 - val_acc: 0.9652 - val_precision_23: 0.8636 - val_auc_23: 0.9912 - val_recall_23: 0.9133
Epoch 45/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1054 - acc: 0.9497 - precision_23: 0.9115 - auc_23: 0.9801 - recall_23: 0.8321 - val_loss: 0.0940 - val_acc: 0.9622 - val_precision_23: 0.8711 - val_auc_23: 0.9882 - val_recall_23: 0.8844
Epoch 46/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0962 - acc: 0.9518 - precision_23: 0.9219 - auc_23: 0.9844 - recall_23: 0.8344 - val_loss: 0.0814 - val_acc: 0.9683 - val_precision_23: 0.8751 - val_auc_23: 0.9922 - val_recall_23: 0.9169
Epoch 47/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0900 - acc: 0.9527 - precision_23: 0.9194 - auc_23: 0.9867 - recall_23: 0.8473 - val_loss: 0.0886 - val_acc: 0.9647 - val_precision_23: 0.8837 - val_auc_23: 0.9898 - val_recall_23: 0.9035
Epoch 48/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0953 - acc: 0.9518 - precision_23: 0.9196 - auc_23: 0.9854 - recall_23: 0.8408 - val_loss: 0.0791 - val_acc: 0.9688 - val_precision_23: 0.8987 - val_auc_23: 0.9912 - val_recall_23: 0.9020
Epoch 49/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0801 - acc: 0.9548 - precision_23: 0.9292 - auc_23: 0.9899 - recall_23: 0.8455 - val_loss: 0.0836 - val_acc: 0.9661 - val_precision_23: 0.8665 - val_auc_23: 0.9918 - val_recall_23: 0.9276
Epoch 50/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0942 - acc: 0.9517 - precision_23: 0.9163 - auc_23: 0.9854 - recall_23: 0.8407 - val_loss: 0.0841 - val_acc: 0.9678 - val_precision_23: 0.8888 - val_auc_23: 0.9912 - val_recall_23: 0.9075
Epoch 51/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0850 - acc: 0.9547 - precision_23: 0.9295 - auc_23: 0.9873 - recall_23: 0.8466 - val_loss: 0.0841 - val_acc: 0.9670 - val_precision_23: 0.9087 - val_auc_23: 0.9913 - val_recall_23: 0.8833
Epoch 52/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0758 - acc: 0.9556 - precision_23: 0.9294 - auc_23: 0.9913 - recall_23: 0.8573 - val_loss: 0.1128 - val_acc: 0.9521 - val_precision_23: 0.8121 - val_auc_23: 0.9893 - val_recall_23: 0.9321
Epoch 53/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0827 - acc: 0.9532 - precision_23: 0.9203 - auc_23: 0.9898 - recall_23: 0.8476 - val_loss: 0.0860 - val_acc: 0.9666 - val_precision_23: 0.8423 - val_auc_23: 0.9913 - val_recall_23: 0.9347
Epoch 54/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0969 - acc: 0.9518 - precision_23: 0.9174 - auc_23: 0.9853 - recall_23: 0.8398 - val_loss: 0.0849 - val_acc: 0.9669 - val_precision_23: 0.8843 - val_auc_23: 0.9918 - val_recall_23: 0.9251
Epoch 55/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0848 - acc: 0.9534 - precision_23: 0.9266 - auc_23: 0.9884 - recall_23: 0.8411 - val_loss: 0.0830 - val_acc: 0.9671 - val_precision_23: 0.8735 - val_auc_23: 0.9907 - val_recall_23: 0.9073
Epoch 56/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0740 - acc: 0.9562 - precision_23: 0.9311 - auc_23: 0.9918 - recall_23: 0.8597 - val_loss: 0.0762 - val_acc: 0.9700 - val_precision_23: 0.9014 - val_auc_23: 0.9927 - val_recall_23: 0.9136
Epoch 57/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0775 - acc: 0.9564 - precision_23: 0.9337 - auc_23: 0.9900 - recall_23: 0.8561 - val_loss: 0.0821 - val_acc: 0.9675 - val_precision_23: 0.8772 - val_auc_23: 0.9923 - val_recall_23: 0.9226
Epoch 58/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1111 - acc: 0.9445 - precision_23: 0.8978 - auc_23: 0.9814 - recall_23: 0.8172 - val_loss: 0.1115 - val_acc: 0.9558 - val_precision_23: 0.8783 - val_auc_23: 0.9838 - val_recall_23: 0.8548
Epoch 59/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1052 - acc: 0.9453 - precision_23: 0.9003 - auc_23: 0.9840 - recall_23: 0.8200 - val_loss: 0.0891 - val_acc: 0.9648 - val_precision_23: 0.8882 - val_auc_23: 0.9895 - val_recall_23: 0.8653
Epoch 60/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0947 - acc: 0.9504 - precision_23: 0.9169 - auc_23: 0.9852 - recall_23: 0.8358 - val_loss: 0.0840 - val_acc: 0.9667 - val_precision_23: 0.8720 - val_auc_23: 0.9921 - val_recall_23: 0.9280
Epoch 61/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0831 - acc: 0.9527 - precision_23: 0.9192 - auc_23: 0.9897 - recall_23: 0.8476 - val_loss: 0.0869 - val_acc: 0.9659 - val_precision_23: 0.9037 - val_auc_23: 0.9904 - val_recall_23: 0.8864
Epoch 62/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0957 - acc: 0.9503 - precision_23: 0.9198 - auc_23: 0.9852 - recall_23: 0.8310 - val_loss: 0.0940 - val_acc: 0.9643 - val_precision_23: 0.8712 - val_auc_23: 0.9901 - val_recall_23: 0.9135
Epoch 63/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0878 - acc: 0.9517 - precision_23: 0.9175 - auc_23: 0.9878 - recall_23: 0.8404 - val_loss: 0.0811 - val_acc: 0.9681 - val_precision_23: 0.8924 - val_auc_23: 0.9916 - val_recall_23: 0.9125
Epoch 64/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0948 - acc: 0.9522 - precision_23: 0.9272 - auc_23: 0.9839 - recall_23: 0.8280 - val_loss: 0.0886 - val_acc: 0.9647 - val_precision_23: 0.8525 - val_auc_23: 0.9916 - val_recall_23: 0.9291
Epoch 65/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0904 - acc: 0.9535 - precision_23: 0.9230 - auc_23: 0.9860 - recall_23: 0.8463 - val_loss: 0.0766 - val_acc: 0.9703 - val_precision_23: 0.9072 - val_auc_23: 0.9918 - val_recall_23: 0.9036
Epoch 66/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0867 - acc: 0.9542 - precision_23: 0.9239 - auc_23: 0.9875 - recall_23: 0.8526 - val_loss: 0.1037 - val_acc: 0.9596 - val_precision_23: 0.9066 - val_auc_23: 0.9863 - val_recall_23: 0.8188
Epoch 67/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0774 - acc: 0.9552 - precision_23: 0.9286 - auc_23: 0.9914 - recall_23: 0.8526 - val_loss: 0.0818 - val_acc: 0.9673 - val_precision_23: 0.8782 - val_auc_23: 0.9910 - val_recall_23: 0.9095
Epoch 68/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0762 - acc: 0.9552 - precision_23: 0.9266 - auc_23: 0.9912 - recall_23: 0.8582 - val_loss: 0.0739 - val_acc: 0.9712 - val_precision_23: 0.9134 - val_auc_23: 0.9924 - val_recall_23: 0.9124
Epoch 69/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0844 - acc: 0.9551 - precision_23: 0.9351 - auc_23: 0.9881 - recall_23: 0.8465 - val_loss: 0.0839 - val_acc: 0.9681 - val_precision_23: 0.9294 - val_auc_23: 0.9899 - val_recall_23: 0.8689
Epoch 70/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0728 - acc: 0.9571 - precision_23: 0.9345 - auc_23: 0.9917 - recall_23: 0.8576 - val_loss: 0.0725 - val_acc: 0.9717 - val_precision_23: 0.9140 - val_auc_23: 0.9921 - val_recall_23: 0.8952
Epoch 71/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0755 - acc: 0.9564 - precision_23: 0.9359 - auc_23: 0.9913 - recall_23: 0.8561 - val_loss: 0.0733 - val_acc: 0.9704 - val_precision_23: 0.8886 - val_auc_23: 0.9930 - val_recall_23: 0.9222
Epoch 72/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0735 - acc: 0.9576 - precision_23: 0.9376 - auc_23: 0.9910 - recall_23: 0.8563 - val_loss: 0.0789 - val_acc: 0.9690 - val_precision_23: 0.8927 - val_auc_23: 0.9927 - val_recall_23: 0.9199
Epoch 73/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1058 - acc: 0.9505 - precision_23: 0.9191 - auc_23: 0.9801 - recall_23: 0.8313 - val_loss: 0.0759 - val_acc: 0.9701 - val_precision_23: 0.8962 - val_auc_23: 0.9919 - val_recall_23: 0.9158
Epoch 74/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0926 - acc: 0.9539 - precision_23: 0.9243 - auc_23: 0.9847 - recall_23: 0.8503 - val_loss: 0.0760 - val_acc: 0.9699 - val_precision_23: 0.8913 - val_auc_23: 0.9927 - val_recall_23: 0.9275
Epoch 75/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0912 - acc: 0.9535 - precision_23: 0.9285 - auc_23: 0.9859 - recall_23: 0.8391 - val_loss: 0.0786 - val_acc: 0.9687 - val_precision_23: 0.8753 - val_auc_23: 0.9923 - val_recall_23: 0.9255
Epoch 76/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0739 - acc: 0.9562 - precision_23: 0.9330 - auc_23: 0.9916 - recall_23: 0.8561 - val_loss: 0.0736 - val_acc: 0.9699 - val_precision_23: 0.8858 - val_auc_23: 0.9927 - val_recall_23: 0.9138
Epoch 77/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0713 - acc: 0.9571 - precision_23: 0.9369 - auc_23: 0.9927 - recall_23: 0.8567 - val_loss: 0.0699 - val_acc: 0.9725 - val_precision_23: 0.8872 - val_auc_23: 0.9943 - val_recall_23: 0.9370
Epoch 78/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0801 - acc: 0.9557 - precision_23: 0.9316 - auc_23: 0.9892 - recall_23: 0.8509 - val_loss: 0.0821 - val_acc: 0.9667 - val_precision_23: 0.8795 - val_auc_23: 0.9925 - val_recall_23: 0.9260
Epoch 79/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0895 - acc: 0.9551 - precision_23: 0.9306 - auc_23: 0.9862 - recall_23: 0.8502 - val_loss: 0.0733 - val_acc: 0.9712 - val_precision_23: 0.9050 - val_auc_23: 0.9920 - val_recall_23: 0.9169
Epoch 80/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0673 - acc: 0.9589 - precision_23: 0.9454 - auc_23: 0.9929 - recall_23: 0.8609 - val_loss: 0.0765 - val_acc: 0.9683 - val_precision_23: 0.8714 - val_auc_23: 0.9936 - val_recall_23: 0.9338
Epoch 81/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0741 - acc: 0.9580 - precision_23: 0.9407 - auc_23: 0.9907 - recall_23: 0.8619 - val_loss: 0.0729 - val_acc: 0.9710 - val_precision_23: 0.8949 - val_auc_23: 0.9932 - val_recall_23: 0.9241
Epoch 82/100
33/33 [==============================] - 3s 89ms/step - loss: 0.0829 - acc: 0.9562 - precision_23: 0.9350 - auc_23: 0.9879 - recall_23: 0.8519 - val_loss: 0.0757 - val_acc: 0.9690 - val_precision_23: 0.8822 - val_auc_23: 0.9937 - val_recall_23: 0.9320
Epoch 83/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0963 - acc: 0.9545 - precision_23: 0.9280 - auc_23: 0.9821 - recall_23: 0.8511 - val_loss: 0.0737 - val_acc: 0.9714 - val_precision_23: 0.9097 - val_auc_23: 0.9926 - val_recall_23: 0.9189
Epoch 84/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0821 - acc: 0.9570 - precision_23: 0.9395 - auc_23: 0.9877 - recall_23: 0.8506 - val_loss: 0.0679 - val_acc: 0.9734 - val_precision_23: 0.8855 - val_auc_23: 0.9929 - val_recall_23: 0.9238
Epoch 85/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0801 - acc: 0.9576 - precision_23: 0.9385 - auc_23: 0.9889 - recall_23: 0.8584 - val_loss: 0.0790 - val_acc: 0.9693 - val_precision_23: 0.9071 - val_auc_23: 0.9911 - val_recall_23: 0.9032
Epoch 86/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0841 - acc: 0.9570 - precision_23: 0.9388 - auc_23: 0.9874 - recall_23: 0.8521 - val_loss: 0.0728 - val_acc: 0.9706 - val_precision_23: 0.8928 - val_auc_23: 0.9930 - val_recall_23: 0.9236
Epoch 87/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0764 - acc: 0.9584 - precision_23: 0.9418 - auc_23: 0.9892 - recall_23: 0.8622 - val_loss: 0.0700 - val_acc: 0.9726 - val_precision_23: 0.9178 - val_auc_23: 0.9925 - val_recall_23: 0.9118
Epoch 88/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0624 - acc: 0.9607 - precision_23: 0.9498 - auc_23: 0.9939 - recall_23: 0.8692 - val_loss: 0.0691 - val_acc: 0.9725 - val_precision_23: 0.9255 - val_auc_23: 0.9936 - val_recall_23: 0.9095
Epoch 89/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0762 - acc: 0.9584 - precision_23: 0.9437 - auc_23: 0.9895 - recall_23: 0.8566 - val_loss: 0.0707 - val_acc: 0.9723 - val_precision_23: 0.9088 - val_auc_23: 0.9928 - val_recall_23: 0.9163
Epoch 90/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0763 - acc: 0.9574 - precision_23: 0.9417 - auc_23: 0.9904 - recall_23: 0.8607 - val_loss: 0.0732 - val_acc: 0.9718 - val_precision_23: 0.9200 - val_auc_23: 0.9919 - val_recall_23: 0.8920
Epoch 91/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0914 - acc: 0.9560 - precision_23: 0.9327 - auc_23: 0.9839 - recall_23: 0.8505 - val_loss: 0.0744 - val_acc: 0.9709 - val_precision_23: 0.9112 - val_auc_23: 0.9931 - val_recall_23: 0.9146
Epoch 92/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0760 - acc: 0.9556 - precision_23: 0.9352 - auc_23: 0.9915 - recall_23: 0.8496 - val_loss: 0.0770 - val_acc: 0.9699 - val_precision_23: 0.9027 - val_auc_23: 0.9919 - val_recall_23: 0.9028
Epoch 93/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0779 - acc: 0.9573 - precision_23: 0.9376 - auc_23: 0.9889 - recall_23: 0.8593 - val_loss: 0.0731 - val_acc: 0.9718 - val_precision_23: 0.9257 - val_auc_23: 0.9921 - val_recall_23: 0.9022
Epoch 94/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0672 - acc: 0.9588 - precision_23: 0.9460 - auc_23: 0.9931 - recall_23: 0.8632 - val_loss: 0.0741 - val_acc: 0.9706 - val_precision_23: 0.9054 - val_auc_23: 0.9931 - val_recall_23: 0.9172
Epoch 95/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0721 - acc: 0.9582 - precision_23: 0.9452 - auc_23: 0.9915 - recall_23: 0.8567 - val_loss: 0.0717 - val_acc: 0.9727 - val_precision_23: 0.8901 - val_auc_23: 0.9932 - val_recall_23: 0.9298
Epoch 96/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0711 - acc: 0.9583 - precision_23: 0.9426 - auc_23: 0.9912 - recall_23: 0.8568 - val_loss: 0.0779 - val_acc: 0.9680 - val_precision_23: 0.8764 - val_auc_23: 0.9924 - val_recall_23: 0.9239
Epoch 97/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0720 - acc: 0.9589 - precision_23: 0.9458 - auc_23: 0.9909 - recall_23: 0.8601 - val_loss: 0.0696 - val_acc: 0.9727 - val_precision_23: 0.9156 - val_auc_23: 0.9933 - val_recall_23: 0.9106
Epoch 98/100
32/33 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9581 - precision_23: 0.9417 - auc_23: 0.9909 - recall_23: 0.8579Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 86ms/step - loss: 0.0737 - acc: 0.9580 - precision_23: 0.9414 - auc_23: 0.9909 - recall_23: 0.8590 - val_loss: 0.0768 - val_acc: 0.9702 - val_precision_23: 0.9029 - val_auc_23: 0.9917 - val_recall_23: 0.9032
Epoch 00098: early stopping
6/6 [==============================] - 0s 26ms/step - loss: 0.0731 - acc: 0.9708 - precision_23: 0.8956 - auc_23: 0.9939 - recall_23: 0.9285
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb6121111f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8294773169885356
test_sensitivity 0.9337745024314433
test_specifitivity 0.9773335567796528
test_accuracy 0.9702626546223958
test_precision 0.8886821372552528
test_jaccard_score 0.8294773169885356
test_dicecoef 0.9106704680533784
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-114110.h5
[1.93897942 1.65026733 1.77308683 1.86396242 1.95351669] [0.97026265 0.82947732 0.88868214 0.9337745  0.97733356]

-------------------------
Averaged metrics for Baseline + Augumentations + Gaussian Blur - bacteria: [0.96974736 0.82658155 0.88725632 0.93257897 0.97695008]
-------------------------


-------------------------
RUN: Baseline + Augumentations + Histogram Equalization + Per Channel Normalization - bacteria, PARAMS: {'augumentation': True, 'histogram_equalization': True, 'per_channel_normalization': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-25 11:41:11.023778: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:41:11.023852: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied histogram equalization on all input images
Normalized per channel
Applied normalization
Data augumentation on
Prep done
Training samples: 258, channel mean: 9.372885757797167e-18,
Validation samples: 63, channel mean: -0.0027676740471074255
Model built.
Model: "functional_49"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_25 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_456 (Conv2D)             (None, 256, 256, 32) 320         input_25[0][0]
__________________________________________________________________________________________________
conv2d_457 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_456[0][0]
__________________________________________________________________________________________________
max_pooling2d_96 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_457[0][0]
__________________________________________________________________________________________________
conv2d_458 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_96[0][0]
__________________________________________________________________________________________________
conv2d_459 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_458[0][0]
__________________________________________________________________________________________________
max_pooling2d_97 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_459[0][0]
__________________________________________________________________________________________________
conv2d_460 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_97[0][0]
__________________________________________________________________________________________________
conv2d_461 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_460[0][0]
__________________________________________________________________________________________________
max_pooling2d_98 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_461[0][0]
__________________________________________________________________________________________________
conv2d_462 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_98[0][0]
__________________________________________________________________________________________________
conv2d_463 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_462[0][0]
__________________________________________________________________________________________________
max_pooling2d_99 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_463[0][0]
__________________________________________________________________________________________________
conv2d_464 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_99[0][0]
__________________________________________________________________________________________________
conv2d_465 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_464[0][0]
__________________________________________________________________________________________________
up_sampling2d_96 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_465[0][0]
__________________________________________________________________________________________________
concatenate_96 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_96[0][0]
                                                                 conv2d_463[0][0]
__________________________________________________________________________________________________
conv2d_466 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_96[0][0]
__________________________________________________________________________________________________
conv2d_467 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_466[0][0]
__________________________________________________________________________________________________
up_sampling2d_97 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_467[0][0]
__________________________________________________________________________________________________
concatenate_97 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_97[0][0]
                                                                 conv2d_461[0][0]
__________________________________________________________________________________________________
conv2d_468 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_97[0][0]
__________________________________________________________________________________________________
conv2d_469 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_468[0][0]
__________________________________________________________________________________________________
up_sampling2d_98 (UpSampling2D) (None, 128, 128, 128 0           conv2d_469[0][0]
__________________________________________________________________________________________________
concatenate_98 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_98[0][0]
                                                                 conv2d_459[0][0]
__________________________________________________________________________________________________
conv2d_470 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_98[0][0]
__________________________________________________________________________________________________
conv2d_471 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_470[0][0]
__________________________________________________________________________________________________
up_sampling2d_99 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_471[0][0]
__________________________________________________________________________________________________
concatenate_99 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_99[0][0]
                                                                 conv2d_457[0][0]
__________________________________________________________________________________________________
conv2d_472 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_99[0][0]
__________________________________________________________________________________________________
conv2d_473 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_472[0][0]
__________________________________________________________________________________________________
conv2d_474 (Conv2D)             (None, 256, 256, 1)  33          conv2d_473[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6926 - acc: 0.7510 - precision_24: 0.3730 - auc_24: 0.6824 - recall_24: 0.78132021-09-25 11:41:16.049894: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:41:16.049952: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 11:41:16.324417: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 11:41:16.336326: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-114111/train/plugins/profile/2021_09_25_11_41_16
2021-09-25 11:41:16.339843: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-114111/train/plugins/profile/2021_09_25_11_41_16/xeon-e-v100.trace.json.gz
2021-09-25 11:41:16.358602: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-114111/train/plugins/profile/2021_09_25_11_41_16
2021-09-25 11:41:16.365253: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-114111/train/plugins/profile/2021_09_25_11_41_16/xeon-e-v100.memory_profile.json.gz
2021-09-25 11:41:16.385214: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-114111/train/plugins/profile/2021_09_25_11_41_16Dumped tool data for xplane.pb to logs/fit/20210925-114111/train/plugins/profile/2021_09_25_11_41_16/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-114111/train/plugins/profile/2021_09_25_11_41_16/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-114111/train/plugins/profile/2021_09_25_11_41_16/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-114111/train/plugins/profile/2021_09_25_11_41_16/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-114111/train/plugins/profile/2021_09_25_11_41_16/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6882 - acc: 0.7973 - precision_24: 0.4795 - auc_24: 0.6985 - recall_24: 0.5737WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0656s vs `on_train_batch_end` time: 0.2714s). Check your callbacks.
33/33 [==============================] - 4s 107ms/step - loss: 0.6508 - acc: 0.8849 - precision_24: 0.8128 - auc_24: 0.7402 - recall_24: 0.4093 - val_loss: 0.2962 - val_acc: 0.9042 - val_precision_24: 0.7414 - val_auc_24: 0.8978 - val_recall_24: 0.6006
Epoch 2/100
33/33 [==============================] - 3s 87ms/step - loss: 0.2390 - acc: 0.9004 - precision_24: 0.8126 - auc_24: 0.9037 - recall_24: 0.5737 - val_loss: 0.2283 - val_acc: 0.9138 - val_precision_24: 0.7451 - val_auc_24: 0.9310 - val_recall_24: 0.6549
Epoch 3/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2214 - acc: 0.9045 - precision_24: 0.7977 - auc_24: 0.9204 - recall_24: 0.6409 - val_loss: 0.2243 - val_acc: 0.9141 - val_precision_24: 0.8258 - val_auc_24: 0.9329 - val_recall_24: 0.5629
Epoch 4/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2131 - acc: 0.9079 - precision_24: 0.8339 - auc_24: 0.9241 - recall_24: 0.6106 - val_loss: 0.2426 - val_acc: 0.9041 - val_precision_24: 0.7455 - val_auc_24: 0.9261 - val_recall_24: 0.6564
Epoch 5/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1866 - acc: 0.9147 - precision_24: 0.8437 - auc_24: 0.9471 - recall_24: 0.6591 - val_loss: 0.1722 - val_acc: 0.9305 - val_precision_24: 0.8123 - val_auc_24: 0.9617 - val_recall_24: 0.7159
Epoch 6/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1756 - acc: 0.9177 - precision_24: 0.8399 - auc_24: 0.9552 - recall_24: 0.6728 - val_loss: 0.1777 - val_acc: 0.9323 - val_precision_24: 0.8442 - val_auc_24: 0.9539 - val_recall_24: 0.7006
Epoch 7/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1726 - acc: 0.9193 - precision_24: 0.8405 - auc_24: 0.9559 - recall_24: 0.6930 - val_loss: 0.1666 - val_acc: 0.9346 - val_precision_24: 0.8467 - val_auc_24: 0.9651 - val_recall_24: 0.6891
Epoch 8/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1588 - acc: 0.9262 - precision_24: 0.8542 - auc_24: 0.9624 - recall_24: 0.7325 - val_loss: 0.2031 - val_acc: 0.9238 - val_precision_24: 0.8511 - val_auc_24: 0.9512 - val_recall_24: 0.6451
Epoch 9/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1661 - acc: 0.9231 - precision_24: 0.8371 - auc_24: 0.9572 - recall_24: 0.7273 - val_loss: 0.1641 - val_acc: 0.9391 - val_precision_24: 0.7795 - val_auc_24: 0.9690 - val_recall_24: 0.8551
Epoch 10/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1607 - acc: 0.9267 - precision_24: 0.8320 - auc_24: 0.9583 - recall_24: 0.7688 - val_loss: 0.1528 - val_acc: 0.9421 - val_precision_24: 0.9080 - val_auc_24: 0.9715 - val_recall_24: 0.6988
Epoch 11/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1404 - acc: 0.9336 - precision_24: 0.8685 - auc_24: 0.9666 - recall_24: 0.7668 - val_loss: 0.1250 - val_acc: 0.9523 - val_precision_24: 0.8562 - val_auc_24: 0.9803 - val_recall_24: 0.8429
Epoch 12/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1334 - acc: 0.9371 - precision_24: 0.8722 - auc_24: 0.9691 - recall_24: 0.7859 - val_loss: 0.1260 - val_acc: 0.9548 - val_precision_24: 0.8209 - val_auc_24: 0.9833 - val_recall_24: 0.8789
Epoch 13/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1200 - acc: 0.9392 - precision_24: 0.8801 - auc_24: 0.9781 - recall_24: 0.7956 - val_loss: 0.1122 - val_acc: 0.9564 - val_precision_24: 0.8184 - val_auc_24: 0.9864 - val_recall_24: 0.9169
Epoch 14/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1270 - acc: 0.9386 - precision_24: 0.8746 - auc_24: 0.9758 - recall_24: 0.7952 - val_loss: 0.1098 - val_acc: 0.9588 - val_precision_24: 0.8958 - val_auc_24: 0.9849 - val_recall_24: 0.8400
Epoch 15/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1138 - acc: 0.9415 - precision_24: 0.8943 - auc_24: 0.9809 - recall_24: 0.7952 - val_loss: 0.1279 - val_acc: 0.9557 - val_precision_24: 0.8650 - val_auc_24: 0.9741 - val_recall_24: 0.8641
Epoch 16/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1123 - acc: 0.9443 - precision_24: 0.8954 - auc_24: 0.9787 - recall_24: 0.8115 - val_loss: 0.0973 - val_acc: 0.9633 - val_precision_24: 0.8730 - val_auc_24: 0.9865 - val_recall_24: 0.8938
Epoch 17/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0946 - acc: 0.9489 - precision_24: 0.9158 - auc_24: 0.9860 - recall_24: 0.8181 - val_loss: 0.0900 - val_acc: 0.9649 - val_precision_24: 0.8738 - val_auc_24: 0.9900 - val_recall_24: 0.9080
Epoch 18/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1052 - acc: 0.9473 - precision_24: 0.9040 - auc_24: 0.9809 - recall_24: 0.8246 - val_loss: 0.0926 - val_acc: 0.9639 - val_precision_24: 0.8718 - val_auc_24: 0.9901 - val_recall_24: 0.9132
Epoch 19/100
33/33 [==============================] - 3s 88ms/step - loss: 0.0895 - acc: 0.9503 - precision_24: 0.9141 - auc_24: 0.9882 - recall_24: 0.8344 - val_loss: 0.0831 - val_acc: 0.9683 - val_precision_24: 0.9028 - val_auc_24: 0.9891 - val_recall_24: 0.8729
Epoch 20/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0969 - acc: 0.9498 - precision_24: 0.9169 - auc_24: 0.9842 - recall_24: 0.8265 - val_loss: 0.0764 - val_acc: 0.9701 - val_precision_24: 0.8994 - val_auc_24: 0.9927 - val_recall_24: 0.9091
Epoch 21/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1018 - acc: 0.9480 - precision_24: 0.9061 - auc_24: 0.9831 - recall_24: 0.8255 - val_loss: 0.0755 - val_acc: 0.9720 - val_precision_24: 0.9092 - val_auc_24: 0.9920 - val_recall_24: 0.8977
Epoch 22/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0830 - acc: 0.9532 - precision_24: 0.9250 - auc_24: 0.9897 - recall_24: 0.8418 - val_loss: 0.0791 - val_acc: 0.9689 - val_precision_24: 0.8979 - val_auc_24: 0.9910 - val_recall_24: 0.8933
Epoch 23/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1020 - acc: 0.9493 - precision_24: 0.9067 - auc_24: 0.9835 - recall_24: 0.8371 - val_loss: 0.0981 - val_acc: 0.9644 - val_precision_24: 0.9278 - val_auc_24: 0.9867 - val_recall_24: 0.8487
Epoch 24/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0971 - acc: 0.9500 - precision_24: 0.9159 - auc_24: 0.9841 - recall_24: 0.8312 - val_loss: 0.0752 - val_acc: 0.9704 - val_precision_24: 0.8967 - val_auc_24: 0.9924 - val_recall_24: 0.9011
Epoch 25/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0881 - acc: 0.9523 - precision_24: 0.9222 - auc_24: 0.9874 - recall_24: 0.8409 - val_loss: 0.0785 - val_acc: 0.9691 - val_precision_24: 0.8903 - val_auc_24: 0.9921 - val_recall_24: 0.9165
Epoch 26/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0905 - acc: 0.9524 - precision_24: 0.9187 - auc_24: 0.9865 - recall_24: 0.8465 - val_loss: 0.0843 - val_acc: 0.9655 - val_precision_24: 0.8690 - val_auc_24: 0.9913 - val_recall_24: 0.9170
Epoch 27/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0886 - acc: 0.9509 - precision_24: 0.9114 - auc_24: 0.9878 - recall_24: 0.8420 - val_loss: 0.0676 - val_acc: 0.9733 - val_precision_24: 0.9153 - val_auc_24: 0.9937 - val_recall_24: 0.9094
Epoch 28/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0763 - acc: 0.9553 - precision_24: 0.9300 - auc_24: 0.9909 - recall_24: 0.8512 - val_loss: 0.0794 - val_acc: 0.9694 - val_precision_24: 0.8933 - val_auc_24: 0.9909 - val_recall_24: 0.8992
Epoch 29/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0998 - acc: 0.9520 - precision_24: 0.9177 - auc_24: 0.9826 - recall_24: 0.8406 - val_loss: 0.0852 - val_acc: 0.9666 - val_precision_24: 0.9135 - val_auc_24: 0.9909 - val_recall_24: 0.8866
Epoch 30/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0803 - acc: 0.9543 - precision_24: 0.9299 - auc_24: 0.9897 - recall_24: 0.8443 - val_loss: 0.0733 - val_acc: 0.9717 - val_precision_24: 0.9101 - val_auc_24: 0.9931 - val_recall_24: 0.9077
Epoch 31/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0833 - acc: 0.9540 - precision_24: 0.9254 - auc_24: 0.9891 - recall_24: 0.8495 - val_loss: 0.0758 - val_acc: 0.9701 - val_precision_24: 0.9247 - val_auc_24: 0.9926 - val_recall_24: 0.8899
Epoch 32/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0777 - acc: 0.9542 - precision_24: 0.9273 - auc_24: 0.9907 - recall_24: 0.8487 - val_loss: 0.0950 - val_acc: 0.9663 - val_precision_24: 0.9255 - val_auc_24: 0.9844 - val_recall_24: 0.8465
Epoch 33/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0829 - acc: 0.9542 - precision_24: 0.9324 - auc_24: 0.9884 - recall_24: 0.8434 - val_loss: 0.0828 - val_acc: 0.9664 - val_precision_24: 0.8820 - val_auc_24: 0.9917 - val_recall_24: 0.9077
Epoch 34/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0876 - acc: 0.9510 - precision_24: 0.9128 - auc_24: 0.9886 - recall_24: 0.8399 - val_loss: 0.0778 - val_acc: 0.9695 - val_precision_24: 0.8952 - val_auc_24: 0.9922 - val_recall_24: 0.9131
Epoch 35/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0757 - acc: 0.9553 - precision_24: 0.9338 - auc_24: 0.9909 - recall_24: 0.8452 - val_loss: 0.0781 - val_acc: 0.9682 - val_precision_24: 0.8971 - val_auc_24: 0.9934 - val_recall_24: 0.9257
Epoch 36/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0926 - acc: 0.9538 - precision_24: 0.9293 - auc_24: 0.9861 - recall_24: 0.8419 - val_loss: 0.0691 - val_acc: 0.9736 - val_precision_24: 0.9313 - val_auc_24: 0.9932 - val_recall_24: 0.8995
Epoch 37/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0829 - acc: 0.9557 - precision_24: 0.9335 - auc_24: 0.9876 - recall_24: 0.8464 - val_loss: 0.0801 - val_acc: 0.9683 - val_precision_24: 0.8657 - val_auc_24: 0.9921 - val_recall_24: 0.9234
Epoch 38/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0821 - acc: 0.9552 - precision_24: 0.9297 - auc_24: 0.9893 - recall_24: 0.8481 - val_loss: 0.0755 - val_acc: 0.9697 - val_precision_24: 0.9117 - val_auc_24: 0.9927 - val_recall_24: 0.9015
Epoch 39/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0773 - acc: 0.9564 - precision_24: 0.9357 - auc_24: 0.9898 - recall_24: 0.8518 - val_loss: 0.0762 - val_acc: 0.9710 - val_precision_24: 0.9365 - val_auc_24: 0.9921 - val_recall_24: 0.8790
Epoch 40/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0742 - acc: 0.9575 - precision_24: 0.9396 - auc_24: 0.9910 - recall_24: 0.8583 - val_loss: 0.0701 - val_acc: 0.9715 - val_precision_24: 0.9070 - val_auc_24: 0.9936 - val_recall_24: 0.9100
Epoch 41/100
32/33 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9565 - precision_24: 0.9332 - auc_24: 0.9896 - recall_24: 0.8523Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 87ms/step - loss: 0.0793 - acc: 0.9564 - precision_24: 0.9329 - auc_24: 0.9896 - recall_24: 0.8517 - val_loss: 0.0703 - val_acc: 0.9723 - val_precision_24: 0.9202 - val_auc_24: 0.9927 - val_recall_24: 0.9025
Epoch 00041: early stopping
6/6 [==============================] - 0s 26ms/step - loss: 0.0741 - acc: 0.9710 - precision_24: 0.9070 - auc_24: 0.9929 - recall_24: 0.9152
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbe00563790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8256414991033354
test_sensitivity 0.9240278404438429
test_specifitivity 0.9801221830381575
test_accuracy 0.9710164388020833
test_precision 0.9000822033760357
test_jaccard_score 0.8256414991033354
test_dicecoef 0.9118978511469895
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-114328.h5
[0. 0. 0. 0. 0.] [0.97101644 0.8256415  0.9000822  0.92402784 0.98012218]

-------------------------
Rep: 1
-------------------------

2021-09-25 11:43:29.613947: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:43:29.614006: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied histogram equalization on all input images
Normalized per channel
Applied normalization
Data augumentation on
Prep done
Training samples: 258, channel mean: 9.372885757797167e-18,
Validation samples: 63, channel mean: -0.0027676740471074255
Model built.
Model: "functional_51"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_26 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_475 (Conv2D)             (None, 256, 256, 32) 320         input_26[0][0]
__________________________________________________________________________________________________
conv2d_476 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_475[0][0]
__________________________________________________________________________________________________
max_pooling2d_100 (MaxPooling2D (None, 128, 128, 32) 0           conv2d_476[0][0]
__________________________________________________________________________________________________
conv2d_477 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_100[0][0]
__________________________________________________________________________________________________
conv2d_478 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_477[0][0]
__________________________________________________________________________________________________
max_pooling2d_101 (MaxPooling2D (None, 64, 64, 64)   0           conv2d_478[0][0]
__________________________________________________________________________________________________
conv2d_479 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_101[0][0]
__________________________________________________________________________________________________
conv2d_480 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_479[0][0]
__________________________________________________________________________________________________
max_pooling2d_102 (MaxPooling2D (None, 32, 32, 128)  0           conv2d_480[0][0]
__________________________________________________________________________________________________
conv2d_481 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_102[0][0]
__________________________________________________________________________________________________
conv2d_482 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_481[0][0]
__________________________________________________________________________________________________
max_pooling2d_103 (MaxPooling2D (None, 16, 16, 256)  0           conv2d_482[0][0]
__________________________________________________________________________________________________
conv2d_483 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_103[0][0]
__________________________________________________________________________________________________
conv2d_484 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_483[0][0]
__________________________________________________________________________________________________
up_sampling2d_100 (UpSampling2D (None, 32, 32, 512)  0           conv2d_484[0][0]
__________________________________________________________________________________________________
concatenate_100 (Concatenate)   (None, 32, 32, 768)  0           up_sampling2d_100[0][0]
                                                                 conv2d_482[0][0]
__________________________________________________________________________________________________
conv2d_485 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_100[0][0]
__________________________________________________________________________________________________
conv2d_486 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_485[0][0]
__________________________________________________________________________________________________
up_sampling2d_101 (UpSampling2D (None, 64, 64, 256)  0           conv2d_486[0][0]
__________________________________________________________________________________________________
concatenate_101 (Concatenate)   (None, 64, 64, 384)  0           up_sampling2d_101[0][0]
                                                                 conv2d_480[0][0]
__________________________________________________________________________________________________
conv2d_487 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_101[0][0]
__________________________________________________________________________________________________
conv2d_488 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_487[0][0]
__________________________________________________________________________________________________
up_sampling2d_102 (UpSampling2D (None, 128, 128, 128 0           conv2d_488[0][0]
__________________________________________________________________________________________________
concatenate_102 (Concatenate)   (None, 128, 128, 192 0           up_sampling2d_102[0][0]
                                                                 conv2d_478[0][0]
__________________________________________________________________________________________________
conv2d_489 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_102[0][0]
__________________________________________________________________________________________________
conv2d_490 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_489[0][0]
__________________________________________________________________________________________________
up_sampling2d_103 (UpSampling2D (None, 256, 256, 64) 0           conv2d_490[0][0]
__________________________________________________________________________________________________
concatenate_103 (Concatenate)   (None, 256, 256, 96) 0           up_sampling2d_103[0][0]
                                                                 conv2d_476[0][0]
__________________________________________________________________________________________________
conv2d_491 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_103[0][0]
__________________________________________________________________________________________________
conv2d_492 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_491[0][0]
__________________________________________________________________________________________________
conv2d_493 (Conv2D)             (None, 256, 256, 1)  33          conv2d_492[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6931 - acc: 0.2585 - precision_25: 0.1673 - auc_25: 0.8080 - recall_25: 0.94082021-09-25 11:43:35.077691: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:43:35.077818: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 11:43:35.359610: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 11:43:35.369248: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-114329/train/plugins/profile/2021_09_25_11_43_35
2021-09-25 11:43:35.373207: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-114329/train/plugins/profile/2021_09_25_11_43_35/xeon-e-v100.trace.json.gz
2021-09-25 11:43:35.395371: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-114329/train/plugins/profile/2021_09_25_11_43_35
2021-09-25 11:43:35.402356: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-114329/train/plugins/profile/2021_09_25_11_43_35/xeon-e-v100.memory_profile.json.gz
2021-09-25 11:43:35.419839: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-114329/train/plugins/profile/2021_09_25_11_43_35Dumped tool data for xplane.pb to logs/fit/20210925-114329/train/plugins/profile/2021_09_25_11_43_35/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-114329/train/plugins/profile/2021_09_25_11_43_35/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-114329/train/plugins/profile/2021_09_25_11_43_35/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-114329/train/plugins/profile/2021_09_25_11_43_35/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-114329/train/plugins/profile/2021_09_25_11_43_35/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6886 - acc: 0.5239 - precision_25: 0.2633 - auc_25: 0.8088 - recall_25: 0.8347WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0656s vs `on_train_batch_end` time: 0.2780s). Check your callbacks.
33/33 [==============================] - 4s 108ms/step - loss: 0.3649 - acc: 0.8707 - precision_25: 0.6489 - auc_25: 0.8295 - recall_25: 0.5321 - val_loss: 0.3406 - val_acc: 0.9070 - val_precision_25: 0.7202 - val_auc_25: 0.8916 - val_recall_25: 0.6688
Epoch 2/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2071 - acc: 0.9087 - precision_25: 0.8196 - auc_25: 0.9336 - recall_25: 0.6436 - val_loss: 0.1968 - val_acc: 0.9246 - val_precision_25: 0.7796 - val_auc_25: 0.9477 - val_recall_25: 0.6999
Epoch 3/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2058 - acc: 0.9108 - precision_25: 0.8097 - auc_25: 0.9342 - recall_25: 0.6745 - val_loss: 0.2247 - val_acc: 0.9176 - val_precision_25: 0.8272 - val_auc_25: 0.9342 - val_recall_25: 0.5895
Epoch 4/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1930 - acc: 0.9141 - precision_25: 0.8313 - auc_25: 0.9401 - recall_25: 0.6628 - val_loss: 0.2153 - val_acc: 0.9149 - val_precision_25: 0.7776 - val_auc_25: 0.9422 - val_recall_25: 0.6947
Epoch 5/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1807 - acc: 0.9166 - precision_25: 0.8360 - auc_25: 0.9512 - recall_25: 0.6804 - val_loss: 0.1662 - val_acc: 0.9318 - val_precision_25: 0.8343 - val_auc_25: 0.9653 - val_recall_25: 0.6969
Epoch 6/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1801 - acc: 0.9167 - precision_25: 0.8345 - auc_25: 0.9508 - recall_25: 0.6709 - val_loss: 0.2043 - val_acc: 0.9189 - val_precision_25: 0.7561 - val_auc_25: 0.9424 - val_recall_25: 0.7178
Epoch 7/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1792 - acc: 0.9140 - precision_25: 0.8145 - auc_25: 0.9552 - recall_25: 0.6921 - val_loss: 0.1815 - val_acc: 0.9299 - val_precision_25: 0.7983 - val_auc_25: 0.9565 - val_recall_25: 0.7130
Epoch 8/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1619 - acc: 0.9238 - precision_25: 0.8354 - auc_25: 0.9614 - recall_25: 0.7361 - val_loss: 0.1990 - val_acc: 0.9244 - val_precision_25: 0.8403 - val_auc_25: 0.9562 - val_recall_25: 0.6610
Epoch 9/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1696 - acc: 0.9234 - precision_25: 0.8324 - auc_25: 0.9550 - recall_25: 0.7375 - val_loss: 0.1686 - val_acc: 0.9339 - val_precision_25: 0.7416 - val_auc_25: 0.9715 - val_recall_25: 0.8902
Epoch 10/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1419 - acc: 0.9319 - precision_25: 0.8498 - auc_25: 0.9691 - recall_25: 0.7791 - val_loss: 0.1326 - val_acc: 0.9506 - val_precision_25: 0.8896 - val_auc_25: 0.9799 - val_recall_25: 0.7788
Epoch 11/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1234 - acc: 0.9378 - precision_25: 0.8784 - auc_25: 0.9771 - recall_25: 0.7843 - val_loss: 0.1129 - val_acc: 0.9560 - val_precision_25: 0.8379 - val_auc_25: 0.9857 - val_recall_25: 0.8980
Epoch 12/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1373 - acc: 0.9369 - precision_25: 0.8720 - auc_25: 0.9699 - recall_25: 0.7855 - val_loss: 0.1146 - val_acc: 0.9602 - val_precision_25: 0.9113 - val_auc_25: 0.9828 - val_recall_25: 0.8028
Epoch 13/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1137 - acc: 0.9420 - precision_25: 0.8880 - auc_25: 0.9801 - recall_25: 0.8028 - val_loss: 0.1017 - val_acc: 0.9597 - val_precision_25: 0.8312 - val_auc_25: 0.9880 - val_recall_25: 0.9224
Epoch 14/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1102 - acc: 0.9439 - precision_25: 0.8919 - auc_25: 0.9809 - recall_25: 0.8128 - val_loss: 0.0938 - val_acc: 0.9656 - val_precision_25: 0.9149 - val_auc_25: 0.9887 - val_recall_25: 0.8649
Epoch 15/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1068 - acc: 0.9456 - precision_25: 0.9023 - auc_25: 0.9819 - recall_25: 0.8117 - val_loss: 0.1179 - val_acc: 0.9572 - val_precision_25: 0.8706 - val_auc_25: 0.9790 - val_recall_25: 0.8674
Epoch 16/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1156 - acc: 0.9457 - precision_25: 0.9035 - auc_25: 0.9759 - recall_25: 0.8069 - val_loss: 0.0968 - val_acc: 0.9612 - val_precision_25: 0.8460 - val_auc_25: 0.9895 - val_recall_25: 0.9174
Epoch 17/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0954 - acc: 0.9496 - precision_25: 0.9119 - auc_25: 0.9854 - recall_25: 0.8282 - val_loss: 0.0872 - val_acc: 0.9657 - val_precision_25: 0.8721 - val_auc_25: 0.9903 - val_recall_25: 0.9166
Epoch 18/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1072 - acc: 0.9471 - precision_25: 0.8983 - auc_25: 0.9796 - recall_25: 0.8293 - val_loss: 0.0888 - val_acc: 0.9656 - val_precision_25: 0.8776 - val_auc_25: 0.9902 - val_recall_25: 0.9172
Epoch 19/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0834 - acc: 0.9530 - precision_25: 0.9222 - auc_25: 0.9894 - recall_25: 0.8418 - val_loss: 0.0801 - val_acc: 0.9688 - val_precision_25: 0.8909 - val_auc_25: 0.9898 - val_recall_25: 0.8916
Epoch 20/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0949 - acc: 0.9508 - precision_25: 0.9161 - auc_25: 0.9855 - recall_25: 0.8317 - val_loss: 0.0750 - val_acc: 0.9713 - val_precision_25: 0.9252 - val_auc_25: 0.9920 - val_recall_25: 0.8862
Epoch 21/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0871 - acc: 0.9511 - precision_25: 0.9124 - auc_25: 0.9881 - recall_25: 0.8436 - val_loss: 0.0719 - val_acc: 0.9724 - val_precision_25: 0.8886 - val_auc_25: 0.9937 - val_recall_25: 0.9271
Epoch 22/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0987 - acc: 0.9506 - precision_25: 0.9173 - auc_25: 0.9842 - recall_25: 0.8272 - val_loss: 0.0787 - val_acc: 0.9701 - val_precision_25: 0.9237 - val_auc_25: 0.9907 - val_recall_25: 0.8720
Epoch 23/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0971 - acc: 0.9512 - precision_25: 0.9145 - auc_25: 0.9840 - recall_25: 0.8379 - val_loss: 0.0826 - val_acc: 0.9679 - val_precision_25: 0.8949 - val_auc_25: 0.9905 - val_recall_25: 0.9115
Epoch 24/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0918 - acc: 0.9531 - precision_25: 0.9239 - auc_25: 0.9859 - recall_25: 0.8430 - val_loss: 0.0784 - val_acc: 0.9691 - val_precision_25: 0.8709 - val_auc_25: 0.9923 - val_recall_25: 0.9256
Epoch 25/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0816 - acc: 0.9531 - precision_25: 0.9259 - auc_25: 0.9901 - recall_25: 0.8427 - val_loss: 0.0807 - val_acc: 0.9687 - val_precision_25: 0.9170 - val_auc_25: 0.9906 - val_recall_25: 0.8810
Epoch 26/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1012 - acc: 0.9512 - precision_25: 0.9188 - auc_25: 0.9810 - recall_25: 0.8361 - val_loss: 0.0810 - val_acc: 0.9686 - val_precision_25: 0.8941 - val_auc_25: 0.9905 - val_recall_25: 0.9056
Epoch 27/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0956 - acc: 0.9510 - precision_25: 0.9157 - auc_25: 0.9843 - recall_25: 0.8349 - val_loss: 0.0675 - val_acc: 0.9736 - val_precision_25: 0.9240 - val_auc_25: 0.9936 - val_recall_25: 0.9017
Epoch 28/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0808 - acc: 0.9553 - precision_25: 0.9309 - auc_25: 0.9891 - recall_25: 0.8492 - val_loss: 0.0734 - val_acc: 0.9714 - val_precision_25: 0.9039 - val_auc_25: 0.9922 - val_recall_25: 0.9005
Epoch 29/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0963 - acc: 0.9529 - precision_25: 0.9199 - auc_25: 0.9832 - recall_25: 0.8423 - val_loss: 0.0848 - val_acc: 0.9674 - val_precision_25: 0.9171 - val_auc_25: 0.9906 - val_recall_25: 0.8871
Epoch 30/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0818 - acc: 0.9536 - precision_25: 0.9306 - auc_25: 0.9898 - recall_25: 0.8375 - val_loss: 0.0735 - val_acc: 0.9721 - val_precision_25: 0.9037 - val_auc_25: 0.9929 - val_recall_25: 0.9187
Epoch 31/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0896 - acc: 0.9544 - precision_25: 0.9289 - auc_25: 0.9853 - recall_25: 0.8456 - val_loss: 0.0734 - val_acc: 0.9713 - val_precision_25: 0.9201 - val_auc_25: 0.9928 - val_recall_25: 0.9030
Epoch 32/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0866 - acc: 0.9530 - precision_25: 0.9201 - auc_25: 0.9872 - recall_25: 0.8455 - val_loss: 0.0923 - val_acc: 0.9663 - val_precision_25: 0.9290 - val_auc_25: 0.9862 - val_recall_25: 0.8428
Epoch 33/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0890 - acc: 0.9532 - precision_25: 0.9330 - auc_25: 0.9864 - recall_25: 0.8328 - val_loss: 0.0754 - val_acc: 0.9706 - val_precision_25: 0.9029 - val_auc_25: 0.9924 - val_recall_25: 0.9112
Epoch 34/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0811 - acc: 0.9544 - precision_25: 0.9246 - auc_25: 0.9899 - recall_25: 0.8493 - val_loss: 0.0721 - val_acc: 0.9714 - val_precision_25: 0.9120 - val_auc_25: 0.9930 - val_recall_25: 0.9059
Epoch 35/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0797 - acc: 0.9562 - precision_25: 0.9372 - auc_25: 0.9892 - recall_25: 0.8485 - val_loss: 0.0746 - val_acc: 0.9708 - val_precision_25: 0.9011 - val_auc_25: 0.9936 - val_recall_25: 0.9372
Epoch 36/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0840 - acc: 0.9558 - precision_25: 0.9346 - auc_25: 0.9881 - recall_25: 0.8499 - val_loss: 0.0672 - val_acc: 0.9740 - val_precision_25: 0.9270 - val_auc_25: 0.9930 - val_recall_25: 0.9071
Epoch 37/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0706 - acc: 0.9587 - precision_25: 0.9434 - auc_25: 0.9916 - recall_25: 0.8572 - val_loss: 0.0742 - val_acc: 0.9712 - val_precision_25: 0.8851 - val_auc_25: 0.9924 - val_recall_25: 0.9201
Epoch 38/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0801 - acc: 0.9564 - precision_25: 0.9319 - auc_25: 0.9894 - recall_25: 0.8536 - val_loss: 0.0738 - val_acc: 0.9701 - val_precision_25: 0.9143 - val_auc_25: 0.9927 - val_recall_25: 0.9009
Epoch 39/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0777 - acc: 0.9567 - precision_25: 0.9363 - auc_25: 0.9897 - recall_25: 0.8536 - val_loss: 0.0747 - val_acc: 0.9712 - val_precision_25: 0.9306 - val_auc_25: 0.9921 - val_recall_25: 0.8868
Epoch 40/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0804 - acc: 0.9560 - precision_25: 0.9344 - auc_25: 0.9890 - recall_25: 0.8538 - val_loss: 0.0692 - val_acc: 0.9732 - val_precision_25: 0.9057 - val_auc_25: 0.9935 - val_recall_25: 0.9238
Epoch 41/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0662 - acc: 0.9591 - precision_25: 0.9427 - auc_25: 0.9935 - recall_25: 0.8596 - val_loss: 0.0688 - val_acc: 0.9729 - val_precision_25: 0.9272 - val_auc_25: 0.9932 - val_recall_25: 0.8984
Epoch 42/100
33/33 [==============================] - 3s 88ms/step - loss: 0.0737 - acc: 0.9582 - precision_25: 0.9427 - auc_25: 0.9904 - recall_25: 0.8570 - val_loss: 0.0647 - val_acc: 0.9747 - val_precision_25: 0.9068 - val_auc_25: 0.9945 - val_recall_25: 0.9338
Epoch 43/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0689 - acc: 0.9588 - precision_25: 0.9433 - auc_25: 0.9923 - recall_25: 0.8601 - val_loss: 0.0725 - val_acc: 0.9719 - val_precision_25: 0.9138 - val_auc_25: 0.9933 - val_recall_25: 0.9119
Epoch 44/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0783 - acc: 0.9576 - precision_25: 0.9407 - auc_25: 0.9891 - recall_25: 0.8529 - val_loss: 0.0670 - val_acc: 0.9729 - val_precision_25: 0.9093 - val_auc_25: 0.9938 - val_recall_25: 0.9109
Epoch 45/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0745 - acc: 0.9561 - precision_25: 0.9341 - auc_25: 0.9909 - recall_25: 0.8521 - val_loss: 0.0733 - val_acc: 0.9717 - val_precision_25: 0.9155 - val_auc_25: 0.9924 - val_recall_25: 0.8987
Epoch 46/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0879 - acc: 0.9551 - precision_25: 0.9359 - auc_25: 0.9852 - recall_25: 0.8420 - val_loss: 0.0743 - val_acc: 0.9705 - val_precision_25: 0.8789 - val_auc_25: 0.9934 - val_recall_25: 0.9292
Epoch 47/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0762 - acc: 0.9573 - precision_25: 0.9375 - auc_25: 0.9905 - recall_25: 0.8597 - val_loss: 0.0735 - val_acc: 0.9717 - val_precision_25: 0.9176 - val_auc_25: 0.9920 - val_recall_25: 0.9083
Epoch 48/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0926 - acc: 0.9547 - precision_25: 0.9319 - auc_25: 0.9831 - recall_25: 0.8453 - val_loss: 0.0642 - val_acc: 0.9747 - val_precision_25: 0.9139 - val_auc_25: 0.9943 - val_recall_25: 0.9247
Epoch 49/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0647 - acc: 0.9596 - precision_25: 0.9470 - auc_25: 0.9935 - recall_25: 0.8597 - val_loss: 0.0727 - val_acc: 0.9711 - val_precision_25: 0.8870 - val_auc_25: 0.9933 - val_recall_25: 0.9357
Epoch 50/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0650 - acc: 0.9592 - precision_25: 0.9426 - auc_25: 0.9935 - recall_25: 0.8662 - val_loss: 0.0708 - val_acc: 0.9724 - val_precision_25: 0.9162 - val_auc_25: 0.9929 - val_recall_25: 0.9063
Epoch 51/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0730 - acc: 0.9584 - precision_25: 0.9469 - auc_25: 0.9905 - recall_25: 0.8519 - val_loss: 0.0712 - val_acc: 0.9714 - val_precision_25: 0.9142 - val_auc_25: 0.9933 - val_recall_25: 0.9072
Epoch 52/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0641 - acc: 0.9597 - precision_25: 0.9464 - auc_25: 0.9935 - recall_25: 0.8655 - val_loss: 0.0736 - val_acc: 0.9701 - val_precision_25: 0.8927 - val_auc_25: 0.9944 - val_recall_25: 0.9352
Epoch 53/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0725 - acc: 0.9589 - precision_25: 0.9438 - auc_25: 0.9906 - recall_25: 0.8594 - val_loss: 0.0711 - val_acc: 0.9717 - val_precision_25: 0.8641 - val_auc_25: 0.9940 - val_recall_25: 0.9447
Epoch 54/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0781 - acc: 0.9578 - precision_25: 0.9368 - auc_25: 0.9887 - recall_25: 0.8590 - val_loss: 0.0708 - val_acc: 0.9723 - val_precision_25: 0.9195 - val_auc_25: 0.9933 - val_recall_25: 0.9157
Epoch 55/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0700 - acc: 0.9589 - precision_25: 0.9435 - auc_25: 0.9916 - recall_25: 0.8634 - val_loss: 0.0697 - val_acc: 0.9723 - val_precision_25: 0.9075 - val_auc_25: 0.9928 - val_recall_25: 0.9037
Epoch 56/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0722 - acc: 0.9581 - precision_25: 0.9463 - auc_25: 0.9912 - recall_25: 0.8542 - val_loss: 0.0641 - val_acc: 0.9746 - val_precision_25: 0.9227 - val_auc_25: 0.9949 - val_recall_25: 0.9190
Epoch 57/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0715 - acc: 0.9592 - precision_25: 0.9450 - auc_25: 0.9903 - recall_25: 0.8599 - val_loss: 0.0666 - val_acc: 0.9736 - val_precision_25: 0.9065 - val_auc_25: 0.9939 - val_recall_25: 0.9276
Epoch 58/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0638 - acc: 0.9606 - precision_25: 0.9500 - auc_25: 0.9933 - recall_25: 0.8676 - val_loss: 0.0679 - val_acc: 0.9730 - val_precision_25: 0.9268 - val_auc_25: 0.9935 - val_recall_25: 0.9111
Epoch 59/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0680 - acc: 0.9583 - precision_25: 0.9441 - auc_25: 0.9928 - recall_25: 0.8574 - val_loss: 0.0648 - val_acc: 0.9743 - val_precision_25: 0.9145 - val_auc_25: 0.9939 - val_recall_25: 0.9069
Epoch 60/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0651 - acc: 0.9602 - precision_25: 0.9495 - auc_25: 0.9928 - recall_25: 0.8668 - val_loss: 0.0651 - val_acc: 0.9741 - val_precision_25: 0.9178 - val_auc_25: 0.9938 - val_recall_25: 0.9202
Epoch 61/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0855 - acc: 0.9577 - precision_25: 0.9414 - auc_25: 0.9845 - recall_25: 0.8561 - val_loss: 0.0658 - val_acc: 0.9741 - val_precision_25: 0.9185 - val_auc_25: 0.9938 - val_recall_25: 0.9237
Epoch 62/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0677 - acc: 0.9601 - precision_25: 0.9482 - auc_25: 0.9921 - recall_25: 0.8668 - val_loss: 0.0701 - val_acc: 0.9730 - val_precision_25: 0.9098 - val_auc_25: 0.9937 - val_recall_25: 0.9239
Epoch 63/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0591 - acc: 0.9614 - precision_25: 0.9508 - auc_25: 0.9944 - recall_25: 0.8726 - val_loss: 0.0647 - val_acc: 0.9750 - val_precision_25: 0.9259 - val_auc_25: 0.9940 - val_recall_25: 0.9187
Epoch 64/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0805 - acc: 0.9561 - precision_25: 0.9406 - auc_25: 0.9883 - recall_25: 0.8420 - val_loss: 0.0680 - val_acc: 0.9739 - val_precision_25: 0.9021 - val_auc_25: 0.9943 - val_recall_25: 0.9295
Epoch 65/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0816 - acc: 0.9580 - precision_25: 0.9399 - auc_25: 0.9865 - recall_25: 0.8572 - val_loss: 0.0653 - val_acc: 0.9742 - val_precision_25: 0.9220 - val_auc_25: 0.9938 - val_recall_25: 0.9135
Epoch 66/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0737 - acc: 0.9570 - precision_25: 0.9376 - auc_25: 0.9914 - recall_25: 0.8579 - val_loss: 0.0853 - val_acc: 0.9673 - val_precision_25: 0.9349 - val_auc_25: 0.9888 - val_recall_25: 0.8435
Epoch 67/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0754 - acc: 0.9574 - precision_25: 0.9381 - auc_25: 0.9905 - recall_25: 0.8537 - val_loss: 0.0709 - val_acc: 0.9716 - val_precision_25: 0.9002 - val_auc_25: 0.9927 - val_recall_25: 0.9125
Epoch 68/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0716 - acc: 0.9593 - precision_25: 0.9451 - auc_25: 0.9903 - recall_25: 0.8633 - val_loss: 0.0647 - val_acc: 0.9748 - val_precision_25: 0.9164 - val_auc_25: 0.9946 - val_recall_25: 0.9329
Epoch 69/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0609 - acc: 0.9611 - precision_25: 0.9526 - auc_25: 0.9940 - recall_25: 0.8703 - val_loss: 0.0702 - val_acc: 0.9726 - val_precision_25: 0.9494 - val_auc_25: 0.9924 - val_recall_25: 0.8777
Epoch 70/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0692 - acc: 0.9597 - precision_25: 0.9467 - auc_25: 0.9915 - recall_25: 0.8614 - val_loss: 0.0630 - val_acc: 0.9748 - val_precision_25: 0.9095 - val_auc_25: 0.9943 - val_recall_25: 0.9240
Epoch 71/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0754 - acc: 0.9591 - precision_25: 0.9451 - auc_25: 0.9897 - recall_25: 0.8647 - val_loss: 0.0664 - val_acc: 0.9738 - val_precision_25: 0.9063 - val_auc_25: 0.9936 - val_recall_25: 0.9245
Epoch 72/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0641 - acc: 0.9608 - precision_25: 0.9513 - auc_25: 0.9928 - recall_25: 0.8627 - val_loss: 0.0633 - val_acc: 0.9750 - val_precision_25: 0.9254 - val_auc_25: 0.9943 - val_recall_25: 0.9207
Epoch 73/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0708 - acc: 0.9600 - precision_25: 0.9498 - auc_25: 0.9906 - recall_25: 0.8646 - val_loss: 0.0658 - val_acc: 0.9737 - val_precision_25: 0.9126 - val_auc_25: 0.9943 - val_recall_25: 0.9211
Epoch 74/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0653 - acc: 0.9593 - precision_25: 0.9464 - auc_25: 0.9934 - recall_25: 0.8646 - val_loss: 0.0706 - val_acc: 0.9729 - val_precision_25: 0.9443 - val_auc_25: 0.9922 - val_recall_25: 0.8851
Epoch 75/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0907 - acc: 0.9558 - precision_25: 0.9420 - auc_25: 0.9826 - recall_25: 0.8411 - val_loss: 0.0621 - val_acc: 0.9757 - val_precision_25: 0.9114 - val_auc_25: 0.9943 - val_recall_25: 0.9301
Epoch 76/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0608 - acc: 0.9611 - precision_25: 0.9526 - auc_25: 0.9939 - recall_25: 0.8697 - val_loss: 0.0589 - val_acc: 0.9770 - val_precision_25: 0.9282 - val_auc_25: 0.9943 - val_recall_25: 0.9148
Epoch 77/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0694 - acc: 0.9598 - precision_25: 0.9476 - auc_25: 0.9913 - recall_25: 0.8620 - val_loss: 0.0636 - val_acc: 0.9750 - val_precision_25: 0.8994 - val_auc_25: 0.9952 - val_recall_25: 0.9395
Epoch 78/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0753 - acc: 0.9583 - precision_25: 0.9408 - auc_25: 0.9894 - recall_25: 0.8572 - val_loss: 0.0688 - val_acc: 0.9719 - val_precision_25: 0.9007 - val_auc_25: 0.9946 - val_recall_25: 0.9334
Epoch 79/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0905 - acc: 0.9566 - precision_25: 0.9361 - auc_25: 0.9839 - recall_25: 0.8569 - val_loss: 0.0632 - val_acc: 0.9755 - val_precision_25: 0.9250 - val_auc_25: 0.9939 - val_recall_25: 0.9222
Epoch 80/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0724 - acc: 0.9603 - precision_25: 0.9516 - auc_25: 0.9896 - recall_25: 0.8602 - val_loss: 0.0624 - val_acc: 0.9749 - val_precision_25: 0.9067 - val_auc_25: 0.9950 - val_recall_25: 0.9346
Epoch 81/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0638 - acc: 0.9604 - precision_25: 0.9501 - auc_25: 0.9928 - recall_25: 0.8685 - val_loss: 0.0667 - val_acc: 0.9738 - val_precision_25: 0.9296 - val_auc_25: 0.9934 - val_recall_25: 0.9017
Epoch 82/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0666 - acc: 0.9594 - precision_25: 0.9511 - auc_25: 0.9924 - recall_25: 0.8570 - val_loss: 0.0672 - val_acc: 0.9734 - val_precision_25: 0.9071 - val_auc_25: 0.9947 - val_recall_25: 0.9304
Epoch 83/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0739 - acc: 0.9589 - precision_25: 0.9457 - auc_25: 0.9895 - recall_25: 0.8604 - val_loss: 0.0650 - val_acc: 0.9747 - val_precision_25: 0.9232 - val_auc_25: 0.9941 - val_recall_25: 0.9240
Epoch 84/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0598 - acc: 0.9614 - precision_25: 0.9534 - auc_25: 0.9942 - recall_25: 0.8700 - val_loss: 0.0601 - val_acc: 0.9765 - val_precision_25: 0.9051 - val_auc_25: 0.9934 - val_recall_25: 0.9241
Epoch 85/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0601 - acc: 0.9619 - precision_25: 0.9533 - auc_25: 0.9939 - recall_25: 0.8741 - val_loss: 0.0662 - val_acc: 0.9738 - val_precision_25: 0.9383 - val_auc_25: 0.9936 - val_recall_25: 0.8973
Epoch 86/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0653 - acc: 0.9610 - precision_25: 0.9531 - auc_25: 0.9916 - recall_25: 0.8640 - val_loss: 0.0599 - val_acc: 0.9763 - val_precision_25: 0.9238 - val_auc_25: 0.9947 - val_recall_25: 0.9248
Epoch 87/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0726 - acc: 0.9609 - precision_25: 0.9539 - auc_25: 0.9888 - recall_25: 0.8670 - val_loss: 0.0646 - val_acc: 0.9746 - val_precision_25: 0.9295 - val_auc_25: 0.9934 - val_recall_25: 0.9121
Epoch 88/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0694 - acc: 0.9610 - precision_25: 0.9527 - auc_25: 0.9907 - recall_25: 0.8660 - val_loss: 0.0621 - val_acc: 0.9758 - val_precision_25: 0.9320 - val_auc_25: 0.9942 - val_recall_25: 0.9228
Epoch 89/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0570 - acc: 0.9627 - precision_25: 0.9570 - auc_25: 0.9943 - recall_25: 0.8749 - val_loss: 0.0631 - val_acc: 0.9753 - val_precision_25: 0.9219 - val_auc_25: 0.9941 - val_recall_25: 0.9211
Epoch 90/100
32/33 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9590 - precision_25: 0.9498 - auc_25: 0.9904 - recall_25: 0.8633Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 86ms/step - loss: 0.0740 - acc: 0.9591 - precision_25: 0.9499 - auc_25: 0.9904 - recall_25: 0.8630 - val_loss: 0.0646 - val_acc: 0.9748 - val_precision_25: 0.9194 - val_auc_25: 0.9936 - val_recall_25: 0.9149
Epoch 00090: early stopping
6/6 [==============================] - 0s 27ms/step - loss: 0.0635 - acc: 0.9745 - precision_25: 0.9159 - auc_25: 0.9952 - recall_25: 0.9280
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb615777e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.849435691758615
test_sensitivity 0.9346267609164285
test_specifitivity 0.9818814180097734
test_accuracy 0.9742106119791667
test_precision 0.9090598790715818
test_jaccard_score 0.849435691758615
test_dicecoef 0.921666048696082
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-114810.h5
[0.97101644 0.8256415  0.9000822  0.92402784 0.98012218] [0.97421061 0.84943569 0.90905988 0.93462676 0.98188142]

-------------------------
Rep: 2
-------------------------

2021-09-25 11:48:11.352145: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:48:11.352203: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (258, 256, 256, 1)
Validation set shape (63, 256, 256, 1)
Test set shape (45, 256, 256, 1)
Data loaded
Applied histogram equalization on all input images
Normalized per channel
Applied normalization
Data augumentation on
Prep done
Training samples: 258, channel mean: 9.372885757797167e-18,
Validation samples: 63, channel mean: -0.0027676740471074255
Model built.
Model: "functional_53"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_27 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_494 (Conv2D)             (None, 256, 256, 32) 320         input_27[0][0]
__________________________________________________________________________________________________
conv2d_495 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_494[0][0]
__________________________________________________________________________________________________
max_pooling2d_104 (MaxPooling2D (None, 128, 128, 32) 0           conv2d_495[0][0]
__________________________________________________________________________________________________
conv2d_496 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_104[0][0]
__________________________________________________________________________________________________
conv2d_497 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_496[0][0]
__________________________________________________________________________________________________
max_pooling2d_105 (MaxPooling2D (None, 64, 64, 64)   0           conv2d_497[0][0]
__________________________________________________________________________________________________
conv2d_498 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_105[0][0]
__________________________________________________________________________________________________
conv2d_499 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_498[0][0]
__________________________________________________________________________________________________
max_pooling2d_106 (MaxPooling2D (None, 32, 32, 128)  0           conv2d_499[0][0]
__________________________________________________________________________________________________
conv2d_500 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_106[0][0]
__________________________________________________________________________________________________
conv2d_501 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_500[0][0]
__________________________________________________________________________________________________
max_pooling2d_107 (MaxPooling2D (None, 16, 16, 256)  0           conv2d_501[0][0]
__________________________________________________________________________________________________
conv2d_502 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_107[0][0]
__________________________________________________________________________________________________
conv2d_503 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_502[0][0]
__________________________________________________________________________________________________
up_sampling2d_104 (UpSampling2D (None, 32, 32, 512)  0           conv2d_503[0][0]
__________________________________________________________________________________________________
concatenate_104 (Concatenate)   (None, 32, 32, 768)  0           up_sampling2d_104[0][0]
                                                                 conv2d_501[0][0]
__________________________________________________________________________________________________
conv2d_504 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_104[0][0]
__________________________________________________________________________________________________
conv2d_505 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_504[0][0]
__________________________________________________________________________________________________
up_sampling2d_105 (UpSampling2D (None, 64, 64, 256)  0           conv2d_505[0][0]
__________________________________________________________________________________________________
concatenate_105 (Concatenate)   (None, 64, 64, 384)  0           up_sampling2d_105[0][0]
                                                                 conv2d_499[0][0]
__________________________________________________________________________________________________
conv2d_506 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_105[0][0]
__________________________________________________________________________________________________
conv2d_507 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_506[0][0]
__________________________________________________________________________________________________
up_sampling2d_106 (UpSampling2D (None, 128, 128, 128 0           conv2d_507[0][0]
__________________________________________________________________________________________________
concatenate_106 (Concatenate)   (None, 128, 128, 192 0           up_sampling2d_106[0][0]
                                                                 conv2d_497[0][0]
__________________________________________________________________________________________________
conv2d_508 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_106[0][0]
__________________________________________________________________________________________________
conv2d_509 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_508[0][0]
__________________________________________________________________________________________________
up_sampling2d_107 (UpSampling2D (None, 256, 256, 64) 0           conv2d_509[0][0]
__________________________________________________________________________________________________
concatenate_107 (Concatenate)   (None, 256, 256, 96) 0           up_sampling2d_107[0][0]
                                                                 conv2d_495[0][0]
__________________________________________________________________________________________________
conv2d_510 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_107[0][0]
__________________________________________________________________________________________________
conv2d_511 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_510[0][0]
__________________________________________________________________________________________________
conv2d_512 (Conv2D)             (None, 256, 256, 1)  33          conv2d_511[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 1/33 [..............................] - ETA: 0s - loss: 0.6963 - acc: 0.1283 - precision_26: 0.1504 - auc_26: 0.4927 - recall_26: 0.98602021-09-25 11:48:16.418213: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 11:48:16.418322: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 11:48:16.699442: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 11:48:16.709273: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-114811/train/plugins/profile/2021_09_25_11_48_16
2021-09-25 11:48:16.714121: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-114811/train/plugins/profile/2021_09_25_11_48_16/xeon-e-v100.trace.json.gz
2021-09-25 11:48:16.733914: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-114811/train/plugins/profile/2021_09_25_11_48_16
2021-09-25 11:48:16.741198: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-114811/train/plugins/profile/2021_09_25_11_48_16/xeon-e-v100.memory_profile.json.gz
2021-09-25 11:48:16.759753: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-114811/train/plugins/profile/2021_09_25_11_48_16Dumped tool data for xplane.pb to logs/fit/20210925-114811/train/plugins/profile/2021_09_25_11_48_16/xeon-e-v100.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-114811/train/plugins/profile/2021_09_25_11_48_16/xeon-e-v100.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-114811/train/plugins/profile/2021_09_25_11_48_16/xeon-e-v100.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-114811/train/plugins/profile/2021_09_25_11_48_16/xeon-e-v100.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-114811/train/plugins/profile/2021_09_25_11_48_16/xeon-e-v100.kernel_stats.pb

 2/33 [>.............................] - ETA: 5s - loss: 0.6928 - acc: 0.4625 - precision_26: 0.2324 - auc_26: 0.6456 - recall_26: 0.7994WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0651s vs `on_train_batch_end` time: 0.2779s). Check your callbacks.
33/33 [==============================] - 4s 108ms/step - loss: 0.3801 - acc: 0.8631 - precision_26: 0.6217 - auc_26: 0.8142 - recall_26: 0.4870 - val_loss: 0.3218 - val_acc: 0.9102 - val_precision_26: 0.7482 - val_auc_26: 0.9073 - val_recall_26: 0.6476
Epoch 2/100
33/33 [==============================] - 3s 86ms/step - loss: 0.2460 - acc: 0.9019 - precision_26: 0.7970 - auc_26: 0.9188 - recall_26: 0.6070 - val_loss: 0.1936 - val_acc: 0.9203 - val_precision_26: 0.7673 - val_auc_26: 0.9485 - val_recall_26: 0.6796
Epoch 3/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1914 - acc: 0.9145 - precision_26: 0.8472 - auc_26: 0.9446 - recall_26: 0.6447 - val_loss: 0.2237 - val_acc: 0.9221 - val_precision_26: 0.8465 - val_auc_26: 0.9409 - val_recall_26: 0.6058
Epoch 4/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1923 - acc: 0.9145 - precision_26: 0.8267 - auc_26: 0.9414 - recall_26: 0.6732 - val_loss: 0.2192 - val_acc: 0.9112 - val_precision_26: 0.7640 - val_auc_26: 0.9375 - val_recall_26: 0.6860
Epoch 5/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1853 - acc: 0.9157 - precision_26: 0.8282 - auc_26: 0.9459 - recall_26: 0.6866 - val_loss: 0.1762 - val_acc: 0.9287 - val_precision_26: 0.8395 - val_auc_26: 0.9626 - val_recall_26: 0.6662
Epoch 6/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1710 - acc: 0.9186 - precision_26: 0.8398 - auc_26: 0.9556 - recall_26: 0.6769 - val_loss: 0.1877 - val_acc: 0.9261 - val_precision_26: 0.7857 - val_auc_26: 0.9486 - val_recall_26: 0.7313
Epoch 7/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1699 - acc: 0.9175 - precision_26: 0.8303 - auc_26: 0.9593 - recall_26: 0.6937 - val_loss: 0.1662 - val_acc: 0.9351 - val_precision_26: 0.8343 - val_auc_26: 0.9639 - val_recall_26: 0.7086
Epoch 8/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1681 - acc: 0.9219 - precision_26: 0.8352 - auc_26: 0.9569 - recall_26: 0.7250 - val_loss: 0.2278 - val_acc: 0.9173 - val_precision_26: 0.8064 - val_auc_26: 0.9450 - val_recall_26: 0.6472
Epoch 9/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1667 - acc: 0.9210 - precision_26: 0.8399 - auc_26: 0.9578 - recall_26: 0.7133 - val_loss: 0.1738 - val_acc: 0.9409 - val_precision_26: 0.8039 - val_auc_26: 0.9707 - val_recall_26: 0.8257
Epoch 10/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1524 - acc: 0.9290 - precision_26: 0.8540 - auc_26: 0.9650 - recall_26: 0.7524 - val_loss: 0.1504 - val_acc: 0.9400 - val_precision_26: 0.8604 - val_auc_26: 0.9725 - val_recall_26: 0.7329
Epoch 11/100
33/33 [==============================] - 3s 85ms/step - loss: 0.1386 - acc: 0.9327 - precision_26: 0.8589 - auc_26: 0.9707 - recall_26: 0.7785 - val_loss: 0.1328 - val_acc: 0.9472 - val_precision_26: 0.8070 - val_auc_26: 0.9804 - val_recall_26: 0.8797
Epoch 12/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1411 - acc: 0.9352 - precision_26: 0.8657 - auc_26: 0.9677 - recall_26: 0.7807 - val_loss: 0.1148 - val_acc: 0.9615 - val_precision_26: 0.8869 - val_auc_26: 0.9807 - val_recall_26: 0.8404
Epoch 13/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1348 - acc: 0.9375 - precision_26: 0.8725 - auc_26: 0.9683 - recall_26: 0.7894 - val_loss: 0.1111 - val_acc: 0.9571 - val_precision_26: 0.8210 - val_auc_26: 0.9863 - val_recall_26: 0.9180
Epoch 14/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1276 - acc: 0.9376 - precision_26: 0.8695 - auc_26: 0.9738 - recall_26: 0.7943 - val_loss: 0.1178 - val_acc: 0.9570 - val_precision_26: 0.8668 - val_auc_26: 0.9833 - val_recall_26: 0.8633
Epoch 15/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1231 - acc: 0.9411 - precision_26: 0.9009 - auc_26: 0.9757 - recall_26: 0.7807 - val_loss: 0.1363 - val_acc: 0.9469 - val_precision_26: 0.8080 - val_auc_26: 0.9778 - val_recall_26: 0.8857
Epoch 16/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1195 - acc: 0.9417 - precision_26: 0.8851 - auc_26: 0.9764 - recall_26: 0.8020 - val_loss: 0.1067 - val_acc: 0.9589 - val_precision_26: 0.8446 - val_auc_26: 0.9870 - val_recall_26: 0.9011
Epoch 17/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1039 - acc: 0.9467 - precision_26: 0.9033 - auc_26: 0.9823 - recall_26: 0.8134 - val_loss: 0.1002 - val_acc: 0.9626 - val_precision_26: 0.8921 - val_auc_26: 0.9865 - val_recall_26: 0.8674
Epoch 18/100
33/33 [==============================] - 3s 86ms/step - loss: 0.1015 - acc: 0.9465 - precision_26: 0.8991 - auc_26: 0.9827 - recall_26: 0.8213 - val_loss: 0.0982 - val_acc: 0.9618 - val_precision_26: 0.8942 - val_auc_26: 0.9883 - val_recall_26: 0.8688
Epoch 19/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0932 - acc: 0.9495 - precision_26: 0.9160 - auc_26: 0.9864 - recall_26: 0.8228 - val_loss: 0.0849 - val_acc: 0.9668 - val_precision_26: 0.9037 - val_auc_26: 0.9893 - val_recall_26: 0.8596
Epoch 20/100
33/33 [==============================] - 3s 88ms/step - loss: 0.0849 - acc: 0.9514 - precision_26: 0.9203 - auc_26: 0.9896 - recall_26: 0.8322 - val_loss: 0.0801 - val_acc: 0.9692 - val_precision_26: 0.9196 - val_auc_26: 0.9917 - val_recall_26: 0.8781
Epoch 21/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1036 - acc: 0.9484 - precision_26: 0.9142 - auc_26: 0.9812 - recall_26: 0.8178 - val_loss: 0.0751 - val_acc: 0.9713 - val_precision_26: 0.9130 - val_auc_26: 0.9925 - val_recall_26: 0.8877
Epoch 22/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0843 - acc: 0.9530 - precision_26: 0.9293 - auc_26: 0.9891 - recall_26: 0.8318 - val_loss: 0.0838 - val_acc: 0.9680 - val_precision_26: 0.9124 - val_auc_26: 0.9893 - val_recall_26: 0.8693
Epoch 23/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0983 - acc: 0.9511 - precision_26: 0.9172 - auc_26: 0.9842 - recall_26: 0.8329 - val_loss: 0.0847 - val_acc: 0.9673 - val_precision_26: 0.8960 - val_auc_26: 0.9903 - val_recall_26: 0.9054
Epoch 24/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0940 - acc: 0.9510 - precision_26: 0.9171 - auc_26: 0.9845 - recall_26: 0.8345 - val_loss: 0.0782 - val_acc: 0.9693 - val_precision_26: 0.8826 - val_auc_26: 0.9922 - val_recall_26: 0.9106
Epoch 25/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0817 - acc: 0.9538 - precision_26: 0.9323 - auc_26: 0.9899 - recall_26: 0.8388 - val_loss: 0.0794 - val_acc: 0.9686 - val_precision_26: 0.9024 - val_auc_26: 0.9913 - val_recall_26: 0.8975
Epoch 26/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0814 - acc: 0.9547 - precision_26: 0.9284 - auc_26: 0.9897 - recall_26: 0.8506 - val_loss: 0.0829 - val_acc: 0.9675 - val_precision_26: 0.8892 - val_auc_26: 0.9911 - val_recall_26: 0.9043
Epoch 27/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0979 - acc: 0.9506 - precision_26: 0.9208 - auc_26: 0.9840 - recall_26: 0.8251 - val_loss: 0.0658 - val_acc: 0.9740 - val_precision_26: 0.9212 - val_auc_26: 0.9940 - val_recall_26: 0.9071
Epoch 28/100
33/33 [==============================] - 3s 87ms/step - loss: 0.1028 - acc: 0.9512 - precision_26: 0.9206 - auc_26: 0.9816 - recall_26: 0.8311 - val_loss: 0.0811 - val_acc: 0.9702 - val_precision_26: 0.9142 - val_auc_26: 0.9895 - val_recall_26: 0.8794
Epoch 29/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0748 - acc: 0.9560 - precision_26: 0.9326 - auc_26: 0.9912 - recall_26: 0.8510 - val_loss: 0.0850 - val_acc: 0.9668 - val_precision_26: 0.9147 - val_auc_26: 0.9902 - val_recall_26: 0.8864
Epoch 30/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0789 - acc: 0.9546 - precision_26: 0.9370 - auc_26: 0.9903 - recall_26: 0.8365 - val_loss: 0.0772 - val_acc: 0.9697 - val_precision_26: 0.8886 - val_auc_26: 0.9928 - val_recall_26: 0.9211
Epoch 31/100
33/33 [==============================] - 3s 85ms/step - loss: 0.0742 - acc: 0.9560 - precision_26: 0.9323 - auc_26: 0.9915 - recall_26: 0.8554 - val_loss: 0.0755 - val_acc: 0.9698 - val_precision_26: 0.9181 - val_auc_26: 0.9927 - val_recall_26: 0.8953
Epoch 32/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0835 - acc: 0.9544 - precision_26: 0.9297 - auc_26: 0.9878 - recall_26: 0.8448 - val_loss: 0.0778 - val_acc: 0.9695 - val_precision_26: 0.9351 - val_auc_26: 0.9913 - val_recall_26: 0.8589
Epoch 33/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0801 - acc: 0.9555 - precision_26: 0.9386 - auc_26: 0.9898 - recall_26: 0.8445 - val_loss: 0.0744 - val_acc: 0.9708 - val_precision_26: 0.9072 - val_auc_26: 0.9926 - val_recall_26: 0.9071
Epoch 34/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0987 - acc: 0.9499 - precision_26: 0.9141 - auc_26: 0.9841 - recall_26: 0.8282 - val_loss: 0.0866 - val_acc: 0.9682 - val_precision_26: 0.8825 - val_auc_26: 0.9923 - val_recall_26: 0.9207
Epoch 35/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0895 - acc: 0.9532 - precision_26: 0.9271 - auc_26: 0.9861 - recall_26: 0.8370 - val_loss: 0.0813 - val_acc: 0.9677 - val_precision_26: 0.8866 - val_auc_26: 0.9933 - val_recall_26: 0.9367
Epoch 36/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0802 - acc: 0.9559 - precision_26: 0.9352 - auc_26: 0.9893 - recall_26: 0.8509 - val_loss: 0.0713 - val_acc: 0.9722 - val_precision_26: 0.9296 - val_auc_26: 0.9928 - val_recall_26: 0.8918
Epoch 37/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0733 - acc: 0.9577 - precision_26: 0.9414 - auc_26: 0.9911 - recall_26: 0.8516 - val_loss: 0.0808 - val_acc: 0.9682 - val_precision_26: 0.8638 - val_auc_26: 0.9927 - val_recall_26: 0.9254
Epoch 38/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0703 - acc: 0.9573 - precision_26: 0.9351 - auc_26: 0.9924 - recall_26: 0.8586 - val_loss: 0.0736 - val_acc: 0.9704 - val_precision_26: 0.9205 - val_auc_26: 0.9931 - val_recall_26: 0.8953
Epoch 39/100
33/33 [==============================] - 3s 87ms/step - loss: 0.0688 - acc: 0.9581 - precision_26: 0.9419 - auc_26: 0.9925 - recall_26: 0.8552 - val_loss: 0.0759 - val_acc: 0.9706 - val_precision_26: 0.9204 - val_auc_26: 0.9921 - val_recall_26: 0.8942
Epoch 40/100
33/33 [==============================] - 3s 86ms/step - loss: 0.0680 - acc: 0.9585 - precision_26: 0.9424 - auc_26: 0.9926 - recall_26: 0.8630 - val_loss: 0.0735 - val_acc: 0.9708 - val_precision_26: 0.8888 - val_auc_26: 0.9934 - val_recall_26: 0.9284
Epoch 41/100
32/33 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9579 - precision_26: 0.9400 - auc_26: 0.9901 - recall_26: 0.8535Restoring model weights from the end of the best epoch.
33/33 [==============================] - 3s 87ms/step - loss: 0.0768 - acc: 0.9578 - precision_26: 0.9397 - auc_26: 0.9901 - recall_26: 0.8530 - val_loss: 0.0707 - val_acc: 0.9721 - val_precision_26: 0.9144 - val_auc_26: 0.9927 - val_recall_26: 0.9076
Epoch 00041: early stopping
6/6 [==============================] - 0s 26ms/step - loss: 0.0703 - acc: 0.9728 - precision_26: 0.9165 - auc_26: 0.9938 - recall_26: 0.9157
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbdc8191670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
---------------TEST METRICS----------------------
jaccard_index 0.8330968646990644
test_sensitivity 0.9254336491703014
test_specifitivity 0.9820243103118858
test_accuracy 0.9728379991319445
test_precision 0.9088971315326256
test_jaccard_score 0.8330968646990644
test_dicecoef 0.9170908518825958
isic_eval_score 0.9777777777777777
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-115030.h5
[1.94522705 1.67507719 1.80914208 1.8586546  1.9620036 ] [0.972838   0.83309686 0.90889713 0.92543365 0.98202431]

-------------------------
Averaged metrics for Baseline + Augumentations + Histogram Equalization + Per Channel Normalization - bacteria: [0.97268835 0.83605802 0.90601307 0.92802942 0.98134264]
-------------------------

(tfenv) [ignacys@xeon-e-v100 segmentation]$ client_loop: send disconnect: Connection reset by peer
(base)
ignac@DESKTOP-UR509JM MINGW64 ~
$
